{
  "hash": "ca217844296fd4c3f2248eb44d30b114",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Distribution Functions\"\ndescription: |\n  Provides an overview distribution function (mathematical models).\nformat:\n  revealjs:\n    width: 1200\n    scrollable: true\n    sc-sb-title: true\n    footer: m201.inqs.info/lectures/4\n    theme: [default, styles.scss]\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 32\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    fig-align: center\n    code-fold: true\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters:\n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n\n\n# Probability Theory\n\n## What is Probability?\n\n**Probability** is the measure of how likely an event is to occur. It ranges from 0 to 1:\n\n- $P(A) = 0$: The event $A$ will definitely not happen.\n- $P(A) = 1$: The event $A$ will definitely happen.\n- Values between 0 and 1 represent varying degrees of likelihood.\n\n## Everyday Examples\n- What's the probability it will rain tomorrow?\n- What are the chances of rolling a 6 on a standard die?\n- How likely is it that a randomly chosen student has a GPA above 3.0?\n\n\n## Key Terms and Definitions\n\n::: panel-tabset\n\n### **Experiment**\nAn action or process that generates outcomes.  \n- Example: Rolling a die.\n\n### **Sample Space**\nThe set of all possible outcomes of an experiment.  \n- Example: For rolling a die, $S = \\{1, 2, 3, 4, 5, 6\\}$.\n\n### **Event**\nA subset of the sample space, representing outcomes of interest.  \n\n- Example: Rolling an even number ($A = \\{2, 4, 6\\}$).\n\n### **Prob. of an Event**\nThe proportion of times an event is expected to occur if the experiment is repeated many times.\n\n:::\n\n## The Probability Formula\n\nThe probability of an event $A$ is defined as:\n\n$$\nP(A) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}}\n$$\n\n## Example\n\nIf we roll a die, what is the probability of rolling a 4?\n\n- **Favorable outcomes**: 1 (rolling a 4).\n- **Total outcomes**: 6 (since $S = \\{1, 2, 3, 4, 5, 6\\}$).\n\n$$\nP(\\text{rolling a 4}) = \\frac{1}{6} \\approx 0.167\n$$\n\n\n## Rules of Probability\n\n::: panel-tabset\n\n### Rule 1\n\n**Probability of the Sample Space**\n\nThe probability of the sample space is always 1:\n$$\nP(S) = 1\n$$\n\n### Rule 2 \n\n**Probability of Impossible Events**\n\nThe probability of an event that cannot happen is 0:\n\n$$\nP(\\emptyset) = 0\n$$\n\n### Rule 3\n\n**Complement Rule**\n\nThe probability of the complement of an event $A$ (not $A$) is:\n\n$$\nP(A^c) = 1 - P(A)\n$$\n\n- Example: If $P(\\text{rain}) = 0.3$, then $P(\\text{no rain}) = 1 - 0.3 = 0.7$.\n\n\n:::\n\n## Applications\n\n::: panel-tabset\n\n### Drawing a Card\nIf you draw a card from a standard deck of 52 cards, what is the probability of drawing:\n\n1. A heart?  \n$$\n P(\\text{heart}) = \\frac{13}{52} = 0.25\n$$\n\n2. A red card (heart or diamond)?  \n$$\n P(\\text{red card}) = \\frac{26}{52} = 0.5\n$$\n\n### Tossing a Coin Twice\nWhat is the probability of getting:\n\n1. Exactly one head?  \n   Sample space: $S = \\{\\text{HH, HT, TH, TT}\\}$.  \n   Event: $A = \\{\\text{HT, TH}\\}$.  \n$$\nP(A) = \\frac{2}{4} = 0.5\n$$\n\n2. At least one head?  \n   Event: $B = \\{\\text{HH, HT, TH}\\}$.  \n$$\nP(B) = \\frac{3}{4} = 0.75\n$$\n\n### Traffic Lights\nA commuter encounters three traffic lights, each with a 70% chance of being green. Assuming independence, what is the probability that all three lights are green?\n\n$$\nP(\\text{all green}) = 0.7 \\cdot 0.7 \\cdot 0.7 = 0.343\n$$\n\n:::\n\n\n## More Problems\n\n::: panel-tabset\n\n### Rolling a Die\n\n|1 | 2 | 3 | 4 | 5 | 6 |\n|-|-|-|-|-|-|\n|1/6| 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |\n\n\n### 1\n\n$$\nP(X = 2)\n$$\n\n### 2\n\n\n$$\nP(1 \\le X \\le 3)\n$$\n\n\n### 3\n\n$$\nP(1 < X < 3)\n$$\n\n### 4\n\n$$\nP(X > 1)\n$$\n\n:::\n\n\n# Joint Probability\n\n## Joint Probability\n\n*   Often, we're interested in the probabilities of multiple events occurring.\n*   This presentation focuses on calculating probabilities involving two events.\n*   We'll explore key concepts like:\n    *   Joint Probability\n    *   Union of Events\n    *   Conditional Probability\n    *   Independence\n\n\n## Joint Probability\n\n*   The probability of *both* events A and B occurring.\n*   Denoted as $P(A\\ \\mathrm{and}\\ B)$ or $P(A \\cap B)$.\n*   Examples:\n    *   Drawing a King *and* a Heart from a deck of cards.\n    *   Flipping two coins and getting heads on both.\n\n## Union of Events\n\n*   The probability of *either* event A *or* event B (or both) occurring.\n*   Denoted as $P(A\\ \\mathrm{or}\\ B)$ or $P(A \\cup B)$.\n*   Examples:\n    *   Rolling a 3 or a 5 on a die.\n    *   Drawing a red card or a face card.\n\n\n## Calculating Union Probability\n\n*   **General Case:** $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$  (Inclusion-Exclusion Principle)\n*   **Mutually Exclusive Events:** If A and B are mutually exclusive (cannot both occur), then $P(A \\cap B) = 0$, and $P(A \\cup B) = P(A) + P(B)$\n\n\n## Example: Union Probability\n\n*   What is the probability of rolling a number greater than 4 or an even number on a six-sided die?\n\n    *   Event A: Rolling a number greater than 4 (5 or 6).\n    *   Event B: Rolling an even number (2, 4, or 6).\n\n::: fragment\n\n*   $P(A) = 2/6$\n*   $P(B) = 3/6$\n*   $P(A \\cap B) = 1/6$ (rolling a 6)\n*   $P(A \\cup B) = (2/6) + (3/6) - (1/6) = 4/6 = 2/3$\n:::\n\n\n## Conditional Probability\n\n*   The conditional probability of event A occurring *given* that event B has occurred:  $P(A|B)$ (read as \"the probability of A given B\").\n\n*   Formula:  $P(A|B) =\\frac{P(A \\cap B)}{P(B)}$\n\n    *   $P(A|B)$: Conditional probability of A given B.\n    *   $P(A \\cap B)$: Probability of both A and B occurring.\n    *   $P(B)$: Probability of B occurring.\n\n\n## Independence\n\n*   Two events are independent if the occurrence of one does not affect the probability of the other.\n*   If A and B are independent:\n    *   $P(A|B) = P(A)$\n    *   $P(B|A) = P(B)$\n    *   $P(A \\cap B) = P(A) P(B)$\n\n## Bayes' Theorem\n\n*   Reverses the conditioning:  Relates P(A|B) to P(B|A).\n*   $P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}$\n*   Useful when we know P(B|A) but want P(A|B).\n\n\n# Distribution Functions\n\n## Random Variable\n\nA **random variable** is a variable whose value is a numerical outcome of a random phenomenon. It's a way to map the outcomes of a probabilistic event to numbers. This allows us to analyze these outcomes mathematically.\n\n## RV: Key Concepts\n\n* **Random Phenomenon:** This is any process or experiment whose outcome is uncertain. \n\n* **Outcomes:** The possible results of a random phenomenon are called outcomes. \n\n* **Numerical Value:** A random variable assigns a numerical value to each outcome. \n\n## Types of Random Variables\n\n::: panel-tabset\n\n### Discrete Random Variable \n\nA discrete random variable can only take on a finite number of values or a countably infinite number of values. The values are often integers. Examples:\n\n- The number of heads in three coin flips (0, 1, 2, or 3).\n- The number of defective light bulbs in a box of 100.\n- The number of customers entering a store in an hour.\n\n### Continuous Random Variable\n\n A continuous random variable can take on any value within a given range. Examples:\n\n- The height of a person.\n- The temperature of a room.\n- The time it takes to complete a task.\n\n:::\n\n\n## Distribution Functions\n\nA **distribution function** describes the probabilities of a random variable across its possible values. It answers questions like:\n\n- How likely is a random variable to take on a specific value or fall within a range?\n- What is the overall \"shape\" of the distribution of outcomes?\n\n## Cumulative Distribution Function\n\nThe **Cumulative Distribution Function (CDF)** describes the probability that a random variable $X$ is less than or equal to a certain value $x$:\n\n$$\nF_X(x) = P(X \\leq x)\n$$\n\n## Properties of the CDF\n\n1. **Range**: The CDF is always between 0 and 1:\n$$\n0 \\leq F_X(x) \\leq 1\n$$\n\n2. **Non-Decreasing**: The CDF never decreases as $x$ increases.\n\n3. **Asymptotic Limits**:\n   - $\\lim_{x \\to -\\infty} F_X(x) = 0$\n   - $\\lim_{x \\to \\infty} F_X(x) = 1$\n\n## CDF Example\n\nFor a die roll (discrete case):\n\n- $F_X(2) = P(X \\leq 2) = P(X = 1) + P(X = 2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}$.\n\nFor a normal distribution (continuous case):\n\n- Use the CDF to find probabilities, typically provided via tables or software.\n\n\n## Probability Density Function (PDF)\n\nThe **Probability Density Function (PDF)** is used for **continuous random variables** and describes the likelihood of the variable falling within a small interval.\n\n$$\nP(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, dx\n$$\n\n\n::: fragment\n::: callout-important\n$$\nP(X = a) = P(a \\leq X \\leq a) = \\int_a^a f_X(x) \\, dx = 0\n$$\n\n:::\n:::\n\n## Normal PDF\n\nThe PDF of the normal distribution is:\n$$\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$\n\nHere, $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n\n\n\n## Probability Mass Function (PMF)\n\nThe **Probability Mass Function (PMF)** is used for **discrete random variables** and gives the probability of each possible value:\n$$\nP(X = x) = p_X(x)\n$$\n\n## Properties of the PMF\n1. **Non-Negative**:\n   $$\n   p_X(x) \\geq 0 \\quad \\forall x\n   $$\n\n2. **Sum of Probabilities**:\n   $$\n   \\sum_x p_X(x) = 1\n   $$\n\n## PMF Example\n\nFor a fair die, the PMF is:\n$$\np_X(x) = \\frac{1}{6}, \\quad x \\in \\{1, 2, 3, 4, 5, 6\\}\n$$\n\n\n## Percentiles\n\nA **percentile** is a measure indicating the value below which a given percentage of observations in a group of observations falls. It is used in statistics to provide information about the relative standing of a particular value within a dataset.\n\n## Percentile Example\n\nSuppose we have a random variable X representing the height of adults in a population. If the 90th percentile of X is 180 cm, it means that the probability of a randomly selected adult being 180 cm or shorter is 90%.\n\n## Percentile\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata <- rnorm(500000, mean = 100, sd = 15) |> data.frame(x = _)  # Example: normally distributed data\n\n# Calculate the 75th percentile\npercentile_75 <- qnorm(0.75, 100, 15)\n\nggplot(data = data, aes(x = x)) +\n  geom_density(fill = \"plum\", alpha = 0.5) +  # Density curve\n  geom_vline(xintercept = percentile_75, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Density Plot with 75th Percentile\", x = \"Data Value\", y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](4_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Applications\n\n::: panel-tabset\n\n### PMF of a Coin Toss\nFor a fair coin tossed once:\n\n- $X = 0$: Tails, $P(X = 0) = 0.5$\n- $X = 1$: Heads, $P(X = 1) = 0.5$\n\nThe PMF is:\n$$\np_X(x) = \n\\begin{cases} \n0.5 & x = 0 \\text{ or } x = 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\n\n### CDF of an Exp. Dist.\nAn exponential random variable with rate $\\lambda > 0$ has a CDF:\n$$\nF_X(x) = \n\\begin{cases} \n1 - e^{-\\lambda x}, & x \\geq 0 \\\\\n0, & x < 0\n\\end{cases}\n$$\n\nThis can be used to model waiting times between events.\n\n### Apps. in Real Life\n\n1. **PMF**:\n\n   - Number of emails received per hour.\n\n2. **PDF**:\n\n   - Heights of students in a class.\n\n3. **CDF**:\n\n   - The likelihood of completing a task within a certain time frame.\n\n:::\n\n\n## Common Contisuous RV\n\n| Continuous RV | Parameters | Notation | Support |\n|---|---|---|---|\n| Uniform | a (minimum), b (maximum) | Unif(a, b) | [a, b] |\n| Normal (Gaussian) | μ (mean), σ (standard deviation) | N(μ, σ²) | (-∞, ∞) |\n| Exponential | λ (rate) | Exp(λ) | [0, ∞) | \n| Gamma | k (shape), θ (scale) | Γ(k, θ) | [0, ∞) | \n| Beta | α (shape 1), β (shape 2) | Beta(α, β) | [0,1] | \n| Chi-Squared | k (degrees of freedom) | χ²(k) | [0, ∞) | \n| t-distribution | ν (degrees of freedom) | t(ν) | (-∞, ∞) | \n| F-distribution | ν₁ (degrees of freedom 1), ν₂ (degrees of freedom 2) | F(ν₁, ν₂) | [0, ∞) | \n| Weibull | k (shape), λ (scale) | Weibull(k, λ) | [0, ∞) | \n| Lognormal | μ (location), σ (scale) | Lognormal(μ, σ) | [0, ∞) | \n\n## Common Discrete RV\n\n| Discrete RV | Parameters | Notation | Support |\n|---|---|---|---|\n| Bernoulli | p (probability of success) | Bernoulli(p) | {0, 1} |\n| Binomial | n (number of trials), p (probability of success) | Bin(n, p) | {0, 1, 2, ..., n} |\n| Geometric | p (probability of success) | Geo(p) | {1, 2, 3, ...} |\n| Poisson | λ (average rate of events) | Pois(λ) | {0, 1, 2, ...} |\n| Negative Binomial | r (number of successes), p (probability of success) | NB(r, p) | {r, r+1, r+2, ...} |\n| Discrete Uniform | n (number of possible outcomes) | DUnif(1, n) | {1, 2, ..., n} or a set of n values |\n\n## Probability and PD/MF\n\n::: panel-tabset\n\n### Continuous\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nmean <- 0  # Mean of the distribution\nsd <- 1   # Standard deviation of the distribution\n\n# Generate data points for the curve (more points = smoother curve)\nx <- seq(mean - 3*sd, mean + 3*sd, length.out = 100)  # Range covering most of the distribution\ny <- dnorm(x, mean = mean, sd = sd)  # Calculate the density at each x value\n\n# Create the plot\nggplot(data = data.frame(x, y), aes(x = x, y = y)) +\n  geom_line(color = \"sienna\", size = 1) +  # Line for the distribution curve\n  labs(x = \"X\",\n       y = \"Probability Density\") +\n  scale_x_continuous(breaks = seq(mean - 3*sd, mean + 3*sd, by = sd)) # Ticks at standard deviations\n```\n\n::: {.cell-output-display}\n![](4_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n### Discrete\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- 10   # Number of trials\np <- 0.3  # Probability of success on each trial\n\n# Generate the data for the plot\nx <- 0:n  # Possible values for the random variable (number of successes)\ny <- dbinom(x, size = n, prob = p)  # Calculate the probabilities\n\n# Create the plot using geom_col (for discrete data)\nggplot(data = data.frame(x, y), aes(x = factor(x), y = y)) +  # x as factor for discrete bars\n  geom_col(fill = \"bisque4\", color = \"black\", alpha =  0.75) +\n  labs(title = paste(\"Binomial Distribution (n =\", n, \", p =\", p, \")\"),\n       x = \"Number of Successes (x)\",\n       y = \"Probability P(X = x)\") \n```\n\n::: {.cell-output-display}\n![](4_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n\n# Normal Distribution\n\n## Normal Distribution\n\nThe **Normal Distribution** is a probability distribution that is symmetric, with most of the data points clustering around the mean. \n\n- It's **bell-shaped** and is defined mathematically by two parameters:\n  - **Mean ($\\mu$)**: The center or peak of the distribution.\n  - **Standard Deviation ($\\sigma$)**: Controls the spread of the distribution.\n\n\n::: fragment\n::: callout-important\nFor any normally distributed data, the highest probability density is at the mean, and as you move away from the mean, the probability density gradually decreases.\n:::\n:::\n\n## Properties\n\n\n1. **Symmetry**: It is perfectly symmetric about the mean, meaning the left side is a mirror image of the right.\n2. **Unimodal**: There is a single peak at the mean.\n3. **Mean, Median, and Mode are Equal**: In a normal distribution, these three measures of central tendency are located at the same point.\n4. **68-95-99.7 Rule** (Empirical Rule)\n\n::: notes\n\nThis rule helps us understand how data is distributed in a normal curve and provides a quick way to estimate probabilities for normally distributed data.\n\n:::\n\n## Normal Distribution\n\n\n::: {.column}\n\n$$\nX\\sim N(\\mu, \\sigma)\n$$\n\n$$\n-\\infty < X < \\infty\n$$\n\n:::\n::: {.column}\n\n$$\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$\n\n:::\n\n\n## Normal PDF\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Standard Normal Distribution\n\nThe **Standard Normal Distribution** is a special type of normal distribution with a **mean of 0** and a **standard deviation of 1**. It's often used as a reference to convert any normal distribution to a standard form.\n\n\n## Standard Normal Distribution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nmean <- 0  # Mean of the distribution\nsd <- 1   # Standard deviation of the distribution\n\n# Generate data points for the curve (more points = smoother curve)\nx <- seq(mean - 3*sd, mean + 3*sd, length.out = 100)  # Range covering most of the distribution\ny <- dnorm(x, mean = mean, sd = sd)  # Calculate the density at each x value\n\n# Create the plot\nggplot(data = data.frame(x, y), aes(x = x, y = y)) +\n  geom_line(color = \"palegreen4\", size = 1) +  # Line for the distribution curve\n  labs(x = \"X\",\n       y = \"Probability Density\") +\n  scale_x_continuous(breaks = seq(mean - 3*sd, mean + 3*sd, by = sd)) # Ticks at standard deviations\n```\n\n::: {.cell-output-display}\n![](4_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Z-Scores\n\nA **Z-score** (or standard score) tells us how many standard deviations an individual data point is from the mean. It’s calculated as:\n\n$$\nZ = \\frac{X - \\mu}{\\sigma}\n$$\n\n- If $Z$ is positive, the data point is above the mean.\n- If $Z$ is negative, the data point is below the mean.\n- Using Z-scores, we can compare values across different normal distributions or find the probability associated with a particular score.\n\n\n## Why the Normal Distribution is Important\n\n\n1. **It Describes Many Natural Phenomena**: Heights, weights, test scores, measurement errors, and countless other variables follow a normal distribution, especially when influenced by many small, random factors.\n2. **Predictive Power**: With normally distributed data, we can make predictions and infer probabilities, thanks to the 68-95-99.7 rule.\n3. **Central Limit Theorem**: The normal distribution is foundational to the Central Limit Theorem, which tells us that, regardless of the original data distribution, the sampling distribution of the sample mean will approach a normal distribution as sample size increases.\n4. **Ease of Use in Statistical Methods**: Many statistical tests and methods assume normality, allowing for simplified calculations and reliable inferences.\n\n\n## Apps of the Normal Dist.\n\n::: panel-tabset\n\n### Standardized Testing\nScores on standardized tests, such as IQ tests or the SAT, are often designed to follow a normal distribution. By knowing a student’s score in terms of Z-scores, we can determine their percentile or how they compare to other test-takers.\n\n### Finance\nIn finance, stock returns and other economic factors are often modeled with a normal distribution to estimate risk, forecast trends, and make informed investment decisions.\n\n:::\n\n## Finding Probability\n\n$$ \nX \\sim N(\\mu, \\sigma)\n$$\n\n::: fragment\n\n$$\nP(a \\leq X \\leq b) = P(a < X < b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx\n$$\n\n:::\n\n\n# Empirical Rule\n\n\n## What is the Empirical Rule?\n\nThe **Empirical Rule** provides a way to understand the spread of data in a normal distribution by describing how data points cluster around the mean. According to this rule:\n\n1. Approximately **68%** of data points fall within **one standard deviation** of the mean.\n2. Approximately **95%** of data points fall within **two standard deviations** of the mean.\n3. Approximately **99.7%** of data points fall within **three standard deviations** of the mean.\n\nThe empirical rule is very helpful because, with just the mean and standard deviation, we can quickly estimate how data is distributed within a normal curve.\n\n\n## Empirical Rule to the Normal Dist.\n\nLet's define the key terms and apply the empirical rule to the normal distribution.\n\n- **Mean (μ)**: This is the central point of the normal distribution where the data clusters around.\n- **Standard Deviation (σ)**: This is a measure of how spread out the data points are from the mean.\n\n## Empirical Rule to the Normal Dist.\n\nIn a normal distribution:\n\n- **68% of data** lies between $(\\mu - \\sigma)$ and $(\\mu + \\sigma)$.\n- **95% of data** lies between $(\\mu - 2\\sigma)$ and $(\\mu + 2\\sigma)$.\n- **99.7% of data** lies between $(\\mu - 3\\sigma)$ and $(\\mu + 3\\sigma)$.\n\nThese intervals allow us to estimate probabilities for data within each range without needing to calculate exact probabilities.\n\n\n## Visualizing the Empirical Rule\n\nTo better understand the empirical rule, imagine a symmetric, bell-shaped normal curve. Here’s how it would look based on the empirical rule:\n\n1. The **68% region** represents the middle of the curve, starting one standard deviation left of the mean and ending one standard deviation right.\n2. The **95% region** stretches further out, covering almost the entire curve except for the outer tails.\n3. The **99.7% region** includes nearly all data points, covering the entire curve except for a tiny fraction at each extreme.\n\nThis visualization shows how the data is most concentrated around the mean, with less data appearing as we move further away.\n\n\n## Using the Empirical Rule for Probabilities\n\nThe empirical rule helps us answer questions like:\n\n- **What percentage of data points fall within a certain range?**\n- **How unusual is a data point located far from the mean?**\n\n## Examples\n\n- If we know that a data point lies more than two standard deviations away from the mean, we know it’s in the outer 5% of the distribution, making it relatively rare.\n- Using the rule, we can estimate that around 95% of values should lie within two standard deviations of the mean. If we observe data points outside of this range, we might consider them outliers.\n\n\n## More Examples\n\n::: panel-tabset\n\n### Exam Scores\nSuppose exam scores are normally distributed with a mean of 70 and a standard deviation of 10.\n\n- **68% of students** scored between **60 and 80** (70 ± 10).\n- **95% of students** scored between **50 and 90** (70 ± 20).\n- **99.7% of students** scored between **40 and 100** (70 ± 30).\n\n### Heights of Adults\nAssume that adult heights follow a normal distribution with a mean height of 170 cm and a standard deviation of 8 cm.\n\n- **68% of adults** have heights between **162 cm and 178 cm** (170 ± 8).\n- **95% of adults** have heights between **154 cm and 186 cm** (170 ± 16).\n- **99.7% of adults** have heights between **146 cm and 194 cm** (170 ± 24).\n\n:::\n\n# Binomial Distribution\n\n## Binomial Distribution\n\nThe **Binomial Distribution** is a probability distribution that models the number of successes in a fixed number of trials, where each trial has:\n\n- **Two possible outcomes**: typically called \"success\" and \"failure.\"\n- A constant probability of success, $p$, on each trial.\n\n## Real-World Examples:\n\n- Tossing a coin $n$ times and counting how many heads you get.\n- Rolling a die $n$ times and counting how many times you roll a 6.\n- Administering a medical treatment to $n$ patients and recording how many recover.\n\n::: fragment\n\nThe binomial distribution answers questions like:\n\n- \"What is the probability of getting exactly 3 heads in 5 coin tosses?\"\n- \"What is the likelihood of at least 4 successes in 10 trials?\"\n\n:::\n\n## Conditions for a Binomial Experiment\n\n\n1. **Fixed Number of Trials** ($n$):\n   - The experiment consists of a set number of trials.\n\n2. **Two Possible Outcomes**:\n   - Each trial results in either a success (e.g., heads) or a failure (e.g., tails).\n\n3. **Constant Probability of Success** ($p$):\n   - The probability of success remains the same for each trial.\n\n4. **Independence**:\n   - The outcome of one trial does not affect the outcomes of other trials.\n\n\n\n## The Binomial Probability Formula\n\n$$\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n$$\n\nWhere:\n\n- $P(X = k)$: Probability of exactly $k$ successes.\n- $n$: Number of trials.\n- $k$: Number of successes.\n- $p$: Probability of success on a single trial.\n- $1-p$: Probability of failure.\n- $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$: Represents the number of ways to choose $k$ successes from $n$ trials\n\n\n## Binomial Distribution\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Example Calculation\nSuppose you flip a fair coin 5 times ($n = 5, p = 0.5$) and want to know the probability of getting exactly 3 heads ($k = 3$).\n\n::: fragment\n$$\nP(X = 3) = \\binom{5}{3} (0.5)^3 (1-0.5)^{5-3}\n$$\n\n:::\n\n## Properties of the Binomial Dist.\n\n::: panel-tabset\n\n### Mean and Variance\nFor a binomial random variable $X$:\n\n- **Mean (Expected Value)**: $\\mu = n \\cdot p$\n- **Variance**: $\\sigma^2 = n \\cdot p \\cdot (1-p)$\n- **Standard Deviation**: $\\sigma = \\sqrt{n \\cdot p \\cdot (1-p)}$\n\n### Shape of the Distribution\n\n- If $p = 0.5$, the distribution is symmetric.\n- If $p > 0.5$, the distribution is skewed left.\n- If $p < 0.5$, the distribution is skewed right.\n\n:::\n\n## Applications\n\n::: panel-tabset\n\n### Quality Control\nA factory produces lightbulbs, and 95% of them meet quality standards. If you randomly test 10 bulbs, what is the probability that exactly 8 bulbs pass the test?\n\nHere:\n\n- $n = 10$, $p = 0.95$, $k = 8$.\n\n\n$$\nP(X = 8) = \\binom{10}{8} (0.95)^8 (0.05)^2\n$$\n\n### Sports\nA basketball player has a 60% chance of making a free throw. If they take 5 shots, what is the probability of making at least 3 shots?\n\nHere:\n- $n = 5$, $p = 0.6$.\n\n$$ \nP(X \\geq 3) = P(X = 3) + P(X = 4) + P(X = 5) \n$$\n\n### Clinical Trials\n\nIn a clinical trial, a new drug has a 70% success rate. If 15 patients are treated, what is the probability that exactly 10 respond positively to the treatment?\n\n:::\n\n# Poisson Distribution\n\n## Poisson Distribution\n\nThe **Poisson Distribution** is a discrete probability distribution that models the number of events occurring within a fixed interval. These events must happen independently and at a constant average rate. \n\n## Real-World Examples\n\n- The number of emails you receive in an hour.\n- The number of cars passing through a toll booth in 10 minutes.\n- The number of defects in a square meter of fabric.\n\n::: fragment\n\nThe Poisson distribution helps us answer questions like:\n\n- \"What is the probability of receiving 5 emails in the next hour?\"\n- \"How likely is it to have 2 defects in a single square meter?\"\n\n:::\n\n## Conditions for the Poisson Dist.\n\n1. **Events Occur Randomly**:\n   - The events are random and unpredictable.\n\n2. **Independence**:\n   - The occurrence of one event does not affect the probability of another event occurring.\n\n3. **Constant Average Rate** ($\\lambda$):\n   - The average number of events ($\\lambda$) over a fixed interval remains constant.\n\n4. **Non-Overlapping Intervals**:\n   - Events in one interval do not influence events in another.\n\n\n## The Poisson Probability Formula\n\n$$\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n$$\n\nWhere:\n\n- $P(X = k)$: Probability of $k$ events occurring.\n- $\\lambda$: Average number of events in the interval.\n- $e \\approx 2.718$\n- $k!$: Factorial of $k$, calculated as $k \\times (k-1) \\times \\dots \\times 1$.\n\n## Poisson Distribution\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Example Calculation\n\nSuppose a call center receives an average of 10 calls per hour ($\\lambda = 10$). What is the probability of receiving exactly 7 calls in the next hour ($ k = 7 $)?\n\n\n$$\nP(X = 7) = \\frac{10^7 e^{-10}}{7!}\n$$\n\n## Poisson Distribution Properties\n\n- **Mean**: $\\mu = \\lambda$\n- **Variance**: $\\sigma^2 = \\lambda$\n- **Standard Deviation**: $\\sigma = \\sqrt{\\lambda}$\n\n\n\n## Applications\n\n::: panel-tabset\n\n### Traffic Flow\nA toll booth observes an average of 3 cars passing through every 5 minutes ($\\lambda = 3$). What is the probability of seeing exactly 5 cars in the next 5 minutes?\n\nUsing the formula:\n$$\nP(X = 5) = \\frac{3^5 e^{-3}}{5!} = \\frac{243 \\cdot 0.0498}{120} \\approx 0.1008\n$$\n\n\n### Defective Products\n\nA factory produces an average of 2 defective items per day ($\\lambda = 2$). What is the probability of finding no defective items in a day ($k = 0$)?\n\n$$\nP(X = 0) = \\frac{2^0 e^{-2}}{0!} = e^{-2} \\approx 0.1353\n$$\n\n\n\n:::\n\n# Probability in R\n\n## Distributions in R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\n?Distributions\n```\n:::\n\n\n\n## Function Structure\n\n|Letter|Functionality|\n|:-|:-|\n|\"d\"|returns the height of the probability density function|\n|\"p\"|returns the cummulative density function value|\n|\"q\"|returns the inverse cummulative density function (percentiles) |\n|\"r\"|returns a randomly generated number|\n\n\n## Probabilities\n\nR can compute the probabilities of a distribution given the correct parameters:\n\n-  Cummulative probability: `p` is used in front of the distribution R function \n-  Probability for a discrete distribution: `d` is used in front of the distribution R function\n      - Note: Continuous Distribution Functions will not yield a valid probability value.\n\n\n## Examples\n\n::: panel-tabset\n\n### 1\n\nFind $P(X \\leq 5 )$ where $X \\sim N(6,2)$. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npnorm(5, mean = 6, sd = 2)\n```\n:::\n\n\n### 2\n\nFind $P(X \\geq 7 )$ where $X \\sim N(6,2)$. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n1 - pnorm(7, mean = 6, sd = 2)\n```\n:::\n\n\n\n### 3\n\nFind $P(X = 20 )$ where $X \\sim Bin(30,0.8)$. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndbinom(20, size = 30, prob = 0.8)\n```\n:::\n\n\n\n:::\n\n## More Examples\n\n::: panel-tabset\n\n### 1\n\nFind $P(2 \\leq X \\leq 5 )$ where $X \\sim N(6,2)$. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npnorm(5, 6, 2) - pnorm(2, 6, 2) \n```\n:::\n\n\n### 2\n\nFind $P(14 < X < 20)$ where $X \\sim Pois(16)$. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nppois(19, lambda = 16) - ppois(14, lambda = 16)\n\n## OR\n\ndpois(15, 16) + dpois(16, 16) + dpois(17, 16) + dpois(18, 16) + dpois(19, 16)\n```\n:::\n\n\n### 3\n\nFind $P(23 < X)$ where $X \\sim Pois(12)$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n1 - ppois(23, 12)\n```\n:::\n\n\n:::\n\n## Percentiles\n\nFinding the values (percentiles) for any distributions can be found by using the q-based distribution R function such as `qnorm()`, `qpois()`, and `qbinom()` functions.\n\n\n## Examples\n\n::: panel-tabset\n\n### 1\n\nFinding the $95^{th}$ percentile from $N(0,1)$, we will use the `qnorm()`. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nqnorm(.95, 0, 1)\n```\n:::\n\n\n### 2\n\nFinding the $95^{th}$ percentile from a Poisson distribution with $\\lambda = 9.5$.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nqpois(.95, 9.5)\n```\n:::\n\n\n### 3\n\nFinding the $75^{th}$ percentile for $Bin(45,.4)$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nqbinom(.75, 45, .4)\n```\n:::\n\n\n:::\n\n## Random Number Generator\n\nR is capable of generating random numbers. For example if we want to generate a random sample of size fifty from a normal distribution with mean eight and variance three, we will use the `rnorm()`. If we want to generate a random sample from any distribution, use the distribution function with `r` in front of it. \n\n## Examples\n\n::: panel-tabset\n\n### 1\n\nLet's first generate the random sample of fifty from $X \\sim N(8,3)$. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nrnorm(50, 8, sqrt(3))\n```\n:::\n\n\n### 2\n\nGenerate a random sample of 100 form an $X \\sim Gamma (2,3)$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nrgamma(100, 2,3)\n```\n:::\n\n\n### 3\n\nGenerate a random sample of 100 form an $X \\sim Binom (25,.23)$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nrbinom(n = 100, size  = 25, prob = 0.23)\n```\n:::\n\n\n### 4\n\nGenerate a random sample of 100 form an $X \\sim Pois (34.4)$. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nrpois(n = 100, lambda = 34.4)\n```\n:::\n\n\n:::\n\n",
    "supporting": [
      "4_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}