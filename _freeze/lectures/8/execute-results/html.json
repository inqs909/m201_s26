{
  "hash": "c221c8f07022f3270294e4494249c7f7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multivariable Regression\"\ndescription: |\n  Continues the discussion with linear and logistic regression.\nformat:\n  revealjs:\n    width: 1200\n    scrollable: true\n    sc-sb-title: true\n    footer: m201.inqs.info/lectures/8\n    theme: [default, styles.scss]\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 32\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    code-fold: true\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n\n\n\n# Modeling Outcomes\n\n## Explaining Variation\n\n::: fragment\nThis is the process where we try to reduce the variation with the use of other variables.\n:::\n\n::: fragment\nCan be thought of as getting it less wrong when taking an educated guess.\n:::\n\n## Taylor Swift's Songs Danceability\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(taylor_album_songs, aes(danceability)) +\n  geom_density()\n```\n\n::: {.cell-output-display}\n![](8_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## Danceability by `mode_name`\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntaylor_album_songs |> \n  drop_na(mode_name) |> \n  ggplot(aes(danceability)) +\n  geom_density() +\n  facet_wrap(~ mode_name)\n```\n\n::: {.cell-output-display}\n![](8_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## Danceability by Valence\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(taylor_album_songs, aes(valence, danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](8_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Danceability by Energy\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(taylor_album_songs, aes(energy, danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](8_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Modelling Danceability\n\nHow do we use all the variables to explain danceability?\n\n# Multivariable Linear Regression\n\n## MLR\n\nMultivariable Linear Regression (MLR) is used to model an outcome variable ($Y$) by multiple predictor variables ($X_1, X_2, \\ldots, X_p$).\n\n\n::: fragment\nUsing MLR, you propose that the ouctome variable was constructed from a set of predictors, with their corresponding regression coefficients ($\\beta$), and a bit of error\n:::\n\n\n::: fragment\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n$$\n\n$$\n\\varepsilon \\sim DGP\n$$\n:::\n\n## Model Data\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n$$\n\n$$\n\\varepsilon \\sim DGP\n$$\n\n::: fragment\n\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n$$\n\n$$\n\\varepsilon_i \\sim DGP\n$$\n\n:::\n\n## Unknown Parameters\n\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n$$\n\n::: fragment\n$$\n\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_p\n$$\n:::\n\n## Estimated Model\n\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n$$\n\n$$\n\\hat Y_i = \\hat\\beta_0 + \\hat\\beta_1 X_{i1} + \\hat\\beta_2 X_{i2} + \\cdots + \\hat\\beta_p X_{ip} \n$$\n\n## Estimating Prameters\n\n$\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_p$ are estimated by minimizing the following function:\n\n\n$$\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n$$\n\n\n## Fitting a Model in R\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlm(Y ~ X1 + X2 + ... + Xp, data = DATA)\n```\n:::\n\n\n\n## Modelling Danceability\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlm(danceability ~ mode_name + valence + energy, \n   data = taylor_album_songs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = danceability ~ mode_name + valence + energy, data = taylor_album_songs)\n#> \n#> Coefficients:\n#>    (Intercept)  mode_nameminor         valence          energy  \n#>        0.53816         0.08386         0.17554        -0.05944\n```\n\n\n:::\n:::\n\n\n## Estimated Model\n\n$$\ndance = 0.54 + 0.07\\times Minor + 0.18 \\times valence - 0.06 \\times energy \n$$\n\n## $\\hat \\beta_i$ Representation\n\nEach regression coefficient $\\beta_i$ represents how the predictor variable changes the outcome, as it increase by 1 unit.\n\n::: fragment\nFor categorical dummy variables, the $\\beta_i$ represents how the outcome will change when the data point belongs to that value.\n:::\n\n\n## $\\hat \\beta_i$ Interpretation\n\nFor $hat \\beta_i$, which is the regression coefficient (slope) of $X_i$:\n\nAs $X_i$ increases by 1 unit, the outcome ($Y$) will increase/decrease by $\\hat \\beta_i$ units, adjusting for all other predictor variables.\n\n::: fragment\nFor categorical dummy variables $X_i$:\n\nThe outcome $Y_i$ increases/decreases by $\\beta_i$ units for category $X_i$ compared to the reference category, adjusting for all other predictor variables.\n\n:::\n\n\n## Intepreting Minor coefficient\n\n$$\ndance = 0.54 + 0.07 Minor + 0.18 valence - 0.06 energy \n$$\nMinor song's average danceability score is 0.07 units higher compared to Major song's, adjusting for valence and energy.\n\n## Intepreting valence coefficient\n\n$$\ndance = 0.54 + 0.07Minor + 0.18  valence - 0.06 energy \n$$\n\nAs valence increases by 1 unit, danceability increases by an average of 0.18 units, adjusting for energy and type of song.\n\n\n## Intepreting energy coefficient\n\n$$\ndance = 0.54 + 0.07 Minor + 0.18 valence - 0.06 energy \n$$\n\nAs energy increases by 1 unit, danceability decreases by an average of 0.06 units, adjusting for valence and type of song.\n\n\n# Adjusted $R^2$\n\n## $R^2$\n\nComputing $R^2$ is done by determining how much the variation in the outcome is explained by model and divided by the variation of the outcome.\n\n$$\nR^2 = \\frac{\\text{variation explained by model}}{\\text{variation from outcome}} \\\\\n= 1-\\frac{\\text{variation of residuals}}{\\text{variation from outcome}}\n$$\n\n## $R^2$\n\nProblems arise when multiple predictors are added to the model. As a new predictor is added to the model, new information is added to the model which will always reduce the variation in the residuals. Therefore, the $R^2$ will always increase.\n\n## Problems with $R^2$ in MLR\n\nWhen the number of variables increase, the regular $RÂ²$ will be biased in its prediction capability when new data is obtained.\n\n::: fragment\nTherefore, statisticians uses the adjusted $R^2$, that penalizes the model when more variables are added. This ensures that a variable added will have a significant effect in predicting outcomes.\n:::\n\n## Adjusted $R^2$\n\n$$\nR_a^2 = 1-\\frac{\\text{variation of residuals}}{\\text{variation from outcome}}\\times \\frac{n-1}{n-k-1}\n$$\n\n- $n$: Number of data points\n- $k$: Number of predictor variables in the model\n\n## Adjusted $R^2$ in R\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nxlm <- lm(Y ~ X1 + X2 + ... + Xp, data = DATA)\nar2(xlm)\n```\n:::\n\n\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nxlm <- lm(danceability ~ mode_name + valence + energy, \n   data = taylor_album_songs)\nar2(xlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.09148268\n```\n\n\n:::\n:::\n\n\n8.1% of the variation in danceability is explained by the model.\n\n# Model Selection\n\n## Model Selection\n\nModel Selection is the process of obtaining a \"final\" model containing all the necessary predictors, and eliminating any that are not necessary.\n\n## Forward Selection\n\nBegin with the null model ($Y\\sim 1$) and add variables until a final model is chosen.\n\n## Backward Selection\n\nBegin with the full model, and remove variable until the final model is chosen.\n\n## Hybrid Selection\n\nA hybrid approach between the forward and backward building approach.\n\n## About Model Selection\n\nGenerally, it is not a good idea to conduct model selection. The predictor variables in your model should be guided by a literature review that illustrates important predictor variables in a model.\n",
    "supporting": [
      "8_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}