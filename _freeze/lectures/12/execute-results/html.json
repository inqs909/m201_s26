{
  "hash": "9471f80c663542a586ae83e445efe33c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical <br> Modeling\"\ndescription: |\n  Finish the discussion on model inference.\nformat:\n  revealjs:\n    width: 1200\n    scrollable: true\n    sc-sb-title: true\n    footer: m201.inqs.info/lectures/12\n    theme: [default, styles.scss]\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 32\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    code-fold: true\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  \nfilters:\n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n\n\n\n\n# Hypothesis Tests: Multi Regression\n\n## Hypothesis Tests\n\n\nConducting a hypothesis test for coefficients in regression models with more than one predictor is the same as the standard simple approaches.\n\n::: fragment\n\nYou will just need to specify the hypothesis tests are adjusted for the other covariates.\n\n:::\n\n## Hypothesis Testing Steps\n\n1.  State $H_0$ and $H_1$\n2.  Choose $\\alpha$\n3.  Compute confidence interval/p-value\n4.  Make a decision\n\n\n## Null Hypothesis $H_0$\n\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value.\n\n## Alternative Hypothesis $H_1$\n\nThe alternative hypothesis contradicts the null hypothesis.\n\n## Significance Level\n\nChoose a value that represents the probability of being wrong if you decide to reject the $H_0$.\n\n::: fragment\n$$\n\\alpha = 0.05\n$$\n:::\n\n::: notes\nWalk through the steps slowly with an example in mind. Emphasize that $\\alpha$ is a threshold, not the actual probability of error.\n:::\n\n## Conducting HT of $\\beta_j$ Linear R\n\n```r\nXLM <- lm(Y ~ X1 + X2 + ... + Xp, data = DATA)\ntidy(XLM)\n```\n\n-   `XLM`: Object where the model is stored\n-   `Y`: Name of the outcome variable in `DATA`\n-   `X1`, `X2`, …, `Xp`: predictor variables in `DATA`\n-   `DATA`: Name of the data set\n\n\n## Conducting HT of $\\beta_j$\n\n```r\nXLM <- glm(Y ~ X1 + X2 + ... + Xp, data = DATA, family = binomial())\ntidy(XLM)\n```\n\n-   `XLM`: Object where the model is stored\n-   `Y`: Name of the outcome variable in `DATA`\n-   `X1`, `X2`, …, `Xp`: predictor variables in `DATA`\n-   `DATA`: Name of the data set\n\n\n\n## Penguins Example\n\nIs there a significant relationship between penguin body mass (outcome; `body_mass`) and flipper length (predictor; `flipper_len`), adjusting for `species`? Use the `penguins` data set to determine a significant association.\n\n## Penguins: Hypothesis\n\n$H_0$: There is no relationship between penguin body mass and flipper length, adjusting for penguin species ($\\beta_{flipper\\_len} = 0$)\n\n$H_1$: There is a relationship between penguin body mass and flipper length, adjusting for penguin species ($\\beta_{flipper\\_len} \\ne 0$)\n\n## Penguins: $\\alpha$-level\n\n$$\n\\alpha = 0.05 = 5.0*10^{-2} = 5.0e-2\n$$\n\n## Penguins: Code\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nm1 <- lm(body_mass ~ flipper_len + species, penguins)\ntidy(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 5\n#>   term             estimate std.error statistic  p.value\n#>   <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)       -4031.     584.       -6.90 2.55e-11\n#> 2 flipper_len          40.7      3.07     13.3  1.40e-32\n#> 3 speciesChinstrap   -207.      57.7      -3.58 3.98e- 4\n#> 4 speciesGentoo       267.      95.3       2.80 5.39e- 3\n```\n\n\n:::\n:::\n\n\n## Penguins: Decision Making\n\n$$\np = 1.4e-32 < 5e-2 = 0.05 = \\alpha\n$$\n\n\n**Reject $H_0$**\n\n## Penguins: Interpretation\n\nThere is a significant association between penguins flipper length and body mass, after adjusting for species (p < 0.0001; $\\beta = 40.7$). As flipper length increases by 1 unit, body mass increases by 40.7 units, adjusting for penguin species.\n\n## Heart Disease Example\n\nIs there a significant association between heart disease (outcome; `disease`) and resting blood pressure (predictor; `trestbps`), adjusting for chest pain (`cp`). Use the `heart_disease` data set to determine a significant association.\n\n\n## Heart: Hypothesis\n\n$H_0$: There is no relationship between heart disease probability and resting blood pressure, adjusting for chest pain ($\\beta_{bp} = 0$)\n\n$H_1$: There is no relationship between heart disease probability and resting blood pressure, adjusting for chest pain ($\\beta_{bp} \\ne 0$)\n\n## Heart: $\\alpha$-level\n\n$$\n\\alpha = 0.05 = 5.0*10^{-2} = 5.0e-2\n$$\n\n## Heart: Code\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nm2 <- glm(disease ~ trestbps + cp, heart_disease, family = binomial())\ntidy(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 5 × 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)  -3.80     1.25       -3.04  0.00234  \n#> 2 trestbps      0.0209   0.00807     2.59  0.00967  \n#> 3 cp2          -0.409    0.601      -0.681 0.496    \n#> 4 cp3          -0.236    0.540      -0.437 0.662    \n#> 5 cp4           2.04     0.512       3.99  0.0000674\n```\n\n\n:::\n:::\n\n\n## Heart: Decision Making\n\n$$\np = 0.00967 < 0.05 = \\alpha\n$$\n\n\n**Reject $H_0$**\n\n## Heart: Interpretation\n\nThere is a significant association between heart disease and resting blood pressure, after adjusting for chest pain (p < 0.00967; $\\beta = 0.0209$). As resting blood pressure increases by 1 unit, the odds of having heart disease increases by a factor of 1.02, adjusting for chest pain.\n\n# Model Inference\n\n## Model Inference\n\n\n# Power Analysis\n\n## What is Statistical Power\n\n- **Statistical Power** is the probability of correctly rejecting a false null hypothesis.\n- In other words, it's the chance of **detecting a real effect** when it exists.\n\n## Why Power Matters\n\n- Low power → high risk of **Type II Error** (false negatives)\n- High power → better chance of finding true effects\n- Common threshold: **80% power**\n\n## Errors in Inference\n\n|         |                               |                         |\n|:--------|:------------------------------|:------------------------|\n| Type I  | Reject $H_0$ when true        | False positive          |\n| Type II | Don't reject $H_0$ when false | False negative          |\n| Power   | $1 - P(\\text{Type II})$       | Detecting a true effect |\n\n::: notes\nPower is often overlooked. It's about how sensitive the test is to real effects. Larger samples increase power.\n:::\n\n## Type I Error (False Positive)\n\n-   **Rejecting** $H_0$ when it is actually true\n-   Probability = $\\alpha$ (significance level)\n\n::: notes\nType I errors happen when we detect an effect that doesn't really exist. This is controlled by our chosen alpha level.\n:::\n\n## Type II Error (False Negative)\n\n-   **Failing to reject** $H_0$ when it is actually false\n-   Probability = $\\beta$\n-   Power = $1 - \\beta$\n\n::: notes\nType II errors are often due to small sample sizes or high variability. Power analysis helps us plan to avoid these.\n:::\n\n## Balancing Errors\n\n-   Lowering $\\alpha$ reduces Type I errors, but **increases** risk of Type II errors.\n-   To reduce both:\n    -   Increase sample size\n    -   Use more appropriate statistical tests\n\n::: notes\nThere's a trade-off between these errors. We can't eliminate both, but we can **manage** the risk based on the consequences of each type.\n:::\n\n## What Affects Power?\n\n1. **Effect Size**  \n   - Bigger effects are easier to detect\n\n2. **Sample Size ($n$)**  \n   - Larger samples reduce standard error\n\n3. **Significance Level ($\\alpha$)**  \n   - Higher $\\alpha$ increases power (but riskier!)\n\n4. **Variability**  \n   - Less noise in data = better power\n\n## Boosting Power\n\n- Power = Probability of rejecting $H_0$ when it's false\n- Helps avoid **Type II Errors**\n- Driven by:\n  - Sample size\n  - Effect size\n  - $\\alpha$\n  - Variability\n- Aim for **80% or higher**\n\n# Simpson's Paradox\n\n## Simpson's Paradox\n\n# Model Conditions\n\n## Model Conditions\n\nWhen we are conducting inference with regression models, we will have to check the following conditions:\n\n-   Linearity\n-   Independence\n-   Probability Assumption\n-   Equal Variances\n-   Multicollinearity (for Multi-Regression)\n\n## Linearity\n\nThere must be a linear relationship between both the outcome variable (y) and a set of predictors ($x_1$, $x_2$, ...).\n\n## Independence\n\nThe data points must not influence each other.\n\n## Probability Assumption\n\nThe model errors (also known as residuals) must follow a specified distribution.\n\n- Linear Regression: Normal Distribution\n\n- Logistic Regression: Binomial Distribution\n\n## Equal Variances\n\nThe variability of the data points must be the same for all predictor values.\n\n## Residuals\n\nResiduals are the errors between the observed value and the estimated model. Common residuals include\n\n-   Raw Residual\n\n-   Standardized Residuals\n\n-   Jackknife (studentized) Residuals\n\n-   Deviance Residuals\n\n-   Quantized Residuals\n\n\n\n## Influential Measurements\n\nInfluential measures are statistics that determine how much a data point affects the model. Common influential measures are\n\n-   Leverages\n\n-   Cook's Distance\n\n## Raw Residuals\n\n$$\n\\hat r_i = y_i - \\hat y_i\n$$\n\n## Residual Analysis\n\nA residual analysis is used to test the assumptions of linear regression.\n\n## QQ Plot\n\nA qq (quantile-quantile) plot will plot the estimated quantiles of the residuals against the theoretical quantiles from a normal distribution function. If the points from the qq-plot lie on the $y=x$ line, it is said that the residuals follow a normal distribution.\n\n## Residual vs Fitted Plot\n\nThis plot allows you to assess the linearity, constant variance, and identify potential outliers. Create a scatter plot between the fitted values (x-axis) and the raw/standardized residuals (y-axis).\n\n## Residual Analysis in R\n\nUse the `resid_df` function to obtain the residuals of a model.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nrdf <- resid_df(OBJECT)\n```\n:::\n\n\n## Residual vs Fitted Plot\n\n::: {.columns}\n::: {.column}\n\n### Linear\n\n```r\nggplot(RDF, aes(fitted, resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")\n```\n\n:::\n::: {.column}\n\n### Logistic\n\n```r\nggplot(RDF, aes(fitted, quantile_resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")\n```\n\n:::\n:::\n\n- `RDF`: Name of object from `resid_df()`\n\n## QQ Plot\n\n::: {.columns}\n::: {.column}\n\n### Linear\n\n```r\nggplot(RDF, aes(sample = resid)) + \n  stat_qq() +\n  stat_qq_line() \n```\n\n:::\n::: {.column}\n\n### Logistic\n\n```r\nggplot(RDF, aes(sample = quantile_resid)) + \n  stat_qq() +\n  stat_qq_line() \n```\n:::\n:::\n\n- `RDF`: Name of object from `resid_df()`\n\n## Penguins: Example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nm3 <- lm(body_mass ~   island + species + flipper_len,\n          penguins)\ntidy(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 5\n#>   term             estimate std.error statistic  p.value\n#>   <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)       -4048.     586.      -6.91  2.40e-11\n#> 2 islandDream         -59.8     75.7     -0.789 4.31e- 1\n#> 3 islandTorgersen    -102.      77.7     -1.31  1.90e- 1\n#> 4 speciesChinstrap   -206.      70.4     -2.92  3.71e- 3\n#> 5 speciesGentoo       200.     110.       1.82  6.94e- 2\n#> 6 flipper_len          41.1      3.09    13.3   9.26e-33\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\ndfm3 <- resid_df(m3)\nhead(dfm3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   obs body_mass    island species flipper_len     resid   fitted      sresid\n#> 1   1      3750 Torgersen  Adelie         181  462.5612 3287.439  1.24797225\n#> 2   2      3800 Torgersen  Adelie         186  307.1225 3492.877  0.82640213\n#> 3   3      3250 Torgersen  Adelie         195 -612.6671 3862.667 -1.64784619\n#> 5   4      3450 Torgersen  Adelie         193 -330.4916 3780.492 -0.88855590\n#> 6   5      3650 Torgersen  Adelie         190   -7.2284 3657.228 -0.01943297\n#> 7   6      3625 Torgersen  Adelie         181  337.5612 3287.439  0.91072706\n#>      hatvals   jackknife        cooks\n#> 1 0.02662612  1.24901185 7.100464e-03\n#> 2 0.02143054  0.82601134 2.492718e-03\n#> 3 0.02058469 -1.65208143 9.511731e-03\n#> 5 0.01982753 -0.88827691 2.661855e-03\n#> 6 0.01970442 -0.01940404 1.265126e-06\n#> 7 0.02662612  0.91049529 3.781407e-03\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(dfm3, aes(fitted, resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](12_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(dfm3, aes(sample = resid)) + \n  stat_qq() +\n  stat_qq_line() \n```\n\n::: {.cell-output-display}\n![](12_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\n## Heart: Example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nm4 <- glm(disease ~ trestbps + cp, heart_disease, family = binomial())\ntidy(m4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 5 × 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)  -3.80     1.25       -3.04  0.00234  \n#> 2 trestbps      0.0209   0.00807     2.59  0.00967  \n#> 3 cp2          -0.409    0.601      -0.681 0.496    \n#> 4 cp3          -0.236    0.540      -0.437 0.662    \n#> 5 cp4           2.04     0.512       3.99  0.0000674\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\ndfm4 <- resid_df(m4)\nhead(dfm4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   obs disease trestbps cp    fitted        eta  raw_resid pearson_resid\n#> 1   1      no      145  1 0.3162617 -0.7710053 -0.3162617    -0.6801087\n#> 2   2     yes      160  4 0.8296967  1.5834796  0.1703033     0.4530559\n#> 3   3     yes      120  4 0.6787886  0.7482102  0.3212114     0.6879046\n#> 4   4      no      130  3 0.2107532 -1.3203915 -0.2107532    -0.5167502\n#> 5   5      no      130  2 0.1834250 -1.4933128 -0.1834250    -0.4739486\n#> 6   6      no      120  2 0.1541873 -1.7021301 -0.1541873    -0.4269600\n#>   deviance_resid working_resid partial_resid.trestbps partial_resid.cp\n#> 1     -0.8719863     -2.233553              -1.184687        -2.305014\n#> 2      0.6110565      2.788739               1.796346         2.404052\n#> 3      0.8802790      2.221423               1.229030         2.672005\n#> 4     -0.6880061     -2.587422              -1.302396        -2.345657\n#> 5     -0.6366106     -2.717940              -1.259993        -2.476175\n#> 6     -0.5787181     -2.884425              -1.426478        -2.433842\n#>   std_pear_resid std_dev_resid stud_dev_resid quantile_resid   leverages\n#> 1     -0.6962858    -0.8927274     -0.8846616     0.36349077 0.045927133\n#> 2      0.4562301     0.6153377      0.6134136    -0.03852898 0.013866647\n#> 3      0.6910541     0.8843093      0.8827424    -0.40587339 0.009094376\n#> 4     -0.5199246    -0.6922325     -0.6903935    -0.58934633 0.012173676\n#> 5     -0.4789686    -0.6433536     -0.6403567    -1.97130525 0.020852010\n#> 6     -0.4311339    -0.5843756     -0.5818043     1.00139375 0.019268922\n#>          cooks\n#> 1 0.0046675922\n#> 2 0.0005853744\n#> 3 0.0008765864\n#> 4 0.0006662724\n#> 5 0.0009771107\n#> 6 0.0007304018\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(dfm4, aes(fitted, quantile_resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](12_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(dfm4, aes(sample = quantile_resid)) + \n  stat_qq() +\n  stat_qq_line() \n```\n\n::: {.cell-output-display}\n![](12_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\n",
    "supporting": [
      "12_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}