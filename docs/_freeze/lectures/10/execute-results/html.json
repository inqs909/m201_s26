{
  "hash": "30aed3c93e57b47399dd635c91ee3091",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical Inference\"\ndescription: |\n  Begin the discussion of statistical inference.\n\nformat:\n  revealjs:\n    width: 1200\n    scrollable: true\n    sc-sb-title: true\n    footer: m201.inqs.info/lectures/10\n    theme: [default, styles.scss]\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 32\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    code-fold: true\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n\n\n\n\n# Statistical Inference\n\n## What is Statistical Inference?\n\n-   Drawing conclusions about a **population** based on a **sample**\n-   Population = entire group\n-   Sample = subset\n\n::: notes\nIntroduce the big idea: We want to make st\n:::\n\n## Two Main Types of Inference\n\n1.  Estimation\n2.  Hypothesis Testing\n\n::: notes\nWe'll be focusing on two fundamental techniques in inference. First, estimating population values (like the mean), and second, testing claims about the population.\n:::\n\n## Estimation\n\n- **Point Estimate**: Single best guess (e.g., $\\hat \\beta_1$)\n- **Interval Estimate**: Range likely to contain the true value\n\n::: notes\nPoint estimates are easy but not very informative. Intervals give us a sense of uncertainty, which is critical in inference.\n:::\n\n## Hypothesis Testing\n\n- $H_0$: No effect or difference\\\n- $H_1$: Some effect or difference\\\n- We use sample data to support or reject $H_0$\n\n::: notes\nMention that $H_0$ is the default assumption. We only reject it if the data give us strong enough evidence.\n:::\n\n## Key Concepts and Tools\n\n- Sampling Distribution\n- Central Limit Theorem\n- Standard Error\n\n::: notes\nThese three concepts are foundational. Understanding them helps us assess how reliable our estimates are.\n:::\n\n## p-values\n\n-   Probability of observing data as extreme as this if $H_0$ is true\n\n-   Misinterpretation of p-values is common. \n\n-   Emphasize: low p-value means data is unusual under $H_0$.\n\n## Confidence Intervals\n\n-   A range where we expect the true value to fall\n\n::: notes\nClarify interpretation: it's not about the probability the parameter is inside the interval, but about the method producing accurate intervals in the long run.\n:::\n\n# Hypothesis Testing\n\n## Hypothesis Tests\n\nHypothesis tests are used to test whether claims are valid or not. This is conducted by collecting data, setting the **Null** and **Alternative** Hypothesis.\n\n## Null Hypothesis $H_0$\n\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value.\n\n## Alternative Hypothesis $H_1$\n\nThe alternative hypothesis contradicts the null hypothesis.\n\n## Example of Null and Alternative Hypothesis\n\nWe want to see if $\\beta$ is different from $\\beta^*$\n\n| Null Hypothesis        | Alternative Hypothesis |\n|------------------------|------------------------|\n| $H_0: \\beta=\\beta^*$   | $H_1: \\beta\\ne\\beta^*$ |\n| $H_0: \\beta\\le\\beta^*$ | $H_1: \\beta>\\beta^*$   |\n| $H_0: \\beta\\ge\\beta^*$ | $H_1: \\beta<\\beta^*$   |\n\n## One-Side vs Two-Side Hypothesis Tests\n\nNotice how there are 3 types of null and alternative hypothesis, The first type of hypothesis ($H_1:\\beta\\ne\\beta^*$) is considered a 2-sided hypothesis because the rejection region is located in 2 regions. The remaining two hypotheses are considered 1-sided because the rejection region is located on one side of the distribution.\n\n| Null Hypothesis        | Alternative Hypothesis | Side    |\n|------------------------|------------------------|---------|\n| $H_0: \\beta=\\beta^*$   | $H_1: \\beta\\ne\\beta^*$ | 2-Sided |\n| $H_0: \\beta\\le\\beta^*$ | $H_1: \\beta>\\beta^*$   | 1-Sided |\n| $H_0: \\beta\\ge\\beta^*$ | $H_1: \\beta<\\beta^*$   | 1-Sided |\n\n## Hypothesis Testing Steps\n\n1.  State $H_0$ and $H_1$\n2.  Choose $\\alpha$\n3.  Compute confidence interval/p-value\n4.  Make a decision\n\n::: notes\nWalk through the steps slowly with an example in mind. Emphasize that $\\alpha$ is a threshold, not the actual probability of error.\n:::\n\n## Rejection Region\n\n- The rejection region  is the set of all test statistic values that lead to rejecting $H_0$.\n\n- It’s defined by a significance level ($\\alpha$) — the probability of rejecting $H_0$, when it’s actually true.\n\n\n## Rejection Region\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nalpha <- 0.05\n\n# Critical values for two-tailed test\nz_critical <- qnorm(1 - alpha / 2)\n\n# Create data for the normal curve\nx <- seq(-4, 4, length = 1000)\ny <- dnorm(x)\n\ndf <- data.frame(x = x, y = y)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line(color = \"deepskyblue\", linewidth = 1) +\n  geom_area(data = subset(df, x <= -z_critical), aes(y = y), fill = \"firebrick\", alpha = 0.5) +\n  geom_area(data = subset(df, x >= z_critical), aes(y = y), fill = \"firebrick\", alpha = 0.5) +\n  geom_vline(xintercept = c(-z_critical, z_critical), linetype = \"dashed\", color = \"black\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](10_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' fig-alt='A normal distribution demonstrating the rejection regions.' width=960}\n:::\n:::\n\n\n\n# Decision Making\n\n## Decision Making\n\nHypothesis Testing will force you to make a decision: Reject $H_0$ **OR** Fail to Reject $H_0$\n\n::: fragment\nReject $H_0$: The effect seen is not due to random chance, there is a process contributing to the effect.\n:::\n\n::: fragment\nFail to Reject $H_0$: The effect seen is due to random chance. Random sampling is the reason why an effect is displayed, not an underlying process.\n:::\n\n## Decision Making: P-Value\n\nThe p-value approach is one of the most common methods to report significant results. It is easier to interpret the p-value because it provides the probability of observing our test statistics, or something more extreme, given that the null hypothesis is true.\n\n::: fragment\n**If** $p < \\alpha$, then you reject $H_0$; otherwise, you will fail to reject $H_0$.\n:::\n\n\n## Significance Level $\\alpha$\n\nThe significance level $\\alpha$ is the probability you will reject the null hypothesis given that it was true.\n\n::: fragment\nIn other words, $\\alpha$ is the error rate that a researcher controls.\n:::\n\n::: fragment\nTypically, we want this error rate to be small ($\\alpha = 0.05$).\n:::\n\n# Confidence Intervals\n\n## Confidence Intervals\n\n- A confidence interval gives a **range of plausible values** for a population parameter.\n- It reflects **uncertainty** in point estimates from sample data.\n\n::: notes\nIntroduce confidence intervals as the natural next step after understanding sampling variability and standard error. Emphasize that point estimates are useful, but intervals give a more complete picture.\n:::\n\n## Interpretation\n\n> \"We are 95% confident that the true mean lies between A and B.\"\n\n- This does **not** mean there's a 95% chance the mean is in that interval.\n- It means: if we repeated the sampling process many times, **95% of the intervals would contain the true value**.\n\n::: notes\nThis is one of the most common misconceptions. Clarify that the confidence is in the *method*, not any one interval.\n:::\n\n## Factors Affecting CI Width\n\n-   Sample size ($n$): larger $n$ → narrower CI\\\n-   Standard deviation ($s$ or $\\sigma$): more variability → wider CI\\\n-   Confidence level: higher confidence → wider CI\n\n::: notes\nUse this to summarize what controls how “precise” our confidence interval is. Give examples of each.\n:::\n\n## Decision Making: Confidence Interval Approach\n\nThe confidence interval approach can evaluate a hypothesis test where the alternative hypothesis is $\\beta\\ne\\beta^*$. The confidence interval approach will result in a lower and upper bound denoted as: $(LB, UB)$.\n\n::: fragment\n**If $\\beta^*$ is in $(LB, UB)$, then you fail to reject $H_0$. If $\\beta^*$ is not in $(LB,UB)$, then you reject $H_0$.**\n:::\n\n\n# Linear Regression Inference in R\n\n## Conducting HT of $\\beta_j$\n\n```r\nXLM <- lm(Y ~ X, data = DATA)\ntidy(XLM)\n```\n\n-   `XLM`: Object where the model is stored\n-   `Y`: Name of the outcome variable in `DATA`\n-   `X`: Name of the Predictor Variable(s) in `DATA`\n-   `DATA`: Name of the data set\n\n\n## Example \n\nIs there a significant relationship between penguin body mass (outcome; `body_mass`) and flipper length (predictor; `flipper_len`)? Use the `penguins` data set to determine a significant association.\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nm1 <- lm(body_mass ~ flipper_len, penguins)\ntidy(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)  -5781.     306.       -18.9 5.59e- 55\n#> 2 flipper_len     49.7      1.52      32.7 4.37e-107\n```\n\n\n:::\n:::\n\n\n## Confidence Interval\n\n```r\ntidy(XLM, conf.int = TRUE)\n```\n\n-   `XLM`: Object where the model is stored\n\n\n## X% Confidence Interval\n\n```r\ntidy(XLM, conf.int = TRUE, conf.level = X)\n```\n\n-   `XLM`: Object where the model is stored\n-   `X`: A number between 0 and 1 to specify confidence level\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ntidy(m1, conf.int = TRUE, conf.level = 0.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 7\n#>   term        estimate std.error statistic   p.value conf.low conf.high\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n#> 1 (Intercept)  -5781.     306.       -18.9 5.59e- 55  -6285.    -5276. \n#> 2 flipper_len     49.7      1.52      32.7 4.37e-107     47.2      52.2\n```\n\n\n:::\n:::\n\n\n# Linear Regression Example\n\n## Mammals Sleep Data Set\n\nThe [`msleep`](https://ggplot2.tidyverse.org/reference/msleep.html) data set contains information on sleeping patterns for mammals. We are interested in understanding the relationship of the length of sleep cycle (`sleep_cycle`; in hours) and rem sleep (`sleep_rem`; rapid eye movement; in hours).\n\n\n## Red Wine Data\n\nThe [Wine Quality](https://archive.ics.uci.edu/dataset/186/wine+quality) data set contains data on information on both red and white wine from North Portugal. We are interested in seeing if `pH` of the red wine (predictor variable) affects the `quality` (outcome variable).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nurl <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\nwine <- read_delim(url, delim = \";\")\n```\n:::\n\n\n# Logistic Regression Inference in R\n\n## Conducting HT of $\\beta_j$\n\n```r\nXLM <- glm(Y ~ X, data = DATA, family = binomial())\ntidy(XLM)\n```\n\n-   `XLM`: Object where the model is stored\n-   `Y`: Name of the outcome variable in `DATA`\n-   `X`: Name of the Predictor Variable(s) in `DATA`\n-   `DATA`: Name of the data set\n\n## Example\n\nIs there a significant association between heart disease (outcome; `disease`) and resting blood pressure (predictor; `trestbps`). Use the `heart_disease` data set to determine a significant association.\n\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nm1 <- glm(disease ~ trestbps, heart_disease, family = binomial())\ntidy(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)  -2.49     0.905       -2.76 0.00586\n#> 2 trestbps      0.0177   0.00681      2.61 0.00914\n```\n\n\n:::\n:::\n\n\n## Confidence Interval\n\n```r\ntidy(XLM, conf.int = TRUE, conf.level = LEVEL)\n```\n\n-   `XLM`: Object where the model is stored\n-   `LEVEL`: A number between 0 and 1 to specify confidence level\n  - defaults to 0.95\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ntidy(m1, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 7\n#>   term        estimate std.error statistic p.value conf.low conf.high\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n#> 1 (Intercept)  -2.49     0.905       -2.76 0.00586 -4.31      -0.747 \n#> 2 trestbps      0.0177   0.00681      2.61 0.00914  0.00461    0.0314\n```\n\n\n:::\n:::\n\n\n\n\n## Odds Ratio & Confidence Intervat \n\n```r\ntidy(XLM, exponentiate = TRUE, conf.int = TRUE)\n```\n\n-   `XLM`: Object where the model is stored\n\n\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ntidy(m1, exponentiate = TRUE, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 7\n#>   term        estimate std.error statistic p.value conf.low conf.high\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n#> 1 (Intercept)   0.0826   0.905       -2.76 0.00586   0.0135     0.474\n#> 2 trestbps      1.02     0.00681      2.61 0.00914   1.00       1.03\n```\n\n\n:::\n:::\n\n\n# Logistic Regression Example\n\n## Breast Cancer Data\n\nThe [Breast Cancer](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) data set contains information about image diagnosis of individuals from Wisconsin. We are interested if breast cancer `diagnosis` (outcome variable; Benign or Malignant), is affected by tumor `radius`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nurl <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\nbc <- read.csv(url, header = FALSE)\n\n# Add column names\ncolnames(bc) <- c(\"id\", \"diagnosis\", \"radius\", \"texture\", \"perimeter\", \"area\", \"smoothness\",\n                  \"compactness\", \"concavity\", paste0(\"V\", 10:32))\n\n# Convert diagnosis to factor\nbc$diagnosis <- factor(bc$diagnosis, levels = c(\"B\", \"M\"), labels = c(\"Benign\", \"Malignant\"))\n```\n:::\n\n\n## Bank Note Classification\n\nThe [Bank Note](https://archive.ics.uci.edu/dataset/267/banknote+authentication) data set contains information about bank note authentication based on images. We are interested in seeing if `class` (outcome variable; real or fake) is associated by image `entropy` (predictor).  \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nurl <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\nbank <- read.csv(url, header = FALSE)\n\ncolnames(bank) <- c(\"variance\", \"skewness\", \"curtosis\", \"entropy\", \"class\")\nbank$class <- factor(bank$class, levels = c(0, 1), labels = c(\"Genuine\", \"Forged\"))\n```\n:::\n",
    "supporting": [
      "10_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}