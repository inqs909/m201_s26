---
title: "Group Regression Models"
description: |
  Discuss using categorical predictor variables.

format:
  revealjs:
    scrollable: true
    theme: [default, styles.scss]
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: true
    eval: true
    message: false
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen
  - reveal-auto-agenda
editor: source
---

```{r}
#| include: false
library(tidyverse)
library(csucistats)
library(ThemePark)
library(palmerpenguins)
penguins <- penguins |> drop_na()
theme_set(theme_bw() + 
          theme(axis.text = element_text(size = 24),
                axis.title = element_text(size = 30),
                plot.title = element_text(size = 48),
                strip.text = element_text(size = 20),
                legend.title = element_blank(),
                legend.text = element_text(size = 24)))
```


# Modeling Relationships

## Explaining Variation

::: fragment
This is the process where we try to reduce the variation with the use of other variables.
:::

::: fragment
Can be thought of as getting it less wrong when taking an educated guess.
:::

## Explaining Variation

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass_g)) +
  geom_density() 

```

## Variation with One Variable

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass_g, fill = species)) +
  geom_density(alpha = .5)
```


# A Simple Model

## Generated Model

$$
Y \sim DGP_1
$$

## A Simple Model

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass_g)) +
  geom_density()
```

## A Simple Model

$$
Y = \_\_\_ + error
$$

## Notation

$$
Y = \ \ \ \ \ \ \ \ \ + \varepsilon 
$$

## The Simple Generated Model

$$
Y \sim \beta_0 + \varepsilon
$$

$$
\varepsilon \sim DGP_2
$$

::: fragment
$DGP_2$ is not the same as the $DGP_1$, it is transformed due $\beta_0$. Consider this the NULL $DGP$.
:::

## Observing Data

$$
Y = \beta_0 + \varepsilon
$$

## Estimated Line

$$
\hat Y=\hat\beta_0
$$

## Notation

::: columns
::: {.column width="50%"}
### Observed

$$
Y = \beta_0 + \varepsilon
$$
:::

::: {.column width="50%"}
### Estimated

$$
\hat Y = \hat \beta_0
$$
:::
:::

# Modelling Data

## Indexing Data

The data in a data set can be indexed by a number.

```{r}
penguins[1,-c(1:2)]
```

::: fragment
Making the variable "body_mass_g" be represented by $Y$ and "flipper_length_mm" as $X$:

$$
Y_1 = 3750 \ \ X_1=181
$$

:::

## Indexing Data

$$
Y_i, X_i
$$

## Data

With the data that we collect from a sample, we hypothesize how the data was generated.

::: fragment
Using a simple model:

$$
Y_i = \beta_0 + \varepsilon_i
$$

:::

## Estimated Value

$$
\hat Y_i = \hat \beta_0
$$

## Estimation 

To estimate $\hat \beta_0$, we minimize the follow function:

$$
\sum^n_{i=1} (Y_i-\hat Y_i)^2
$$

::: fragment
This is known as the sum squared errors, SSE 
:::


## Residuals

The residuals are known as the observed errors from the data in the model:

$$
r_i = Y_i - \hat Y_i
$$

## Estimation in R

```{r}
#| echo: true
#| eval: false

lm(Y ~ 1, data = DATA)
```


- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame

## Modeling Body Mass in Penguins

```{r}
lm(body_mass_g ~ 1, data = penguins)
```

::: fragment

$$
\hat Y = 4207
$$

:::


## Visualize

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass_g)) +
  geom_density() +
  geom_vline(xintercept = 4207)
```


# Linear Model

## Linear Model

The goal of Statistics is to develop models the have a better explanation of the outcome $Y$.

::: fragment
In particularly, reduce the sum of squared errors.
:::

::: fragment
By utilizing a bit more of information, $X$, we can increase the predicting capabilities of the model.
:::

::: fragment
Thus, the linear model is born.
:::

## Visualization

::: panel-tabset

### 1-Dimensional

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass_g)) +
  geom_density()
```

### 2-Dimensional

```{r}
#| code-fold: true
ggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, fill = after_stat(level))) +
  stat_density_2d(geom = "polygon")
```

:::

## Linear Model

$$
Y = \beta_0 + \beta_1 X + \varepsilon
$$

$$
\varepsilon \sim DGP_3
$$

## Scatter Plot

```{r}
#| code-fold: true
#| eval: true

ggplot(penguins, aes(flipper_length_mm, body_mass_g)) + 
  geom_point() 
```

## Imposing a Line

```{r}
#| code-fold: true
#| eval: true

ggplot(penguins, aes(flipper_length_mm, body_mass_g)) + 
  geom_point() +
  stat_smooth(method = "lm", se = F) 
```

## Modelling the Data

$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
$$

## Linear Model

$$
\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i
$$

::: fragment
Goal is to obtain numerical values for $\hat \beta_0$ and $\hat \beta_1$ that will minimize the SSE.
:::

## SSE

$$
\sum^n_{i=1} (Y_i-\hat Y_i)^2
$$


$$
\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i
$$

## Fitting a Model in R

```{r}
#| echo: true
#| eval: false

lm(Y ~ X, data = DATA)
```

- `X`: Name Predictor Variable of Interest in data frame `DATA`
- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame

## Example


Y: "body_mass_g"; X: "flipper_length_mm"

```{r}
lm(body_mass_g ~ flipper_length_mm, data = penguins)
```


$$
\hat Y_i = -5872.09 + 50.15 X_i
$$


## Interpretation of $\hat\beta_0$

The intercept $\hat \beta_0$ can be interpreted as the base value when $X$ is set to 0.

::: fragment
Some times the intercept can be interpretable to real world scenarios.
:::

::: fragment
Other times it cannot.
:::

## Interpreting Example

$$
\hat Y_i = -5872.09 + 50.15 X_i
$$

When flipper length is 0 mm, the body mass is -5872 grams.

## Interpretation of $\hat \beta_1$

The slope $\hat \beta_1$ indicates how will y change when x increases by 1 unit.

::: fragment
It will demonstrate if there is, on average, a positive or negative relationship based on the sign provided.
:::

## Interpreting Example

$$
\hat Y_i = -5872.09 + 50.15 X_i
$$

When flipper length increases by 1 mm, the body mass will increase by 50.15 grams.


# Categorical Variables

## Body Mass with Species

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass_g)) +
  geom_boxplot() 
```

## Body Mass with Species

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass_g, fill = species)) +
  geom_boxplot() 
```

## Group Statistics

We can use statistics to explain a continuous variable by the categories.

::: fragment
Compute statistics for each group.
:::

::: fragment

```{r}
#| eval: false
num_by_cat_stats(DATA, NUM, CAT)
```

- `NUM`: Name of the numerical variable
- `CAT`: Name of the categorical variable
- `DATA`: Name of the data frame


:::


## Compute Group Statistics

```{r}
num_by_cat_stats(penguins, body_mass_g, species)
```


## LM with Categorical Variables

A line is normally used to model 2 continuous variables.

::: fragment
However, the predictor variable $X$ can be restricted to a set a variables that can symbolize categories.
:::

::: fragment
A category will be used as a reference for a model.
:::


## Binary (Dummy) Variables

Binary variables are variable that can only take on the value 0 or 1.

$$
D_i = \left\{
\begin{array}{cc}
1 & Category\\
0 & Other
\end{array}
\right.
$$

## Binary (Dummy) Variables

To fit a model with categorical variables, we must utilize dummy (binary) variables that indicate which category is being referenced. We use $C-1$ dummy variables where $C$ indicates the number of categories. When coded correctly, each category will be represented by a combination of dummy variables.

## Example

If we have 4 categories, we will need 3 dummy variables:

|         | Cat 1 | Cat 2 | Cat 3 | Cat 4 |
|---------|-------|-------|-------|-------|
| Dummy 1 | 1     | 0     | 0     | 0     |
| Dummy 2 | 0     | 1     | 0     | 0     |
| Dummy 2 | 0     | 0     | 1     | 0     |


## Species Dummy Variables

|         | Chinstrap | Gentoo | Adelie |
|---------|-------|-------|-------|
| $D_1$ | 1     | 0     | 0     |
| $D_2$ | 0     | 1     | 0     |

## Linear Model

$$
\hat Y_i = \hat \beta_0 + \hat\beta_1 D_{1i} + \hat\beta_2 D_{2i}
$$

::: fragment
$\hat \beta_1$ indicates how body mass changes from Adelie to Chinstrap.
:::

::: fragment
$\hat \beta_2$ indicates how body mass changes from Adelie to Gentoo.
:::

::: fragment
$\hat \beta_0$ represents the baseline level, in this case the body mass of Adelie.
:::

## Fitting a Model in R

```{r}
#| eval: false

lm(Y ~ X, data = DATA)
```

- `X`: Name Predictor Variable of Interest in data frame `DATA`, must be a factor variable
- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame

## X not a Factor

```{r}
#| eval: false

lm(Y ~ factor(X), data = DATA)
```

- `X`: Name Predictor Variable of Interest in data frame `DATA`, not a factor variable
- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame


## Example

```{r}
lm(body_mass_g ~ species, penguins)
```

$$
\hat Y_i = 3706 + 26.92 D_{1i} + 1386.27 D_{2i}
$$

## Finding the Adelie MASS

$$
\hat Y_i = 3706 + 26.92 (0) + 1386.27 (0)
$$

## Finding the Chinstrap MASS

$$
\hat Y_i = 3706 + 26.92 (1) + 1386.27 (0)
$$

## Finding the Gentoo MASS

$$
\hat Y_i = 3706 + 26.92 (0) + 1386.27 (1)
$$

## Intepreting $\hat \beta_1$

On average, Chinstrap has a larger mass than Adelie by about 26.92 grams.

## Intepreting $\hat \beta_2$

On average, Gentoo has a larger mass than Adelie by about 1386.27 grams.

# Strength and Correlation

## Correlation

Correlation is a statistics that can be used to describe the strength of the relationship between 2 continuous variables.

::: fragment
$$
r = \frac{1}{n-1}\sum^n_{i=1}\frac{x_i - \bar x}{s_x}\frac{y_i - \bar y}{s_y}
$$

-   $\bar x$, $\bar y$: sample means
-   $s_x$, $s_y$: sample standard deviations

:::

::: fragment
$$
-1 \leq r \leq 1
$$
:::


## Correlations

![From IMS 2e](https://openintro-ims.netlify.app/model-slr_files/figure-html/fig-posNegCorPlots-1.png)

## Coefficient of Determination

The coefficient of determination evaluates the strength between an outcome $Y$ and the linear model, which includes $X$.

::: fragment
$$
R^2 = r^2
$$
:::

::: fragment
$$
0 \leq R^2 \leq 1
$$
:::

::: fragment
The coefficient of determination measures the total variation explained by the linear model. The closer to 1, the better the linear model.
:::

## Correlation in R

```{r}
#| eval: false
cor(DATA$Y, DATA$X)
```

- `X`: Name Predictor Variable of Interest in data frame `DATA`
- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame

## Example

```{r}
cor(penguins$body_mass_g, penguins$flipper_length_mm)
```

## Coefficient of Determination in R

```{r}
#| eval: false
xlm <- lm(Y ~ X, data = DATA)
r2(xlm)
```

- `X`: Name Predictor Variable of Interest in data frame `DATA`
- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame


## Example

```{r}
xlm <- lm(body_mass_g ~ species, penguins)
r2(xlm)
```

# Prediction

## Statistical Model

$$
\hat Y = \hat \beta_0 + \hat \beta_1 X
$$

- $X$: Input
- $\hat Y$: Output

## Prediction

Using the equation $\hat Y$, we can give it a value of $X$ and then, in return, a value of $\hat Y$ that predicts the true value $Y$.

## Prediction in R

```{r}
#| echo: true
#| eval: false

xlm <- lm(Y ~ X,
            data = DATA)

predict_df <- data.frame(X = VAL)

predict(xlm,
        predict_df)
```
- `X`: Name Predictor Variable of Interest in data frame `DATA`
- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame
- `VAL`: Value for the Predictor Variable


## Example 1

::: panel-tabset

### Example

Predict the body mass for a gentoo penguin.

### Code

```{r}
xlm <- lm(body_mass_g ~ species,
            data = penguins)

xlm

predict_df <- data.frame(species = "Gentoo")

predict(xlm,
        predict_df)
```

:::

## Example 2


::: panel-tabset

### Example

Predict the body mass for a penguin with a flipper length of 190.

### Code

```{r}
xlm <- lm(body_mass_g ~ flipper_length_mm,
            data = penguins)


xlm

predict_df <- data.frame(flipper_length_mm = 190)

predict(xlm,
        predict_df)
```

:::

## Interpolation

Interpolation is the process of estimating a value within the range of the observed input data $X$. 

## Extrapolation

Extrapolation is the process of estimating a value beyond the range of observed input data $X$. It's about venturing into the unknown, using what we know as a guide.

## Extrapolation

```{r}
#| code-fold: true
#| eval: true

ggplot(penguins, aes(flipper_length_mm, body_mass_g)) + 
  xlim(160, 250) +
  ylim(2600, 7000) +
  geom_point() +
  stat_smooth(method = "lm", se = F) 
```



