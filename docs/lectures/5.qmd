---
title: |
  Simple <br>
  Linear Regression
description: |
  Begins the discussion for linear regression.

format:
  revealjs:
    width: 1200
    scrollable: true
    sc-sb-title: true
    footer: m201.inqs.info/lectures/5
    theme: [default, styles.scss]
    navigation-mode: vertical
    controls-layout: edges
    controls-tutorial: true
    slide-number: true
    pointer:
      pointerSize: 32
    incremental: false 
    chalkboard:
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: true
    eval: true
    code-fold: true
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  
filters: 
  - reveal-header
  - code-fullscreen
  - reveal-auto-agenda

editor: source
---

## R Packages

```{r}
#| include: false
#| message: false
library(tidyverse)
library(rcistats)
penguins <- penguins |> drop_na()
theme_set(theme_bw() + 
          theme(axis.text = element_text(size = 24),
                axis.title = element_text(size = 30),
                plot.title = element_text(size = 48),
                strip.text = element_text(size = 20),
                legend.title = element_blank(),
                legend.text = element_text(size = 24)))
```


-   rcistats
-   tidyverse


## Palmer Penguins Data

::: {.columns}
::: {.column}

### Variables of Interest

- `flipper_len`: Flipper Length in [millimeters](https://en.wikipedia.org/wiki/Millimetre)
- `body_mass`: Body mass in [grams](https://en.wikipedia.org/wiki/Gram)

:::
::: {.column}
![Artwork by @allison_horst](img/penguins.png){fig-alt="An image of several penguins in Antartica."}

:::
::: 

## Variables of Interest




# Modeling Relationships

## Explaining Variation

::: fragment
This is the process where we try to reduce the variation with the use of other variables.
:::

::: fragment
Can be thought of as getting it less wrong when taking an educated guess.
:::

## Modeling Variation

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass)) +
  geom_density() 

```


## Modeling Variation with $\bar X$

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass)) +
  geom_density() +
  geom_vline(xintercept = mean(penguins$body_mass))

```


## Modeling with a Numerical Variable

```{r}
#| code-fold: true
ggplot(penguins, aes(x = flipper_len, y = body_mass)) +
  geom_point() +
  stat_density_2d(aes(fill = after_stat(level))) +
  xlim(c(170, 236)) +
  ylim(c(2500, 6250))
```

## Modeling with a Numerical Variable

```{r}
#| code-fold: true
ggplot(penguins, aes(x = flipper_len, y = body_mass)) +
  geom_point() +
  stat_smooth(method = "lm", se = F, col = "red") +
  stat_density_2d(aes(fill = after_stat(level))) +
  xlim(c(170, 236)) +
  ylim(c(2500, 6250))
```



# A Simple Model

## Generated Model

$$
Y \sim DGP_1
$$

## A Simple Model

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass)) +
  geom_density()
```

## A Simple Model

$$
Y = \_\_\_ + error
$$

## Notation

$$
Y = \ \ \ \ \ \ \ \ \ + \varepsilon 
$$

## The Simple Generated Model

$$
Y \sim \beta_0 + \varepsilon
$$

$$
\varepsilon \sim DGP_2
$$

::: fragment
$DGP_2$ is not the same as the $DGP_1$, it is transformed due $\beta_0$. Consider this the NULL $DGP$.
:::

## Observing Data

$$
Y = \beta_0 + \varepsilon
$$

## Estimated Line

$$
\hat Y=\hat\beta_0
$$

## Notation

::: columns
::: {.column width="50%"}
### Observed

$$
Y = \beta_0 + \varepsilon
$$
:::

::: {.column width="50%"}
### Estimated

$$
\hat Y = \hat \beta_0
$$
:::
:::

# Modelling Data

## Indexing Data

The data in a data set can be indexed by a number.

```{r}
penguins[1,-c(1:2)]
```

::: fragment
Making the variable `body_mass` be represented by $Y$ and `flipper_len` as $X$:

$$
Y_1 = 3750 \ \ X_1=181
$$

:::

## Indexing Data

$$
Y_i, X_i
$$

## Data

With the data that we collect from a sample, we hypothesize how the data was generated.

::: fragment
Using a simple model:

$$
Y_i = \beta_0 + \varepsilon_i
$$

:::

## Estimated Value

$$
\hat Y_i = \hat \beta_0
$$

## Estimation 

To estimate $\hat \beta_0$, we minimize the follow function:

$$
\sum^n_{i=1} (Y_i-\hat Y_i)^2
$$

::: fragment
This is known as the sum squared errors, SSE 
:::


## Residuals

The residuals are known as the observed errors from the data in the model:

$$
r_i = Y_i - \hat Y_i
$$

## Estimation in R

```{r}
#| echo: true
#| eval: false

lm(Y ~ 1, data = DATA)
```


- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame

## Modeling Body Mass in Penguins

```{r}
xlm <- lm(body_mass ~ 1, data = penguins)
xlm
```

::: fragment

$$
\hat Y = 4207
$$

:::


## Visualize

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass)) +
  geom_density() +
  geom_vline(xintercept = 4207)
```


# Linear Model

## Linear Model

The goal of Statistics is to develop models the have a better explanation of the outcome $Y$.

::: fragment
In particularly, reduce the sum of squared errors.
:::

::: fragment
By utilizing a bit more of information, $X$, we can increase the predicting capabilities of the model.
:::

::: fragment
Thus, the linear model is born.
:::

## Visualization

::: panel-tabset

### 1-Dimensional

```{r}
#| code-fold: true
ggplot(penguins, aes(body_mass)) +
  geom_density()
```

### 2-Dimensional

```{r}
#| code-fold: true
ggplot(penguins, aes(x = flipper_len, y = body_mass)) +
  geom_point() +
  stat_density_2d(aes(fill = after_stat(level))) +
  xlim(c(170, 236)) +
  ylim(c(2500, 6250))
```

:::

## Linear Model

$$
Y = \beta_0 + \beta_1 X + \varepsilon
$$

$$
\varepsilon \sim DGP_3
$$

## Scatter Plot

```{r}
#| code-fold: true
#| eval: true

ggplot(penguins, aes(flipper_len, body_mass)) + 
  geom_point() 
```

## Imposing a Line

```{r}
#| code-fold: true
#| eval: true

ggplot(penguins, aes(flipper_len, body_mass)) + 
  geom_point() +
  stat_smooth(method = "lm", se = F) 
```

## Modelling the Data

$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
$$

## Linear Model

$$
\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i
$$

::: fragment
Goal is to obtain numerical values for $\hat \beta_0$ and $\hat \beta_1$ that will minimize the SSE.
:::

## SSE

$$
\sum^n_{i=1} (Y_i-\hat Y_i)^2
$$


$$
\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i
$$

## Fitting a Model in R

```{r}
#| echo: true
#| eval: false

xlm <- lm(Y ~ X, data = DATA)
xlm
```

- `X`: Name Predictor Variable of Interest in data frame `DATA`
- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame

## Example


Y: `body_mass`; X: `flipper_len`

```{r}
xlm <- lm(body_mass ~ flipper_len, data = penguins)
xlm
```


$$
\hat Y_i = -5872.09 + 50.15 X_i
$$


## Interpretation of $\hat\beta_0$

The intercept $\hat \beta_0$ can be interpreted as the base value when $X$ is set to 0.

::: fragment
Some times the intercept can be interpretable to real world scenarios.
:::

::: fragment
Other times it cannot.
:::

## Interpreting Example

$$
\hat Y_i = -5872.09 + 50.15 X_i
$$

When flipper length is 0 mm, the body mass is -5872 grams.

## Interpretation of $\hat \beta_1$

The slope $\hat \beta_1$ indicates how will y change when x increases by 1 unit.

::: fragment
It will demonstrate if there is, on average, a positive or negative relationship based on the sign provided.
:::

## Interpreting Example

$$
\hat Y_i = -5872.09 + 50.15 X_i
$$

When flipper length increases by 1 mm, the body mass will increase by 50.15 grams.


# Prediction

## Statistical Model

$$
\hat Y = \hat \beta_0 + \hat \beta_1 X
$$

- $X$: Input
- $\hat Y$: Output

## Prediction

Using the equation $\hat Y$, we can give it a value of $X$ and then, in return, a value of $\hat Y$ that predicts the true value $Y$.

## Prediction in R

```{r}
#| echo: true
#| eval: false

xlm <- lm(Y ~ X,
            data = DATA)

predict_df <- data.frame(X = VAL)

predict(xlm,
        predict_df)
```

- `X`: Name Predictor Variable of Interest in data frame `DATA`
- `Y`: Name Outcome Variable of Interest in data frame `DATA`
- `DATA`: Name of the data frame
- `VAL`: Value for the Predictor Variable


## Example 1

::: panel-tabset

### Example

Predict the body mass for a penguin with a flipper length of 185.

### Code

```{r}
xlm <- lm(body_mass ~ flipper_len,
            data = penguins)

xlm

predict_df <- data.frame(flipper_len = 185)

predict(xlm,
        predict_df)
```

:::

## Example 2


::: panel-tabset

### Example

Predict the body mass for a penguin with a flipper length of 205.

### Code

```{r}
xlm <- lm(body_mass ~ flipper_len,
            data = penguins)


xlm

predict_df <- data.frame(flipper_len = 190)

predict(xlm,
        predict_df)
```

:::

## Interpolation

Interpolation is the process of estimating a value within the range of the observed input data $X$. 

## Extrapolation

Extrapolation is the process of estimating a value beyond the range of observed input data $X$. It's about venturing into the unknown, using what we know as a guide.

## Extrapolation

```{r}
#| code-fold: true
#| eval: true

ggplot(penguins, aes(flipper_len, body_mass)) + 
  xlim(160, 250) +
  ylim(2600, 7000) +
  geom_point() +
  stat_smooth(method = "lm", se = F) 
```



