[
  {
    "objectID": "lectures/10.html#what-is-statistical-inference",
    "href": "lectures/10.html#what-is-statistical-inference",
    "title": "Statistical Inference",
    "section": "What is Statistical Inference?",
    "text": "What is Statistical Inference?\n\nDrawing conclusions about a population based on a sample\nPopulation = entire group\nSample = subset\n\n\nIntroduce the big idea: We want to make st"
  },
  {
    "objectID": "lectures/10.html#two-main-types-of-inference",
    "href": "lectures/10.html#two-main-types-of-inference",
    "title": "Statistical Inference",
    "section": "Two Main Types of Inference",
    "text": "Two Main Types of Inference\n\nEstimation\nHypothesis Testing\n\n\nWe’ll be focusing on two fundamental techniques in inference. First, estimating population values (like the mean), and second, testing claims about the population."
  },
  {
    "objectID": "lectures/10.html#estimation",
    "href": "lectures/10.html#estimation",
    "title": "Statistical Inference",
    "section": "Estimation",
    "text": "Estimation\n\nPoint Estimate: Single best guess (e.g., \\(\\hat \\beta_1\\))\nInterval Estimate: Range likely to contain the true value\n\n\nPoint estimates are easy but not very informative. Intervals give us a sense of uncertainty, which is critical in inference."
  },
  {
    "objectID": "lectures/10.html#hypothesis-testing",
    "href": "lectures/10.html#hypothesis-testing",
    "title": "Statistical Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(H_0\\): No effect or difference\n\n\\(H_1\\): Some effect or difference\n\nWe use sample data to support or reject \\(H_0\\)\n\n\nMention that \\(H_0\\) is the default assumption. We only reject it if the data give us strong enough evidence."
  },
  {
    "objectID": "lectures/10.html#key-concepts-and-tools",
    "href": "lectures/10.html#key-concepts-and-tools",
    "title": "Statistical Inference",
    "section": "Key Concepts and Tools",
    "text": "Key Concepts and Tools\n\nSampling Distribution\nCentral Limit Theorem\nStandard Error\n\n\nThese three concepts are foundational. Understanding them helps us assess how reliable our estimates are."
  },
  {
    "objectID": "lectures/10.html#p-values",
    "href": "lectures/10.html#p-values",
    "title": "Statistical Inference",
    "section": "p-values",
    "text": "p-values\n\nProbability of observing data as extreme as this if \\(H_0\\) is true\n\nMisinterpretation of p-values is common. Emphasize: low p-value means data is unusual under \\(H_0\\)."
  },
  {
    "objectID": "lectures/10.html#confidence-intervals",
    "href": "lectures/10.html#confidence-intervals",
    "title": "Statistical Inference",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nA range where we expect the true value to fall\n\n\nClarify interpretation: it’s not about the probability the parameter is inside the interval, but about the method producing accurate intervals in the long run."
  },
  {
    "objectID": "lectures/10.html#hypothesis-tests",
    "href": "lectures/10.html#hypothesis-tests",
    "title": "Statistical Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nHypothesis tests are used to test whether claims are valid or not. This is conducted by collecting data, setting the Null and Alternative Hypothesis."
  },
  {
    "objectID": "lectures/10.html#null-hypothesis-h_0",
    "href": "lectures/10.html#null-hypothesis-h_0",
    "title": "Statistical Inference",
    "section": "Null Hypothesis \\(H_0\\)",
    "text": "Null Hypothesis \\(H_0\\)\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value."
  },
  {
    "objectID": "lectures/10.html#alternative-hypothesis-h_a",
    "href": "lectures/10.html#alternative-hypothesis-h_a",
    "title": "Statistical Inference",
    "section": "Alternative Hypothesis \\(H_a\\)",
    "text": "Alternative Hypothesis \\(H_a\\)\nThe alternative hypothesis contradicts the null hypothesis."
  },
  {
    "objectID": "lectures/10.html#example-of-null-and-alternative-hypothesis",
    "href": "lectures/10.html#example-of-null-and-alternative-hypothesis",
    "title": "Statistical Inference",
    "section": "Example of Null and Alternative Hypothesis",
    "text": "Example of Null and Alternative Hypothesis\nWe want to see if \\(\\beta\\) is different from \\(\\beta^*\\)\n\n\n\nNull Hypothesis\nAlternative Hypothesis\n\n\n\n\n\\(H_0: \\beta=\\beta^*\\)\n\\(H_a: \\beta\\ne\\beta^*\\)\n\n\n\\(H_0: \\beta\\le\\beta^*\\)\n\\(H_a: \\beta&gt;\\beta^*\\)\n\n\n\\(H_0: \\beta\\ge\\beta^*\\)\n\\(H_0: \\beta&lt;\\beta^*\\)"
  },
  {
    "objectID": "lectures/10.html#one-side-vs-two-side-hypothesis-tests",
    "href": "lectures/10.html#one-side-vs-two-side-hypothesis-tests",
    "title": "Statistical Inference",
    "section": "One-Side vs Two-Side Hypothesis Tests",
    "text": "One-Side vs Two-Side Hypothesis Tests\nNotice how there are 3 types of null and alternative hypothesis, The first type of hypothesis (\\(H_a:\\beta\\ne\\beta^*\\)) is considered a 2-sided hypothesis because the rejection region is located in 2 regions. The remaining two hypotheses are considered 1-sided because the rejection region is located on one side of the distribution.\n\n\n\nNull Hypothesis\nAlternative Hypothesis\nSide\n\n\n\n\n\\(H_0: \\beta=\\beta^*\\)\n\\(H_a: \\beta\\ne\\beta^*\\)\n2-Sided\n\n\n\\(H_0: \\beta\\le\\beta^*\\)\n\\(H_a: \\beta&gt;\\beta^*\\)\n1-Sided\n\n\n\\(H_0: \\beta\\ge\\beta^*\\)\n\\(H_0: \\beta&lt;\\beta^*\\)\n1-Sided"
  },
  {
    "objectID": "lectures/10.html#hypothesis-testing-steps",
    "href": "lectures/10.html#hypothesis-testing-steps",
    "title": "Statistical Inference",
    "section": "Hypothesis Testing Steps",
    "text": "Hypothesis Testing Steps\n\nState \\(H_0\\) and \\(H_1\\)\nChoose \\(\\alpha\\)\nCompute confidence interval/p-value\nMake a decision\n\n\nWalk through the steps slowly with an example in mind. Emphasize that \\(\\alpha\\) is a threshold, not the actual probability of error."
  },
  {
    "objectID": "lectures/10.html#rejection-region",
    "href": "lectures/10.html#rejection-region",
    "title": "Statistical Inference",
    "section": "Rejection Region",
    "text": "Rejection Region"
  },
  {
    "objectID": "lectures/10.html#rejection-region-1",
    "href": "lectures/10.html#rejection-region-1",
    "title": "Statistical Inference",
    "section": "Rejection Region",
    "text": "Rejection Region\n\n\nCode\nalpha &lt;- 0.05\n\n# Critical values for two-tailed test\nz_critical &lt;- qnorm(1 - alpha / 2)\n\n# Create data for the normal curve\nx &lt;- seq(-4, 4, length = 1000)\ny &lt;- dnorm(x)\n\ndf &lt;- data.frame(x = x, y = y)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line(color = \"deepskyblue\", size = 1) +\n  geom_area(data = subset(df, x &lt;= -z_critical), aes(y = y), fill = \"firebrick\", alpha = 0.5) +\n  geom_area(data = subset(df, x &gt;= z_critical), aes(y = y), fill = \"firebrick\", alpha = 0.5) +\n  geom_vline(xintercept = c(-z_critical, z_critical), linetype = \"dashed\", color = \"black\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/10.html#decision-making-1",
    "href": "lectures/10.html#decision-making-1",
    "title": "Statistical Inference",
    "section": "Decision Making",
    "text": "Decision Making\nHypothesis Testing will force you to make a decision: Reject \\(H_0\\) OR Fail to Reject \\(H_0\\)\n\nReject \\(H_0\\): The effect seen is not due to random chance, there is a process making contributing to the effect.\n\n\nFail to Reject \\(H_0\\): The effect seen is due to random chance. Random sampling is the reason why an effect is displayed, not an underlying process."
  },
  {
    "objectID": "lectures/10.html#decision-making-p-value",
    "href": "lectures/10.html#decision-making-p-value",
    "title": "Statistical Inference",
    "section": "Decision Making: P-Value",
    "text": "Decision Making: P-Value\nThe p-value approach is one of the most common methods to report significant results. It is easier to interpret the p-value because it provides the probability of observing our test statistics, or something more extreme, given that the null hypothesis is true.\n\nIf \\(p &lt; \\alpha\\), then you reject \\(H_0\\); otherwise, you will fail to reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/10.html#significance-level-alpha",
    "href": "lectures/10.html#significance-level-alpha",
    "title": "Statistical Inference",
    "section": "Significance Level \\(\\alpha\\)",
    "text": "Significance Level \\(\\alpha\\)\nThe significance level \\(\\alpha\\) is the probability you will reject the null hypothesis given that it was true.\n\nIn other words, \\(\\alpha\\) is the error rate that a research controls.\n\n\nTypically, we want this error rate to be small (\\(\\alpha = 0.05\\))."
  },
  {
    "objectID": "lectures/10.html#confidence-intervals-2",
    "href": "lectures/10.html#confidence-intervals-2",
    "title": "Statistical Inference",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nA confidence interval gives a range of plausible values for a population parameter.\nIt reflects uncertainty in point estimates from sample data.\n\n\nIntroduce confidence intervals as the natural next step after understanding sampling variability and standard error. Emphasize that point estimates are useful, but intervals give a more complete picture."
  },
  {
    "objectID": "lectures/10.html#interpretation",
    "href": "lectures/10.html#interpretation",
    "title": "Statistical Inference",
    "section": "Interpretation",
    "text": "Interpretation\n\n“We are 95% confident that the true mean lies between A and B.”\n\n\nThis does not mean there’s a 95% chance the mean is in that interval.\nIt means: if we repeated the sampling process many times, 95% of the intervals would contain the true value.\n\n\nThis is one of the most common misconceptions. Clarify that the confidence is in the method, not any one interval."
  },
  {
    "objectID": "lectures/10.html#factors-affecting-ci-width",
    "href": "lectures/10.html#factors-affecting-ci-width",
    "title": "Statistical Inference",
    "section": "Factors Affecting CI Width",
    "text": "Factors Affecting CI Width\n\nSample size (\\(n\\)): larger \\(n\\) → narrower CI\n\nStandard deviation (\\(s\\) or \\(\\sigma\\)): more variability → wider CI\n\nConfidence level: higher confidence → wider CI\n\n\nUse this to summarize what controls how “precise” our confidence interval is. Give examples of each."
  },
  {
    "objectID": "lectures/10.html#decision-making-confidence-interval-approach",
    "href": "lectures/10.html#decision-making-confidence-interval-approach",
    "title": "Statistical Inference",
    "section": "Decision Making: Confidence Interval Approach",
    "text": "Decision Making: Confidence Interval Approach\nThe confidence interval approach can evaluate a hypothesis test where the alternative hypothesis is \\(\\beta\\ne\\beta^*\\). The confidence interval approach will result in a lower and upper bound denoted as: \\((LB, UB)\\).\n\nIf \\(\\beta^*\\) is in \\((LB, UB)\\), then you fail to reject \\(H_0\\). If \\(\\beta^*\\) is not in \\((LB,UB)\\), then you reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/10.html#conducting-ht-of-beta_j",
    "href": "lectures/10.html#conducting-ht-of-beta_j",
    "title": "Statistical Inference",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\nxlm &lt;- lm(Y ~ X, data = DATA)\nsummary(xlm)\n\nxlm: Object where the model is stored\nY: Name of the outcome variable in DATA\nX: Name of the Predictor Variable(s) in DATA\nDATA: Name of the data set"
  },
  {
    "objectID": "lectures/10.html#example",
    "href": "lectures/10.html#example",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\nIs there a significant relationship between penguin body mass (outcome; body_mass) and flipper length (predictor; flipper_len)? Use the penguins data set to determine a significant association."
  },
  {
    "objectID": "lectures/10.html#example-1",
    "href": "lectures/10.html#example-1",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- lm(body_mass ~ flipper_len, penguins)\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass ~ flipper_len, data = penguins)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1057.33  -259.79   -12.24   242.97  1293.89 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -5872.09     310.29  -18.93   &lt;2e-16 ***\n#&gt; flipper_len    50.15       1.54   32.56   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 393.3 on 331 degrees of freedom\n#&gt; Multiple R-squared:  0.7621, Adjusted R-squared:  0.7614 \n#&gt; F-statistic:  1060 on 1 and 331 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/10.html#confidence-interval",
    "href": "lectures/10.html#confidence-interval",
    "title": "Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nconfint(xlm, level = LEVEL)\n\nxlm: Name of the model saved in R\nLEVEL: A number between 0 and 1 to specify confidence level"
  },
  {
    "objectID": "lectures/10.html#example-2",
    "href": "lectures/10.html#example-2",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nconfint(m1, level = 0.90)\n\n\n#&gt;                    5 %        95 %\n#&gt; (Intercept) -6383.8988 -5360.28658\n#&gt; flipper_len    47.6127    52.69383"
  },
  {
    "objectID": "lectures/10.html#red-wine-data",
    "href": "lectures/10.html#red-wine-data",
    "title": "Statistical Inference",
    "section": "Red Wine Data",
    "text": "Red Wine Data\nThe Wine Quality data set contains data on information on both red and white wine from North Portugal. We are interested in seeing if pH of the red wine (predictor variable) affects the quality (outcome variable).\n\n\nCode\nurl &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\nwine &lt;- read_delim(url, delim = \";\")\n\nlm(quality ~ pH, wine) |&gt; summary()"
  },
  {
    "objectID": "lectures/10.html#conducting-ht-of-beta_j-1",
    "href": "lectures/10.html#conducting-ht-of-beta_j-1",
    "title": "Statistical Inference",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial())\nsummary(xlm)\n\nxlm: Object where the model is stored\nY: Name of the outcome variable in DATA\nX: Name of the Predictor Variable(s) in DATA\nDATA: Name of the data set"
  },
  {
    "objectID": "lectures/10.html#example-3",
    "href": "lectures/10.html#example-3",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\nIs there a significant association between heart disease (outcome; disease) and resting blood pressure (predictor; trestbps). Use the heart_disease data set to determine a significant association."
  },
  {
    "objectID": "lectures/10.html#example-4",
    "href": "lectures/10.html#example-4",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(disease ~ trestbps, heart_disease, family = binomial())\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = disease ~ trestbps, family = binomial(), data = heart_disease)\n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)   \n#&gt; (Intercept) -2.494132   0.905112  -2.756  0.00586 **\n#&gt; trestbps     0.017745   0.006807   2.607  0.00914 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 409.95  on 296  degrees of freedom\n#&gt; Residual deviance: 402.88  on 295  degrees of freedom\n#&gt; AIC: 406.88\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/10.html#confidence-interval-1",
    "href": "lectures/10.html#confidence-interval-1",
    "title": "Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nconfint(xlm, level = LEVEL)\n\nxlm: Name of the model saved in R\nLEVEL: A number between 0 and 1 to specify confidence level"
  },
  {
    "objectID": "lectures/10.html#example-5",
    "href": "lectures/10.html#example-5",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nconfint(m1, level = 0.95)\n\n\n#&gt;                   2.5 %      97.5 %\n#&gt; (Intercept) -4.30710793 -0.74740574\n#&gt; trestbps     0.00461029  0.03139019"
  },
  {
    "objectID": "lectures/10.html#confidence-interval-for-odds-ratio",
    "href": "lectures/10.html#confidence-interval-for-odds-ratio",
    "title": "Statistical Inference",
    "section": "Confidence Interval for Odds Ratio",
    "text": "Confidence Interval for Odds Ratio\nexp(confint(xlm, level = LEVEL))\n\nxlm: Name of the model saved in R\nLEVEL: A number between 0 and 1 to specify confidence level"
  },
  {
    "objectID": "lectures/10.html#example-6",
    "href": "lectures/10.html#example-6",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nexp(confint(m1, level = 0.95))\n\n\n#&gt;                  2.5 %    97.5 %\n#&gt; (Intercept) 0.01347246 0.4735936\n#&gt; trestbps    1.00462093 1.0318881"
  },
  {
    "objectID": "lectures/10.html#breast-cancer-data",
    "href": "lectures/10.html#breast-cancer-data",
    "title": "Statistical Inference",
    "section": "Breast Cancer Data",
    "text": "Breast Cancer Data\nThe Breast Cancer data set contains information about image diagnosis of individuals from Wisconsin. We are interested if breast cancer diagnosis (outcome variable; Benign or Malignant), is affected by tumor radius.\n\n\nCode\nurl &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\nbc &lt;- read.csv(url, header = FALSE)\n\n# Add column names\ncolnames(bc) &lt;- c(\"id\", \"diagnosis\", \"radius\", \"texture\", \"perimeter\", \"area\", \"smoothness\",\n                  \"compactness\", \"concavity\", paste0(\"V\", 10:32))\n\n# Convert diagnosis to factor\nbc$diagnosis &lt;- factor(bc$diagnosis, levels = c(\"B\", \"M\"), labels = c(\"Benign\", \"Malignant\"))"
  },
  {
    "objectID": "lectures/10.html#bank-note-classification",
    "href": "lectures/10.html#bank-note-classification",
    "title": "Statistical Inference",
    "section": "Bank Note Classification",
    "text": "Bank Note Classification\nThe Bank Note data set contains information about bank note authentication based on images. We are interested in seeing if class (outcome variable; real or fake) is associated by image entropy (predictor).\n\n\nCode\nurl &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\nbank &lt;- read.csv(url, header = FALSE)\n\ncolnames(bank) &lt;- c(\"variance\", \"skewness\", \"curtosis\", \"entropy\", \"class\")\nbank$class &lt;- factor(bank$class, levels = c(0, 1), labels = c(\"Genuine\", \"Forged\"))"
  },
  {
    "objectID": "lectures/10.html#learning-outcomes",
    "href": "lectures/10.html#learning-outcomes",
    "title": "Statistical Inference",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes"
  }
]