[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Spring 2026\nInstructor: Isaac Quintanilla Salinas\nEmail: isaac.qs@csuci.edu\nOffice Location: Marin Hall 2326\nOffice Hours:\nLecture: Gateway Hall 2501 4:30 - 5:45 PM\nWebsite: Canvas (CI Learn)\n\n\n\nCritical reasoning using a quantitative and statistical, problem-solving approach to solving real-world problems. Topics include: probability and statistics, sample data, probability and empirical data distributions, sampling techniques, estimation and hypothesis testing, ANOVA, and correlation and regression analysis. Students will use standard statistical software to analyze real-world and simulated data. GenEd: 2\n\n\n\n\nPrepare students for advanced courses in data-management and statistics\nReason both inductively and deductively with quantitative information and data\nUse statistical software for complex statistical analysis of real-world and simulated data\nEmpower students to apply computational and inferential thinking to address real-world problems\nWrite the results of a statistical study and draw conclusion in reports\n\n\n\n\nIntroduction to Modern Statistics (IMS)\nStatistical Modeling (SM)\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nParticipation\n5%\n\n\nClasswork\n10%\n\n\nVideo Assignments\n20%\n\n\nNotebook Assignments\n20%\n\n\nExam 1\n15%\n\n\nExam 2\n15%\n\n\nExam 3\n15%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\n\nStudents are expected to attend in-person to class to learn the material.\n\n\n\nStudents are expected to check the course Canvas page 3-4 times a week to view assignments, announcements, and other course-related materials.\n\n\nParticipation is based on short writing assignments conducted in class. There will be no make ups for these writing prompts.\n\n\n\nClasswork assignmets are designed to give you an opportunity to practice conceptual topics from the video assignments. The assignments will require you to program in R. The 3 lowest homework assignments will be dropped. Homework assignments will be due every Friday at 11:59 PM.\n\n\n\nNotebook assignments are designed to expand your statistical knowledge. These will be completed in Google Colab which can be accessed from Canvas. There is one notebook assignments every week that you can be completed during class time. Notebook assignments will be due on Thursday at 11:59 PM every week. The 3 lowest notebook assignments will be dropped.\n\n\n\nVideos are used to teach statistical concepts related to the course. Students are expected to watch at least one video a week. The videos are implemented using VoiceThreads. The 3 lowest video assignments will be dropped. Video assignments will be due every Sunday at 11:59 PM.\n\n\n\nThere will be three in exams. Exam #1 will be on September 29, 2025, Exam #2 will be on November 3, and Exam #3 will be on December 10, 2025 from 4-6 PM. While the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success. At the end of the semester, your lowest exam grade will be replaced by your median average of all 3 exam grades. This course will operate under a zero-tolerance policy. Talking during the time of the exam, sharing materials, looking at another students’ exam, or not following directions given will be subject to the University’s academic integrity policy.\n\n\n\nThere will be 4 extra credit opportunities worth a total of 10% of your overall grade. (There are no make-ups for missed extra credit assignments!) More information will be provided on the extra credit assignments on a later date. Information on the extra credit can be found here.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\nWeek\nTopic\nNB Due\nVideo Due\n\n\n\n\n1/26\nWelcome/Intro to Stats and R\n1\n1\n\n\n2/2\nHoliday/ Data Generating Process\n2\n2\n\n\n2/9\nCategorical Data\n3\n3\n\n\n2/16\nNumerical Data\n4\n4\n\n\n2/23\nDistribution Functions\n5\n\n\n\n3/2\nExam 1/ Linear Regression\n\n5\n\n\n3/9\nSimple Linear Regression\n6\n6\n\n\n3/16\nSpring Break\n\n\n\n\n3/23\nSimple Logistic Regression\n7\n7\n\n\n3/30\nHoliday/ Group Models\n8\n8\n\n\n4/6\nMultivariable Regression\n9\n\n\n\n4/13\nModeling Approaches/Exam 2\n\n9\n\n\n4/20\nSampling Distributions\n10\n10\n\n\n4/27\nInference\n11\n11\n\n\n5/4\nInference\n12\n12\n\n\n5/11\nInference\n13\n\n\n\n5/18\nExam 3\n\n\n\n\n\n\n\n\nThe use of generative artificial intelligence (AI) in an ethical manner is permitted for this course.\n\n\nYou may use AI for:\n\nObtain clarification\nBrainstorming ideas, examples, outlines, and strategies\nGenerating questions for practice or exploration\nIdentifying keywords or phrasing to match professional goals\n\n\n\n\nYou may not:\n\nSubmit AI-generated work\nUse AI to complete assignments, quizzes, exams, or other assessments meant to reflect only your own work\nUse AI to generate code\n\nAny AI-generated work will receive a 0 in the class. Severe cases will be reported to Academic Misconduct.\nYou may not upload any course material to any AI platforms such as ChatGPT, Claude, Meta AI, and Google Gemini. Exceptions are allowed for DASS-approved services."
  },
  {
    "objectID": "syllabus.html#math-201-elementary-statistics",
    "href": "syllabus.html#math-201-elementary-statistics",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Spring 2026\nInstructor: Isaac Quintanilla Salinas\nEmail: isaac.qs@csuci.edu\nOffice Location: Marin Hall 2326\nOffice Hours:\nLecture: Gateway Hall 2501 4:30 - 5:45 PM\nWebsite: Canvas (CI Learn)\n\n\n\nCritical reasoning using a quantitative and statistical, problem-solving approach to solving real-world problems. Topics include: probability and statistics, sample data, probability and empirical data distributions, sampling techniques, estimation and hypothesis testing, ANOVA, and correlation and regression analysis. Students will use standard statistical software to analyze real-world and simulated data. GenEd: 2\n\n\n\n\nPrepare students for advanced courses in data-management and statistics\nReason both inductively and deductively with quantitative information and data\nUse statistical software for complex statistical analysis of real-world and simulated data\nEmpower students to apply computational and inferential thinking to address real-world problems\nWrite the results of a statistical study and draw conclusion in reports\n\n\n\n\nIntroduction to Modern Statistics (IMS)\nStatistical Modeling (SM)\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nParticipation\n5%\n\n\nClasswork\n10%\n\n\nVideo Assignments\n20%\n\n\nNotebook Assignments\n20%\n\n\nExam 1\n15%\n\n\nExam 2\n15%\n\n\nExam 3\n15%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\n\nStudents are expected to attend in-person to class to learn the material.\n\n\n\nStudents are expected to check the course Canvas page 3-4 times a week to view assignments, announcements, and other course-related materials.\n\n\nParticipation is based on short writing assignments conducted in class. There will be no make ups for these writing prompts.\n\n\n\nClasswork assignmets are designed to give you an opportunity to practice conceptual topics from the video assignments. The assignments will require you to program in R. The 3 lowest homework assignments will be dropped. Homework assignments will be due every Friday at 11:59 PM.\n\n\n\nNotebook assignments are designed to expand your statistical knowledge. These will be completed in Google Colab which can be accessed from Canvas. There is one notebook assignments every week that you can be completed during class time. Notebook assignments will be due on Thursday at 11:59 PM every week. The 3 lowest notebook assignments will be dropped.\n\n\n\nVideos are used to teach statistical concepts related to the course. Students are expected to watch at least one video a week. The videos are implemented using VoiceThreads. The 3 lowest video assignments will be dropped. Video assignments will be due every Sunday at 11:59 PM.\n\n\n\nThere will be three in exams. Exam #1 will be on September 29, 2025, Exam #2 will be on November 3, and Exam #3 will be on December 10, 2025 from 4-6 PM. While the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success. At the end of the semester, your lowest exam grade will be replaced by your median average of all 3 exam grades. This course will operate under a zero-tolerance policy. Talking during the time of the exam, sharing materials, looking at another students’ exam, or not following directions given will be subject to the University’s academic integrity policy.\n\n\n\nThere will be 4 extra credit opportunities worth a total of 10% of your overall grade. (There are no make-ups for missed extra credit assignments!) More information will be provided on the extra credit assignments on a later date. Information on the extra credit can be found here.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\nWeek\nTopic\nNB Due\nVideo Due\n\n\n\n\n1/26\nWelcome/Intro to Stats and R\n1\n1\n\n\n2/2\nHoliday/ Data Generating Process\n2\n2\n\n\n2/9\nCategorical Data\n3\n3\n\n\n2/16\nNumerical Data\n4\n4\n\n\n2/23\nDistribution Functions\n5\n\n\n\n3/2\nExam 1/ Linear Regression\n\n5\n\n\n3/9\nSimple Linear Regression\n6\n6\n\n\n3/16\nSpring Break\n\n\n\n\n3/23\nSimple Logistic Regression\n7\n7\n\n\n3/30\nHoliday/ Group Models\n8\n8\n\n\n4/6\nMultivariable Regression\n9\n\n\n\n4/13\nModeling Approaches/Exam 2\n\n9\n\n\n4/20\nSampling Distributions\n10\n10\n\n\n4/27\nInference\n11\n11\n\n\n5/4\nInference\n12\n12\n\n\n5/11\nInference\n13\n\n\n\n5/18\nExam 3\n\n\n\n\n\n\n\n\nThe use of generative artificial intelligence (AI) in an ethical manner is permitted for this course.\n\n\nYou may use AI for:\n\nObtain clarification\nBrainstorming ideas, examples, outlines, and strategies\nGenerating questions for practice or exploration\nIdentifying keywords or phrasing to match professional goals\n\n\n\n\nYou may not:\n\nSubmit AI-generated work\nUse AI to complete assignments, quizzes, exams, or other assessments meant to reflect only your own work\nUse AI to generate code\n\nAny AI-generated work will receive a 0 in the class. Severe cases will be reported to Academic Misconduct.\nYou may not upload any course material to any AI platforms such as ChatGPT, Claude, Meta AI, and Google Gemini. Exceptions are allowed for DASS-approved services."
  },
  {
    "objectID": "syllabus.html#university-policies",
    "href": "syllabus.html#university-policies",
    "title": "Syllabus",
    "section": "University Policies",
    "text": "University Policies\n\nSyllabus Policies and Assistance\nCSUCI’s Syllabus Policies and Assistance Website provides important details about academic policies, campus expectations, and student support services that are all highly applicable to your success as a student both in and outside of the classroom. Ensure that you review this site on a regular basis to stay informed about the policies and resources that support your success, as campus resources or policies may change semester to semester.\n\n\nAcademic Honesty\nConduct yourself with honesty and integrity. Do not submit others’ work as your own. Foassignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\n\n\nCSUCI Basic Need\nPlease use the link to the Basic Needs Program on the Syllabus Policies and Assistance website (&lt;go.csuci.edu/syllabuspolicies&gt;) for information on emergency food, housing accommodations, toiletries, and connections to critical resources.\n\n\nCSUCI Disability Statement\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\n\n\nDisruption\n\nIf I Am Out: I will communicate via email and will hold classes asynchronously.\nIf You Are Out: Contact me as soon as possible to talk about your options. Reasonable accommodations will be provided for a brief absence. With proper documentation, extended accommodations will be provided."
  },
  {
    "objectID": "lectures/9.html#sampling-distribution-1",
    "href": "lectures/9.html#sampling-distribution-1",
    "title": "Sampling Distribution",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\nSampling Distribution is the idea that the statistics that you generate (slopes and intercepts) have their own data generation process.\n\nIn other words, the numerical values you obtain from the lm and glm function can be different if we got a different data set.\n\n\nSome values will be more common than others. Because of this, they have their own data generating process, like the outcome of interest has it’s own data generating process."
  },
  {
    "objectID": "lectures/9.html#sampling-distributions",
    "href": "lectures/9.html#sampling-distributions",
    "title": "Sampling Distribution",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\nDistribution of a statistic over repeated samples\nDifferent Samples yield different statistics\n\n\nIf we took many samples, the statistics (like mean) would vary. Their distribution helps us quantify uncertainty."
  },
  {
    "objectID": "lectures/9.html#standard-error",
    "href": "lectures/9.html#standard-error",
    "title": "Sampling Distribution",
    "section": "Standard Error",
    "text": "Standard Error\nThe Standard Error (SE) is the standard deviation of a statistic itself.\n\nSE tells us how much a statistic varies from sample to sample. Smaller SE = more precision."
  },
  {
    "objectID": "lectures/9.html#modelling-the-data",
    "href": "lectures/9.html#modelling-the-data",
    "title": "Sampling Distribution",
    "section": "Modelling the Data",
    "text": "Modelling the Data\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\n\n\\(Y_i\\): Outcome data\n\\(X_i\\): Predictor data\n\\(\\beta_0, \\beta_1\\): parameters\n\\(\\varepsilon_i\\): error term"
  },
  {
    "objectID": "lectures/9.html#error-term",
    "href": "lectures/9.html#error-term",
    "title": "Sampling Distribution",
    "section": "Error Term",
    "text": "Error Term\n\\[\n\\varepsilon_i \\sim DGP\n\\]\n\n\nThe error terms forces the outcome variable to be different from the mathematical model.\nThe numbers being generated are random and cannot be predicted."
  },
  {
    "objectID": "lectures/9.html#randomness-effect",
    "href": "lectures/9.html#randomness-effect",
    "title": "Sampling Distribution",
    "section": "Randomness Effect",
    "text": "Randomness Effect\nThe randomness effect is a sampling phenomenom where you will get different samples every time you sample a population.\n\nGetting different samples means you will get different statistics.\n\n\nThese statistics will have a distribution on their own."
  },
  {
    "objectID": "lectures/9.html#randomness-effect-1",
    "href": "lectures/9.html#randomness-effect-1",
    "title": "Sampling Distribution",
    "section": "Randomness Effect 1",
    "text": "Randomness Effect 1\n\n\nCode\nx &lt;- rnorm(1000)\ny &lt;- 4 + 5 * x + rnorm(1000)\nbb &lt;- round(b(lm(y ~ x),1),2)\nggplot(tibble(x = x, y = y), aes(x,y)) +\n  geom_point() +\n  annotate(\"text\", \n           x = -1, y = 15, \n           label = TeX(sprintf(r'($\\hat{\\beta} = %g$)', bb),\n                       output = \"character\"),\n           parse = TRUE,\n           size = 8)"
  },
  {
    "objectID": "lectures/9.html#randomness-effect-2",
    "href": "lectures/9.html#randomness-effect-2",
    "title": "Sampling Distribution",
    "section": "Randomness Effect 2",
    "text": "Randomness Effect 2\n\n\nCode\nx &lt;- rnorm(1000)\ny &lt;- 4 + 5 * x + rnorm(1000)\nbb &lt;- round(b(lm(y ~ x),1),2)\nggplot(tibble(x = x, y = y), aes(x,y)) +\n  geom_point() +\n  annotate(\"text\", \n           x = -1, y = 15, \n           label = TeX(sprintf(r'($\\hat{\\beta} = %g$)', bb),\n                       output = \"character\"),\n           parse = TRUE,\n           size = 8)"
  },
  {
    "objectID": "lectures/9.html#randomness-effect-3",
    "href": "lectures/9.html#randomness-effect-3",
    "title": "Sampling Distribution",
    "section": "Randomness Effect 3",
    "text": "Randomness Effect 3\n\n\nCode\nx &lt;- rnorm(1000)\ny &lt;- 4 + 5 * x + rnorm(1000)\nbb &lt;- round(b(lm(y ~ x),1),2)\nggplot(tibble(x = x, y = y), aes(x,y)) +\n  geom_point() +\n  annotate(\"text\", \n           x = -1, y = 15, \n           label = TeX(sprintf(r'($\\hat{\\beta} = %g$)', bb),\n                       output = \"character\"),\n           parse = TRUE,\n           size = 8)"
  },
  {
    "objectID": "lectures/9.html#randomness-effect-4",
    "href": "lectures/9.html#randomness-effect-4",
    "title": "Sampling Distribution",
    "section": "Randomness Effect 4",
    "text": "Randomness Effect 4\n\n\nCode\nx &lt;- rnorm(1000)\ny &lt;- 4 + 5 * x + rnorm(1000)\nbb &lt;- round(b(lm(y ~ x),1),2)\nggplot(tibble(x = x, y = y), aes(x,y)) +\n  geom_point() +\n  annotate(\"text\", \n           x = -1, y = 15, \n           label = TeX(sprintf(r'($\\hat{\\beta} = %g$)', bb),\n                       output = \"character\"),\n           parse = TRUE,\n           size = 8)"
  },
  {
    "objectID": "lectures/9.html#randomness-effect-5",
    "href": "lectures/9.html#randomness-effect-5",
    "title": "Sampling Distribution",
    "section": "Randomness Effect 5",
    "text": "Randomness Effect 5\n\n\nCode\nx &lt;- rnorm(1000)\ny &lt;- 4 + 5 * x + rnorm(1000)\nbb &lt;- round(b(lm(y ~ x),1),2)\nggplot(tibble(x = x, y = y), aes(x,y)) +\n  geom_point() +\n  annotate(\"text\", \n           x = -1, y = 15, \n           label = TeX(sprintf(r'($\\hat{\\beta} = %g$)', bb),\n                       output = \"character\"),\n           parse = TRUE,\n           size = 8)"
  },
  {
    "objectID": "lectures/9.html#simulating-unicorns-1",
    "href": "lectures/9.html#simulating-unicorns-1",
    "title": "Sampling Distribution",
    "section": "Simulating Unicorns",
    "text": "Simulating Unicorns\nTo better understand the variation in statistics, let’s simulate a data set of unicorn characteristics to visualize and understand the variation.\n\nWe will simulate a data set using the unicorns function and only we need to specify how many unicorns you want to simulate."
  },
  {
    "objectID": "lectures/9.html#simulating-unicorn-data",
    "href": "lectures/9.html#simulating-unicorn-data",
    "title": "Sampling Distribution",
    "section": "Simulating Unicorn Data",
    "text": "Simulating Unicorn Data\n\n\nCode\nunicorns(10)"
  },
  {
    "objectID": "lectures/9.html#unicorn-data-variables",
    "href": "lectures/9.html#unicorn-data-variables",
    "title": "Sampling Distribution",
    "section": "Unicorn Data Variables",
    "text": "Unicorn Data Variables\n\n\nCode\nnames(unicorns(10))\n\n\n#&gt;  [1] \"Unicorn_ID\"        \"Age\"               \"Gender\"           \n#&gt;  [4] \"Color\"             \"Type_of_Unicorn\"   \"Type_of_Horn\"     \n#&gt;  [7] \"Horn_Length\"       \"Horn_Strength\"     \"Weight\"           \n#&gt; [10] \"Health_Score\"      \"Personality_Score\" \"Magical_Score\"    \n#&gt; [13] \"Elusiveness_Score\" \"Gentleness_Score\"  \"Nature_Score\"\n\n\nWe will only look at Magical_Score and Nature_Score."
  },
  {
    "objectID": "lectures/9.html#magical-and-nature-score",
    "href": "lectures/9.html#magical-and-nature-score",
    "title": "Sampling Distribution",
    "section": "Magical and Nature Score",
    "text": "Magical and Nature Score\n\\[\nMagical =  3423 + 8 \\times Nature + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim N(0, 3.24)\n\\]"
  },
  {
    "objectID": "lectures/9.html#simulating-n0-3.24",
    "href": "lectures/9.html#simulating-n0-3.24",
    "title": "Sampling Distribution",
    "section": "Simulating \\(N(0, 3.24)\\)",
    "text": "Simulating \\(N(0, 3.24)\\)\n\n\nCode\nrnorm(1, 0, sqrt(3.24))\n\n\n#&gt; [1] -0.5668204"
  },
  {
    "objectID": "lectures/9.html#collecting",
    "href": "lectures/9.html#collecting",
    "title": "Sampling Distribution",
    "section": "Collecting",
    "text": "Collecting\n\n\nCode\nunicorns(10) |&gt; select(Nature_Score, Magical_Score)"
  },
  {
    "objectID": "lectures/9.html#dgp-of-magical-score-1",
    "href": "lectures/9.html#dgp-of-magical-score-1",
    "title": "Sampling Distribution",
    "section": "DGP of Magical Score 1",
    "text": "DGP of Magical Score 1\n\n\nCode\nggplot(unicorns(500), aes(Magical_Score)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/9.html#dgp-of-magical-score-2",
    "href": "lectures/9.html#dgp-of-magical-score-2",
    "title": "Sampling Distribution",
    "section": "DGP of Magical Score 2",
    "text": "DGP of Magical Score 2\n\n\nCode\nggplot(unicorns(500), aes(Magical_Score)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/9.html#estimating-beta_1-via-lm",
    "href": "lectures/9.html#estimating-beta_1-via-lm",
    "title": "Sampling Distribution",
    "section": "Estimating \\(\\beta_1\\) via lm",
    "text": "Estimating \\(\\beta_1\\) via lm\n\n\nCode\nu1 &lt;- unicorns(500)\nlm(Magical_Score ~ Nature_Score, u1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Magical_Score ~ Nature_Score, data = u1)\n#&gt; \n#&gt; Coefficients:\n#&gt;  (Intercept)  Nature_Score  \n#&gt;         3423             8"
  },
  {
    "objectID": "lectures/9.html#collecting-a-new-sample",
    "href": "lectures/9.html#collecting-a-new-sample",
    "title": "Sampling Distribution",
    "section": "Collecting a new sample",
    "text": "Collecting a new sample\n\n\nCode\nu2 &lt;- unicorns(500)\nlm(Magical_Score ~ Nature_Score, u2)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Magical_Score ~ Nature_Score, data = u2)\n#&gt; \n#&gt; Coefficients:\n#&gt;  (Intercept)  Nature_Score  \n#&gt;     3423.460         7.999"
  },
  {
    "objectID": "lectures/9.html#collecting-a-new-sample-1",
    "href": "lectures/9.html#collecting-a-new-sample-1",
    "title": "Sampling Distribution",
    "section": "Collecting a new sample",
    "text": "Collecting a new sample\n\n\nCode\nu3 &lt;- unicorns(500)\nlm(Magical_Score ~ Nature_Score, u3)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Magical_Score ~ Nature_Score, data = u3)\n#&gt; \n#&gt; Coefficients:\n#&gt;  (Intercept)  Nature_Score  \n#&gt;     3426.430         7.996"
  },
  {
    "objectID": "lectures/9.html#collecting-a-new-sample-2",
    "href": "lectures/9.html#collecting-a-new-sample-2",
    "title": "Sampling Distribution",
    "section": "Collecting a new sample",
    "text": "Collecting a new sample\n\n\nCode\nu4 &lt;- unicorns(500)\nlm(Magical_Score ~ Nature_Score, u4)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Magical_Score ~ Nature_Score, data = u4)\n#&gt; \n#&gt; Coefficients:\n#&gt;  (Intercept)  Nature_Score  \n#&gt;     3425.700         7.997"
  },
  {
    "objectID": "lectures/9.html#replicating-processes",
    "href": "lectures/9.html#replicating-processes",
    "title": "Sampling Distribution",
    "section": "Replicating Processes",
    "text": "Replicating Processes\n\n\nCode\nreplicate(N, CODE)\n\n\n\nN: number of times to repeat a process\nCODE: what is to repeated"
  },
  {
    "objectID": "lectures/9.html#extracting-hat-beta-coefficeints",
    "href": "lectures/9.html#extracting-hat-beta-coefficeints",
    "title": "Sampling Distribution",
    "section": "Extracting \\(\\hat \\beta\\) Coefficeints",
    "text": "Extracting \\(\\hat \\beta\\) Coefficeints\n\n\nCode\nb(MODEL, INDEX)\n\n\n\nMODEL: a model that can be used to extract components\nINDEX: which component do you want to use\n\n0: Intercept\n1: first slope\n2: second slope\n..."
  },
  {
    "objectID": "lectures/9.html#collecting-1000-samples",
    "href": "lectures/9.html#collecting-1000-samples",
    "title": "Sampling Distribution",
    "section": "Collecting 1000 Samples",
    "text": "Collecting 1000 Samples\n\n\nCode\nbetas &lt;- replicate(1000,\n                   b(lm(Magical_Score ~ Nature_Score, unicorns(500)), 1))\n\nbetas\n\n\n#&gt;    [1] 8.001792 7.990724 8.005220 7.996754 8.007077 7.994742 8.000533 7.994716\n#&gt;    [9] 7.993841 7.999355 7.996996 7.999209 8.002572 8.006277 7.997346 8.005699\n#&gt;   [17] 7.998881 7.995793 8.004817 8.012575 7.998219 8.001980 7.995353 7.997755\n#&gt;   [25] 7.997026 7.989485 7.997585 8.000328 7.997322 7.993165 7.995330 8.001118\n#&gt;   [33] 7.999695 8.001326 7.997579 8.000333 8.007400 7.999175 8.002729 7.998481\n#&gt;   [41] 7.997375 8.004069 8.002015 8.004606 7.999609 8.001450 7.999176 7.995243\n#&gt;   [49] 7.997689 8.010426 7.994887 8.004960 7.996825 7.998923 7.999917 8.000559\n#&gt;   [57] 7.998163 8.004178 7.994534 8.000602 8.000346 7.998288 8.003851 7.995652\n#&gt;   [65] 7.997976 8.002761 8.001684 8.004430 8.001854 8.001545 8.000667 7.996382\n#&gt;   [73] 8.000367 7.999697 8.008907 7.993474 7.998681 7.996191 7.995727 7.998388\n#&gt;   [81] 8.000093 7.999898 7.997231 7.999688 7.998801 7.998745 7.996234 7.997601\n#&gt;   [89] 7.999673 7.999816 7.998125 7.998449 8.000458 7.999391 7.997516 7.993618\n#&gt;   [97] 7.995287 8.002181 8.004746 8.005284 8.002245 8.004156 8.004478 7.998329\n#&gt;  [105] 7.997817 7.999309 8.009440 8.004955 7.997338 7.997901 7.994928 8.005299\n#&gt;  [113] 7.999725 7.997215 7.992919 7.996520 7.995285 7.992343 7.997068 7.994993\n#&gt;  [121] 8.000682 8.001437 7.996953 7.995705 8.004466 8.000150 7.997498 7.993177\n#&gt;  [129] 7.998528 8.000110 8.003487 8.005526 8.005169 8.000495 7.994767 7.998456\n#&gt;  [137] 8.003477 7.995672 8.000939 7.998786 8.004610 8.001729 7.999178 7.995603\n#&gt;  [145] 8.001259 7.994508 7.996341 8.003655 8.005880 8.000314 8.000658 7.995673\n#&gt;  [153] 7.995967 7.997657 8.002871 8.003346 7.996484 8.004039 7.995005 7.998136\n#&gt;  [161] 8.003501 8.005687 8.001079 8.003023 7.992865 8.004081 8.000662 7.994538\n#&gt;  [169] 8.000833 7.998896 8.000393 7.997844 8.007197 8.003560 7.997089 8.003176\n#&gt;  [177] 7.999776 7.998216 7.997876 7.995539 7.996863 8.002468 8.008669 8.001262\n#&gt;  [185] 7.996220 7.998504 8.003628 8.003285 7.994570 7.993817 8.004541 8.003529\n#&gt;  [193] 8.008160 8.006097 8.001723 7.991555 8.001916 7.991896 8.003393 7.992936\n#&gt;  [201] 7.997591 8.000922 8.004328 7.998842 8.000385 8.000263 7.996081 7.995278\n#&gt;  [209] 8.003402 8.001131 7.997743 8.005816 7.997876 8.000043 8.001738 8.008938\n#&gt;  [217] 7.998417 8.000507 7.994143 7.993140 8.002559 7.996920 7.999568 7.999357\n#&gt;  [225] 7.994917 7.995719 8.000380 7.995643 8.000910 7.997649 7.999007 8.003939\n#&gt;  [233] 8.000533 8.008103 8.004556 7.999976 7.994677 8.005441 8.002975 8.004187\n#&gt;  [241] 7.991309 7.997562 8.005858 8.003489 7.997683 7.997345 8.002227 8.002761\n#&gt;  [249] 8.003914 7.998682 8.003948 8.004491 8.004571 7.997535 7.995876 8.002918\n#&gt;  [257] 8.004483 7.997375 7.999200 7.994324 7.999311 8.003714 7.997580 8.003733\n#&gt;  [265] 8.007991 7.998714 8.000968 8.001577 7.997113 7.999279 8.000840 7.999280\n#&gt;  [273] 7.999412 7.999579 8.002798 8.002367 7.998497 8.004173 8.004894 8.002257\n#&gt;  [281] 7.998659 8.000789 7.991781 8.006305 8.001575 8.003028 8.000643 7.992353\n#&gt;  [289] 8.000755 8.001798 7.997957 7.991178 8.001754 7.991629 7.997895 7.995448\n#&gt;  [297] 7.996328 8.004727 8.004737 7.997282 8.002869 8.003461 7.996679 7.996529\n#&gt;  [305] 8.002480 7.998030 8.000989 7.999347 8.001355 8.006232 7.993709 8.006413\n#&gt;  [313] 7.997436 7.995373 8.010958 8.003214 8.004019 7.998471 7.997817 7.991750\n#&gt;  [321] 8.004531 8.002802 8.002236 7.996690 8.001201 8.003866 7.997886 7.998350\n#&gt;  [329] 7.993324 7.996792 8.002658 8.000552 7.997322 8.005513 7.992356 8.002740\n#&gt;  [337] 7.994942 8.004920 7.996660 8.002714 7.999749 7.999355 8.003628 7.999833\n#&gt;  [345] 7.998047 8.007891 8.001027 8.001113 8.001312 8.001975 7.991953 8.001412\n#&gt;  [353] 8.003562 7.998475 8.002964 7.995952 8.000023 7.997526 7.993172 8.000749\n#&gt;  [361] 8.003153 7.998995 8.002012 8.004051 8.003426 8.000897 8.000938 8.001389\n#&gt;  [369] 7.999635 8.001755 8.000504 8.007068 8.003510 7.996456 8.006088 7.998332\n#&gt;  [377] 8.002218 8.003088 7.996177 7.992666 7.993625 8.003841 7.996750 7.994132\n#&gt;  [385] 7.994492 7.997473 7.993542 8.002811 8.006159 7.996434 8.002398 8.001176\n#&gt;  [393] 7.997072 8.005333 8.001907 8.002455 8.000159 7.995074 7.999465 8.007632\n#&gt;  [401] 8.000233 8.001261 8.003139 8.007560 7.998404 8.005736 8.005723 8.003246\n#&gt;  [409] 8.000523 7.996494 8.000267 8.001185 8.002247 7.998468 7.998190 7.996782\n#&gt;  [417] 8.004245 7.995485 8.002847 7.996553 8.008996 8.000397 7.999378 7.999315\n#&gt;  [425] 8.000903 8.001351 8.006004 7.997074 8.001837 8.005076 7.999997 7.997826\n#&gt;  [433] 7.993777 8.003966 8.003950 7.994504 7.999004 8.003667 8.002520 8.006047\n#&gt;  [441] 7.996257 7.995682 8.003868 8.005886 7.998135 7.994065 7.991326 8.002600\n#&gt;  [449] 7.999131 8.000717 8.000092 7.994651 8.002483 8.000537 8.000210 8.004059\n#&gt;  [457] 7.997910 7.992074 7.995612 7.999443 7.996525 7.997123 8.003176 8.000236\n#&gt;  [465] 8.002149 7.998873 8.000840 7.994323 7.991786 8.001730 7.999641 7.996992\n#&gt;  [473] 8.004889 8.000271 7.998843 7.999848 8.004111 8.005257 8.001155 8.000098\n#&gt;  [481] 8.001029 7.996781 8.001747 7.999995 7.995660 8.003075 7.998530 8.001573\n#&gt;  [489] 8.003529 8.003300 7.993940 7.999166 8.006722 7.991564 8.001616 7.999938\n#&gt;  [497] 7.999355 7.997832 7.998287 8.001189 8.000609 7.995239 8.003993 8.000559\n#&gt;  [505] 7.993307 8.000400 8.003173 7.996044 7.996704 8.006333 7.998325 8.000264\n#&gt;  [513] 8.004321 8.004716 7.993676 7.999949 8.000830 7.998495 8.002709 7.998871\n#&gt;  [521] 7.999155 7.997765 7.996505 8.001434 8.000920 8.004350 8.003557 8.002371\n#&gt;  [529] 8.001001 8.002298 8.008943 7.999053 7.998676 8.000736 7.997369 8.004457\n#&gt;  [537] 7.998707 8.003001 7.999091 8.003527 7.999755 7.998625 8.002408 7.999525\n#&gt;  [545] 7.997528 7.994296 7.992991 7.998704 8.004957 8.003641 8.000128 8.002689\n#&gt;  [553] 7.998964 7.997215 7.995148 7.999173 7.998078 7.999905 7.993391 8.005281\n#&gt;  [561] 7.998455 8.000388 8.002382 7.997149 7.998849 8.001399 7.995678 8.000836\n#&gt;  [569] 8.001250 7.994689 7.997502 7.998177 8.003678 8.006411 8.005781 7.997633\n#&gt;  [577] 7.995897 7.998530 8.001816 8.000771 8.003453 7.999015 8.000650 8.006997\n#&gt;  [585] 8.003161 8.003149 7.999450 8.005440 7.999677 8.003645 8.003855 7.998938\n#&gt;  [593] 8.000373 8.000289 7.999696 8.006461 7.998424 8.004365 7.994720 8.002666\n#&gt;  [601] 8.004377 8.006405 8.005506 8.003660 7.991339 8.000225 8.003444 8.000018\n#&gt;  [609] 7.994359 7.997165 7.992049 7.999759 7.993789 7.990176 7.997196 7.995192\n#&gt;  [617] 8.001298 8.003557 8.003914 7.992740 7.996961 7.998675 7.996052 8.002392\n#&gt;  [625] 7.997317 8.006476 7.990695 8.004436 8.007702 8.004951 7.999390 8.004459\n#&gt;  [633] 7.998908 8.003137 7.993207 8.003360 8.006992 8.007749 8.004731 7.998617\n#&gt;  [641] 8.003065 7.998471 8.000898 8.007593 8.000211 7.999791 8.001801 7.993481\n#&gt;  [649] 8.004976 7.995817 8.009148 8.002522 8.005726 8.000971 7.998832 8.003587\n#&gt;  [657] 8.005391 7.995930 7.997495 8.001073 7.993894 8.006086 7.997447 7.999716\n#&gt;  [665] 8.000104 8.001262 8.001624 8.009863 8.005708 8.004260 8.001841 7.999484\n#&gt;  [673] 7.994769 8.006208 7.997718 8.000600 7.999878 7.999899 7.998164 8.006123\n#&gt;  [681] 7.997655 8.007620 7.998751 8.009206 8.002087 7.997829 8.000113 8.000667\n#&gt;  [689] 8.007033 8.005738 8.002534 7.992302 8.002206 7.999005 8.001326 7.996862\n#&gt;  [697] 7.999604 7.995505 8.002266 8.000392 7.999102 8.006959 7.996186 7.998228\n#&gt;  [705] 8.001958 8.001448 8.003841 7.994486 8.006780 8.001673 8.007613 7.999452\n#&gt;  [713] 8.000578 7.995907 8.002502 8.000720 7.998132 8.008793 7.995144 8.001148\n#&gt;  [721] 8.009667 8.007302 8.001188 7.998341 7.998104 8.004479 7.992735 7.997290\n#&gt;  [729] 8.006192 7.996774 8.002343 8.001489 7.994834 8.002884 8.002865 7.994963\n#&gt;  [737] 8.006643 7.990373 7.999122 8.000013 8.001566 8.002047 7.997766 7.997396\n#&gt;  [745] 7.998642 8.003198 7.998150 8.003672 7.999577 8.003899 7.996603 7.997836\n#&gt;  [753] 8.003428 8.003312 7.994366 7.996259 8.003723 7.994949 7.992153 7.999616\n#&gt;  [761] 8.001109 8.000160 7.997698 7.994098 7.997944 7.997457 7.999868 7.995644\n#&gt;  [769] 7.997039 8.000531 8.003259 7.994132 7.991684 7.996492 7.999317 8.001500\n#&gt;  [777] 8.003172 7.998932 7.995139 8.000067 7.996678 7.999421 7.998901 7.997069\n#&gt;  [785] 7.999610 8.002090 8.004931 8.001412 8.004023 8.000560 7.995294 8.001234\n#&gt;  [793] 7.996074 7.997898 7.999632 7.995052 8.002119 8.003215 8.001462 8.006339\n#&gt;  [801] 7.998732 8.004424 7.997131 7.992584 8.005428 8.000306 8.002064 8.005782\n#&gt;  [809] 7.998111 7.997798 8.005402 8.008326 7.998369 8.002453 8.002020 7.992743\n#&gt;  [817] 7.996910 8.003149 7.998019 7.997927 8.006111 7.996815 7.997360 8.000023\n#&gt;  [825] 7.999468 7.999882 8.004537 8.000851 7.996500 7.996232 7.991669 8.000045\n#&gt;  [833] 7.998138 8.005722 8.003684 8.001452 7.999705 7.995815 8.003742 7.999937\n#&gt;  [841] 7.997079 8.004568 8.001243 8.003880 7.994356 8.002707 8.007635 8.000176\n#&gt;  [849] 7.997667 7.994705 8.001225 8.001306 7.995494 7.999351 7.993892 7.995641\n#&gt;  [857] 7.998229 8.003765 8.002415 8.005622 7.990063 8.003708 8.004487 8.007568\n#&gt;  [865] 8.003733 8.003659 8.001153 7.998707 8.006281 8.009562 7.996671 8.001352\n#&gt;  [873] 7.997482 7.999627 8.002902 8.001878 8.005898 7.994876 8.001130 8.002887\n#&gt;  [881] 8.005626 7.998769 8.005590 7.994223 7.998479 8.001988 8.001183 8.001370\n#&gt;  [889] 8.002964 7.999693 8.003393 7.998584 7.989157 8.009888 8.007032 7.999180\n#&gt;  [897] 8.006478 8.000269 8.000093 7.998429 8.004045 7.997768 7.999459 7.996474\n#&gt;  [905] 8.002585 8.000353 8.000581 7.998710 8.001402 8.000921 8.003424 8.000952\n#&gt;  [913] 7.994401 7.993653 7.990516 8.000825 7.990450 8.001141 8.000527 7.997739\n#&gt;  [921] 7.994703 8.000514 8.006539 8.004125 8.002217 8.004816 7.991603 8.000796\n#&gt;  [929] 8.004901 7.996555 8.000295 7.998956 8.002984 7.994693 8.005050 7.994398\n#&gt;  [937] 7.992709 8.002493 7.998888 7.998380 8.004399 8.001113 7.994013 7.995007\n#&gt;  [945] 8.002169 8.002579 8.004284 7.997835 7.992129 7.992168 8.002219 8.003029\n#&gt;  [953] 8.002690 7.994222 7.999117 8.003297 7.995517 8.001267 8.003078 8.001016\n#&gt;  [961] 8.007542 8.003490 7.995073 8.001300 8.003135 8.001499 7.999307 8.003423\n#&gt;  [969] 7.997878 8.001332 8.001146 7.995476 7.998940 8.000059 8.008635 8.003265\n#&gt;  [977] 7.999288 8.006357 8.008208 7.996586 7.992983 7.995794 7.997774 8.000060\n#&gt;  [985] 7.999613 7.997571 7.996866 8.003449 7.991350 7.994932 7.994415 8.000853\n#&gt;  [993] 8.006226 8.008590 7.993132 7.995201 8.000181 7.994935 7.997795 7.995201"
  },
  {
    "objectID": "lectures/9.html#distributions-of-hat-beta_1",
    "href": "lectures/9.html#distributions-of-hat-beta_1",
    "title": "Sampling Distribution",
    "section": "Distributions of \\(\\hat \\beta_1\\)",
    "text": "Distributions of \\(\\hat \\beta_1\\)\n\n\nCode\nggplot(data.frame(x = betas), aes(x = x)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/9.html#central-limit-theorem-1",
    "href": "lectures/9.html#central-limit-theorem-1",
    "title": "Sampling Distribution",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe Central Limit Theorem (CLT) is a fundamental concept in probability and statistics. It states that the distribution of the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables will be approximately normal, regardless of the underlying distribution of those individual variables."
  },
  {
    "objectID": "lectures/9.html#formal-statement-of-the-clt",
    "href": "lectures/9.html#formal-statement-of-the-clt",
    "title": "Sampling Distribution",
    "section": "Formal Statement of the CLT",
    "text": "Formal Statement of the CLT\n\nLet \\(X_1\\), \\(X_2\\), …, \\(X_n\\) be a sequence of i.i.d. random variables with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nLet \\(\\bar X\\) be the sample mean of these variables.\nAs n (the sample size) approaches infinity, the distribution of \\(\\bar X\\) approaches a normal distribution with:\n\nMean: \\(\\mu\\)\nStandard Deviation: \\(\\sigma/\\sqrt{n}\\)"
  },
  {
    "objectID": "lectures/9.html#clt-example",
    "href": "lectures/9.html#clt-example",
    "title": "Sampling Distribution",
    "section": "CLT Example",
    "text": "CLT Example\n\nImagine: You’re flipping a fair coin many times.\n\nEach flip is an independent event (heads or tails).\nThe probability of heads/tails is the same for each flip.\n\nNow: Calculate the average number of heads after each set of 10 flips, then each set of 100 flips, and so on.\nObservation: As the number of flips in each set increases, the distribution of these averages will start to resemble a bell-shaped curve (normal distribution), even though the individual coin flips are not normally distributed."
  },
  {
    "objectID": "lectures/9.html#clt-implications",
    "href": "lectures/9.html#clt-implications",
    "title": "Sampling Distribution",
    "section": "CLT Implications",
    "text": "CLT Implications\n\nApproximation: Even if the underlying data is not normally distributed, the distribution of the sample means will be approximately normal for large enough sample sizes.\nPractical Rule: A common rule of thumb is that the sample size (n) should be at least 30 for the CLT to provide a good approximation. However, this is a guideline, and the actual required sample size can vary depending on the shape of the original distribution."
  },
  {
    "objectID": "lectures/9.html#normal-example-n-10",
    "href": "lectures/9.html#normal-example-n-10",
    "title": "Sampling Distribution",
    "section": "Normal Example \\(n = 10\\)",
    "text": "Normal Example \\(n = 10\\)\nSimulating 500 samples of size 10 from a normal distribution with mean 5 and standard deviation of 2.\n\n\nCode\n#rnorm(10, 5, 2)\nsims &lt;- replicate(500, rnorm(10, 5, 2))\nsims_mean &lt;- colMeans(sims)\nggplot(data.frame(x = sims_mean), aes(x)) +\n  geom_density() +\n  stat_function(fun = dnorm, \n                args = list(mean = 5, sd = 2 / sqrt(10)),\n                col = \"red\")"
  },
  {
    "objectID": "lectures/9.html#normal-example-n-30",
    "href": "lectures/9.html#normal-example-n-30",
    "title": "Sampling Distribution",
    "section": "Normal Example \\(n = 30\\)",
    "text": "Normal Example \\(n = 30\\)\nSimulating 500 samples of size 30 from a normal distribution with mean 5 and standard deviation of 2.\n\n\nCode\n# rnorm(30, 5, 2)\nsims &lt;- replicate(500, rnorm(30, 5, 2))\nsims_mean &lt;- colMeans(sims)\nggplot(data.frame(x = sims_mean), aes(x)) +\n  geom_density() +\n  stat_function(fun = dnorm, \n                args = list(mean = 5, sd = 2 / sqrt(30)),\n                col = \"red\")"
  },
  {
    "objectID": "lectures/9.html#normal-example-n-50",
    "href": "lectures/9.html#normal-example-n-50",
    "title": "Sampling Distribution",
    "section": "Normal Example \\(n = 50\\)",
    "text": "Normal Example \\(n = 50\\)\nSimulating 500 samples of size 50 from a normal distribution with mean 5 and standard deviation of 2.\n\n\nCode\n# rnorm(50, 5, 2)\nsims &lt;- replicate(500, rnorm(50, 5, 2))\nsims_mean &lt;- colMeans(sims)\nggplot(data.frame(x = sims_mean), aes(x)) +\n  geom_density() +\n  stat_function(fun = dnorm, \n                args = list(mean = 5, sd = 2 / sqrt(50)),\n                col = \"red\")"
  },
  {
    "objectID": "lectures/9.html#normal-example-n-100",
    "href": "lectures/9.html#normal-example-n-100",
    "title": "Sampling Distribution",
    "section": "Normal Example \\(n = 100\\)",
    "text": "Normal Example \\(n = 100\\)\nSimulating 500 samples of size 100 from a normal distribution with mean 5 and standard deviation of 2.\n\n\nCode\n# rnorm(100, 5, 2)\nsims &lt;- replicate(500, rnorm(100, 5, 2))\nsims_mean &lt;- colMeans(sims)\nggplot(data.frame(x = sims_mean), aes(x)) +\n  geom_density() +\n  stat_function(fun = dnorm, \n                args = list(mean = 5, sd = 2 / sqrt(100)),\n                col = \"red\")"
  },
  {
    "objectID": "lectures/9.html#normal-dgp",
    "href": "lectures/9.html#normal-dgp",
    "title": "Sampling Distribution",
    "section": "Normal DGP",
    "text": "Normal DGP\nWhen the data is said to have a normal distribution (DGP), there are special properties with both the mean and standard deviation, regardless of sample size."
  },
  {
    "objectID": "lectures/9.html#statistics",
    "href": "lectures/9.html#statistics",
    "title": "Sampling Distribution",
    "section": "Statistics",
    "text": "Statistics\n\n\nMean \\[\n\\bar X = \\sum ^n_{i=1} X_i\n\\]\n\nStandard Deviation \\[\ns^2 = \\frac{1}{n}\\sum ^n_{i=1} (X_i - \\bar X)^2\n\\]"
  },
  {
    "objectID": "lectures/9.html#when-the-true-mu-and-sigma-are-known",
    "href": "lectures/9.html#when-the-true-mu-and-sigma-are-known",
    "title": "Sampling Distribution",
    "section": "When the true \\(\\mu\\) and \\(\\sigma\\) are known",
    "text": "When the true \\(\\mu\\) and \\(\\sigma\\) are known\nA data sample of size \\(n\\) is generated from: \\[\nX_i \\sim N(\\mu, \\sigma)\n\\]"
  },
  {
    "objectID": "lectures/9.html#distribution-of-bar-x",
    "href": "lectures/9.html#distribution-of-bar-x",
    "title": "Sampling Distribution",
    "section": "Distribution of \\(\\bar X\\)",
    "text": "Distribution of \\(\\bar X\\)\n\\[\n\\bar X \\sim N(\\mu, \\sigma/\\sqrt{n})\n\\]"
  },
  {
    "objectID": "lectures/9.html#distribution-of-z",
    "href": "lectures/9.html#distribution-of-z",
    "title": "Sampling Distribution",
    "section": "Distribution of Z",
    "text": "Distribution of Z\n\\[\nZ = \\frac{\\bar X - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "lectures/9.html#when-the-true-mu-and-sigma-are-unknown",
    "href": "lectures/9.html#when-the-true-mu-and-sigma-are-unknown",
    "title": "Sampling Distribution",
    "section": "When the true \\(\\mu\\) and \\(\\sigma\\) are unknown",
    "text": "When the true \\(\\mu\\) and \\(\\sigma\\) are unknown\nA data sample of size \\(n\\) is generated from: \\[\nX_i \\sim N(\\mu, \\sigma)\n\\]"
  },
  {
    "objectID": "lectures/9.html#distribution-of-s2-unknown-mu",
    "href": "lectures/9.html#distribution-of-s2-unknown-mu",
    "title": "Sampling Distribution",
    "section": "Distribution of \\(s^2\\) (unknown \\(\\mu\\))",
    "text": "Distribution of \\(s^2\\) (unknown \\(\\mu\\))\n\\[\n(n-1)s^2/\\sigma^2 \\sim \\chi^2(n-1)\n\\]"
  },
  {
    "objectID": "lectures/9.html#distribution-of-z-unknown-sigma",
    "href": "lectures/9.html#distribution-of-z-unknown-sigma",
    "title": "Sampling Distribution",
    "section": "Distribution of Z (unknown \\(\\sigma\\))",
    "text": "Distribution of Z (unknown \\(\\sigma\\))\n\\[\nZ = \\frac{\\bar X - \\mu}{\\sigma/\\sqrt{n}} \\rightarrow \\frac{\\bar X - \\mu}{s/\\sqrt{n}} \\sim t(n-1)\n\\]"
  },
  {
    "objectID": "lectures/9.html#regression-coefficients",
    "href": "lectures/9.html#regression-coefficients",
    "title": "Sampling Distribution",
    "section": "Regression Coefficients",
    "text": "Regression Coefficients\nThe estimates of regression coefficients (slopes) have a distribution!\n\nBased on our outcome, we will have 2 different distributions to work with: Normal or t."
  },
  {
    "objectID": "lectures/9.html#linear-regression",
    "href": "lectures/9.html#linear-regression",
    "title": "Sampling Distribution",
    "section": "Linear Regression",
    "text": "Linear Regression\n\\[\n\\frac{\\hat\\beta_j-\\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n\\]"
  },
  {
    "objectID": "lectures/9.html#beta_j-0",
    "href": "lectures/9.html#beta_j-0",
    "title": "Sampling Distribution",
    "section": "\\(\\beta_j = 0\\)",
    "text": "\\(\\beta_j = 0\\)\n\\[\n\\frac{\\hat\\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n\\]"
  },
  {
    "objectID": "lectures/9.html#logistic-regression",
    "href": "lectures/9.html#logistic-regression",
    "title": "Sampling Distribution",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\n\\frac{\\hat\\beta_j - \\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "lectures/9.html#beta_j-0-1",
    "href": "lectures/9.html#beta_j-0-1",
    "title": "Sampling Distribution",
    "section": "\\(\\beta_j = 0\\)",
    "text": "\\(\\beta_j = 0\\)\n\\[\n\\frac{\\hat\\beta_j}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "lectures/7b.html#explaining-variation",
    "href": "lectures/7b.html#explaining-variation",
    "title": "Multivariable Linear Regression",
    "section": "Explaining Variation",
    "text": "Explaining Variation\n\nThis is the process where we try to reduce the variation with the use of other variables.\n\n\nCan be thought of as getting it less wrong when taking an educated guess."
  },
  {
    "objectID": "lectures/7b.html#taylor-swifts-songs-danceability",
    "href": "lectures/7b.html#taylor-swifts-songs-danceability",
    "title": "Multivariable Linear Regression",
    "section": "Taylor Swift’s Songs Danceability",
    "text": "Taylor Swift’s Songs Danceability\n\n\nCode\nggplot(taylor_album_songs, aes(danceability)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/7b.html#danceability-by-mode_name",
    "href": "lectures/7b.html#danceability-by-mode_name",
    "title": "Multivariable Linear Regression",
    "section": "Danceability by mode_name",
    "text": "Danceability by mode_name\n\n\nCode\ntaylor_album_songs |&gt; \n  drop_na(mode_name) |&gt; \n  ggplot(aes(danceability)) +\n  geom_density() +\n  facet_wrap(~ mode_name)"
  },
  {
    "objectID": "lectures/7b.html#danceability-by-valence",
    "href": "lectures/7b.html#danceability-by-valence",
    "title": "Multivariable Linear Regression",
    "section": "Danceability by Valence",
    "text": "Danceability by Valence\n\n\nCode\nggplot(taylor_album_songs, aes(valence, danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/7b.html#danceability-by-energy",
    "href": "lectures/7b.html#danceability-by-energy",
    "title": "Multivariable Linear Regression",
    "section": "Danceability by Energy",
    "text": "Danceability by Energy\n\n\nCode\nggplot(taylor_album_songs, aes(energy, danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/7b.html#modelling-danceability",
    "href": "lectures/7b.html#modelling-danceability",
    "title": "Multivariable Linear Regression",
    "section": "Modelling Danceability",
    "text": "Modelling Danceability\nHow do we use all the variables to explain danceability?"
  },
  {
    "objectID": "lectures/7b.html#mlr",
    "href": "lectures/7b.html#mlr",
    "title": "Multivariable Linear Regression",
    "section": "MLR",
    "text": "MLR\nMultivariable Linear Regression (MLR) is used to model an outcome variable (\\(Y\\)) by multiple predictor variables (\\(X_1, X_2, \\ldots, X_p\\)).\n\nUsing MLR, you propose that the ouctome variable was constructed from a set of predictors, with their corresponding regression coefficients (\\(\\beta\\)), and a bit of error\n\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP\n\\]"
  },
  {
    "objectID": "lectures/7b.html#model-data",
    "href": "lectures/7b.html#model-data",
    "title": "Multivariable Linear Regression",
    "section": "Model Data",
    "text": "Model Data\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP\n\\]\n\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n\\]\n\\[\n\\varepsilon_i \\sim DGP\n\\]"
  },
  {
    "objectID": "lectures/7b.html#unknown-parameters",
    "href": "lectures/7b.html#unknown-parameters",
    "title": "Multivariable Linear Regression",
    "section": "Unknown Parameters",
    "text": "Unknown Parameters\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n\\]\n\n\\[\n\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_p\n\\]"
  },
  {
    "objectID": "lectures/7b.html#estimated-model",
    "href": "lectures/7b.html#estimated-model",
    "title": "Multivariable Linear Regression",
    "section": "Estimated Model",
    "text": "Estimated Model\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\varepsilon_i\n\\]\n\\[\n\\hat Y_i = \\hat\\beta_0 + \\hat\\beta_1 X_{i1} + \\hat\\beta_2 X_{i2} + \\cdots + \\hat\\beta_p X_{ip}\n\\]"
  },
  {
    "objectID": "lectures/7b.html#estimating-prameters",
    "href": "lectures/7b.html#estimating-prameters",
    "title": "Multivariable Linear Regression",
    "section": "Estimating Prameters",
    "text": "Estimating Prameters\n\\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_p\\) are estimated by minimizing the following function:\n\\[\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n\\]"
  },
  {
    "objectID": "lectures/7b.html#fitting-a-model-in-r",
    "href": "lectures/7b.html#fitting-a-model-in-r",
    "title": "Multivariable Linear Regression",
    "section": "Fitting a Model in R",
    "text": "Fitting a Model in R\n\n\nCode\nlm(Y ~ X1 + X2 + ... + Xp, data = DATA)"
  },
  {
    "objectID": "lectures/7b.html#modelling-danceability-1",
    "href": "lectures/7b.html#modelling-danceability-1",
    "title": "Multivariable Linear Regression",
    "section": "Modelling Danceability",
    "text": "Modelling Danceability\n\n\nCode\nlm(danceability ~ mode_name + valence + energy, \n   data = taylor_album_songs)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = danceability ~ mode_name + valence + energy, data = taylor_album_songs)\n#&gt; \n#&gt; Coefficients:\n#&gt;    (Intercept)  mode_nameminor         valence          energy  \n#&gt;        0.53816         0.08386         0.17554        -0.05944"
  },
  {
    "objectID": "lectures/7b.html#estimated-model-1",
    "href": "lectures/7b.html#estimated-model-1",
    "title": "Multivariable Linear Regression",
    "section": "Estimated Model",
    "text": "Estimated Model\n\\[\ndance = 0.54 + 0.07\\times Minor + 0.18 \\times valence - 0.06 \\times energy\n\\]"
  },
  {
    "objectID": "lectures/7b.html#hat-beta_i-representation",
    "href": "lectures/7b.html#hat-beta_i-representation",
    "title": "Multivariable Linear Regression",
    "section": "\\(\\hat \\beta_i\\) Representation",
    "text": "\\(\\hat \\beta_i\\) Representation\nEach regression coefficient \\(\\beta_i\\) represents how the predictor variable changes the outcome, as it increase by 1 unit.\n\nFor categorical dummy variables, the \\(\\beta_i\\) represents how the outcome will change when the data point belongs to that value."
  },
  {
    "objectID": "lectures/7b.html#hat-beta_i-interpretation",
    "href": "lectures/7b.html#hat-beta_i-interpretation",
    "title": "Multivariable Linear Regression",
    "section": "\\(\\hat \\beta_i\\) Interpretation",
    "text": "\\(\\hat \\beta_i\\) Interpretation\nFor \\(hat \\beta_i\\), which is the regression coefficient (slope) of \\(X_i\\):\nAs \\(X_i\\) increases by 1 unit, the outcome (\\(Y\\)) will increase/decrease by \\(\\hat \\beta_i\\) units, adjusting for all other predictor variables.\n\nFor categorical dummy variables \\(X_i\\):\nThe outcome \\(Y_i\\) increases/decreases by \\(\\beta_i\\) units for category \\(X_i\\) compared to the reference category, adjusting for all other predictor variables."
  },
  {
    "objectID": "lectures/7b.html#intepreting-minor-coefficient",
    "href": "lectures/7b.html#intepreting-minor-coefficient",
    "title": "Multivariable Linear Regression",
    "section": "Intepreting Minor coefficient",
    "text": "Intepreting Minor coefficient\n\\[\ndance = 0.54 + 0.07 Minor + 0.18 valence - 0.06 energy\n\\] Minor song’s average danceability score is 0.07 units higher compared to Major song’s, adjusting for valence and energy."
  },
  {
    "objectID": "lectures/7b.html#intepreting-valence-coefficient",
    "href": "lectures/7b.html#intepreting-valence-coefficient",
    "title": "Multivariable Linear Regression",
    "section": "Intepreting valence coefficient",
    "text": "Intepreting valence coefficient\n\\[\ndance = 0.54 + 0.07Minor + 0.18  valence - 0.06 energy\n\\]\nAs valence increases by 1 unit, danceability increases by an average of 0.18 units, adjusting for energy and type of song."
  },
  {
    "objectID": "lectures/7b.html#intepreting-energy-coefficient",
    "href": "lectures/7b.html#intepreting-energy-coefficient",
    "title": "Multivariable Linear Regression",
    "section": "Intepreting energy coefficient",
    "text": "Intepreting energy coefficient\n\\[\ndance = 0.54 + 0.07 Minor + 0.18 valence - 0.06 energy\n\\]\nAs energy increases by 1 unit, danceability decreases by an average of 0.06 units, adjusting for valence and type of song."
  },
  {
    "objectID": "lectures/7b.html#r2",
    "href": "lectures/7b.html#r2",
    "title": "Multivariable Linear Regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nComputing \\(R^2\\) is done by determining how much the variation in the outcome is explained by model and divided by the variation of the outcome.\n\\[\nR^2 = \\frac{\\text{variation explained by model}}{\\text{variation from outcome}} \\\\\n= 1-\\frac{\\text{variation of residuals}}{\\text{variation from outcome}}\n\\]"
  },
  {
    "objectID": "lectures/7b.html#r2-1",
    "href": "lectures/7b.html#r2-1",
    "title": "Multivariable Linear Regression",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nProblems arise when multiple predictors are added to the model. As a new predictor is added to the model, new information is added to the model which will always reduce the variation in the residuals. Therefore, the \\(R^2\\) will always increase."
  },
  {
    "objectID": "lectures/7b.html#problems-with-r2-in-mlr",
    "href": "lectures/7b.html#problems-with-r2-in-mlr",
    "title": "Multivariable Linear Regression",
    "section": "Problems with \\(R^2\\) in MLR",
    "text": "Problems with \\(R^2\\) in MLR\nWhen the number of variables increase, the regular \\(R²\\) will be biased in its prediction capability when new data is obtained.\n\nTherefore, statisticians uses the adjusted \\(R^2\\), that penalizes the model when more variables are added. This ensures that a variable added will have a significant effect in predicting outcomes."
  },
  {
    "objectID": "lectures/7b.html#adjusted-r2-1",
    "href": "lectures/7b.html#adjusted-r2-1",
    "title": "Multivariable Linear Regression",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\\[\nR_a^2 = 1-\\frac{\\text{variation of residuals}}{\\text{variation from outcome}}\\times \\frac{n-1}{n-k-1}\n\\]\n\n\\(n\\): Number of data points\n\\(k\\): Number of predictor variables in the model"
  },
  {
    "objectID": "lectures/7b.html#adjusted-r2-in-r",
    "href": "lectures/7b.html#adjusted-r2-in-r",
    "title": "Multivariable Linear Regression",
    "section": "Adjusted \\(R^2\\) in R",
    "text": "Adjusted \\(R^2\\) in R\n\n\nCode\nxlm &lt;- lm(Y ~ X1 + X2 + ... + Xp, data = DATA)\nar2(xlm)"
  },
  {
    "objectID": "lectures/7b.html#example",
    "href": "lectures/7b.html#example",
    "title": "Multivariable Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nxlm &lt;- lm(danceability ~ mode_name + valence + energy, \n   data = taylor_album_songs)\nar2(xlm)\n\n\n#&gt; [1] 0.09148268\n\n\n8.1% of the variation in danceability is explained by the model."
  },
  {
    "objectID": "lectures/7b.html#model-selection-1",
    "href": "lectures/7b.html#model-selection-1",
    "title": "Multivariable Linear Regression",
    "section": "Model Selection",
    "text": "Model Selection\nModel Selection is the process of obtaining a “final” model containing all the necessary predictors, and eliminating any that are not necessary."
  },
  {
    "objectID": "lectures/7b.html#forward-selection",
    "href": "lectures/7b.html#forward-selection",
    "title": "Multivariable Linear Regression",
    "section": "Forward Selection",
    "text": "Forward Selection\nBegin with the null model (\\(Y\\sim 1\\)) and add variables until a final model is chosen."
  },
  {
    "objectID": "lectures/7b.html#backward-selection",
    "href": "lectures/7b.html#backward-selection",
    "title": "Multivariable Linear Regression",
    "section": "Backward Selection",
    "text": "Backward Selection\nBegin with the full model, and remove variable until the final model is chosen."
  },
  {
    "objectID": "lectures/7b.html#hybrid-selection",
    "href": "lectures/7b.html#hybrid-selection",
    "title": "Multivariable Linear Regression",
    "section": "Hybrid Selection",
    "text": "Hybrid Selection\nA hybrid approach between the forward and backward building approach."
  },
  {
    "objectID": "lectures/7b.html#about-model-selection",
    "href": "lectures/7b.html#about-model-selection",
    "title": "Multivariable Linear Regression",
    "section": "About Model Selection",
    "text": "About Model Selection\nGenerally, it is not a good idea to conduct model selection. The predictor variables in your model should be guided by a literature review that illustrates important predictor variables in a model."
  },
  {
    "objectID": "lectures/6.html#explaining-variation",
    "href": "lectures/6.html#explaining-variation",
    "title": "Simple Linear Regression",
    "section": "Explaining Variation",
    "text": "Explaining Variation\n\nThis is the process where we try to reduce the variation with the use of other variables.\n\n\nCan be thought of as getting it less wrong when taking an educated guess."
  },
  {
    "objectID": "lectures/6.html#explaining-variation-1",
    "href": "lectures/6.html#explaining-variation-1",
    "title": "Simple Linear Regression",
    "section": "Explaining Variation",
    "text": "Explaining Variation\n\n\nCode\nggplot(penguins, aes(body_mass_g)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/6.html#variation-with-one-variable",
    "href": "lectures/6.html#variation-with-one-variable",
    "title": "Simple Linear Regression",
    "section": "Variation with One Variable",
    "text": "Variation with One Variable\n\n\nCode\nggplot(penguins, aes(body_mass_g, fill = species)) +\n  geom_density(alpha = .5)"
  },
  {
    "objectID": "lectures/6.html#generated-model",
    "href": "lectures/6.html#generated-model",
    "title": "Simple Linear Regression",
    "section": "Generated Model",
    "text": "Generated Model\n\\[\nY \\sim DGP_1\n\\]"
  },
  {
    "objectID": "lectures/6.html#a-simple-model-1",
    "href": "lectures/6.html#a-simple-model-1",
    "title": "Simple Linear Regression",
    "section": "A Simple Model",
    "text": "A Simple Model\n\n\nCode\nggplot(penguins, aes(body_mass_g)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/6.html#a-simple-model-2",
    "href": "lectures/6.html#a-simple-model-2",
    "title": "Simple Linear Regression",
    "section": "A Simple Model",
    "text": "A Simple Model\n\\[\nY = \\_\\_\\_ + error\n\\]"
  },
  {
    "objectID": "lectures/6.html#notation",
    "href": "lectures/6.html#notation",
    "title": "Simple Linear Regression",
    "section": "Notation",
    "text": "Notation\n\\[\nY = \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\varepsilon\n\\]"
  },
  {
    "objectID": "lectures/6.html#the-simple-generated-model",
    "href": "lectures/6.html#the-simple-generated-model",
    "title": "Simple Linear Regression",
    "section": "The Simple Generated Model",
    "text": "The Simple Generated Model\n\\[\nY \\sim \\beta_0 + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP_2\n\\]\n\n\\(DGP_2\\) is not the same as the \\(DGP_1\\), it is transformed due \\(\\beta_0\\). Consider this the NULL \\(DGP\\)."
  },
  {
    "objectID": "lectures/6.html#observing-data",
    "href": "lectures/6.html#observing-data",
    "title": "Simple Linear Regression",
    "section": "Observing Data",
    "text": "Observing Data\n\\[\nY = \\beta_0 + \\varepsilon\n\\]"
  },
  {
    "objectID": "lectures/6.html#estimated-line",
    "href": "lectures/6.html#estimated-line",
    "title": "Simple Linear Regression",
    "section": "Estimated Line",
    "text": "Estimated Line\n\\[\n\\hat Y=\\hat\\beta_0\n\\]"
  },
  {
    "objectID": "lectures/6.html#notation-1",
    "href": "lectures/6.html#notation-1",
    "title": "Simple Linear Regression",
    "section": "Notation",
    "text": "Notation\n\n\nObserved\n\\[\nY = \\beta_0 + \\varepsilon\n\\]\n\nEstimated\n\\[\n\\hat Y = \\hat \\beta_0\n\\]"
  },
  {
    "objectID": "lectures/6.html#indexing-data",
    "href": "lectures/6.html#indexing-data",
    "title": "Simple Linear Regression",
    "section": "Indexing Data",
    "text": "Indexing Data\nThe data in a data set can be indexed by a number.\n\npenguins[1,-c(1:2)]\n\n#&gt; # A tibble: 1 × 6\n#&gt;   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n#&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n#&gt; 1           39.1          18.7               181        3750 male   2007\n\n\n\nMaking the variable “body_mass_g” be represented by \\(Y\\) and “flipper_length_mm” as \\(X\\):\n\\[\nY_1 = 3750 \\ \\ X_1=181\n\\]"
  },
  {
    "objectID": "lectures/6.html#indexing-data-1",
    "href": "lectures/6.html#indexing-data-1",
    "title": "Simple Linear Regression",
    "section": "Indexing Data",
    "text": "Indexing Data\n\\[\nY_i, X_i\n\\]"
  },
  {
    "objectID": "lectures/6.html#data",
    "href": "lectures/6.html#data",
    "title": "Simple Linear Regression",
    "section": "Data",
    "text": "Data\nWith the data that we collect from a sample, we hypothesize how the data was generated.\n\nUsing a simple model:\n\\[\nY_i = \\beta_0 + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "lectures/6.html#estimated-value",
    "href": "lectures/6.html#estimated-value",
    "title": "Simple Linear Regression",
    "section": "Estimated Value",
    "text": "Estimated Value\n\\[\n\\hat Y_i = \\hat \\beta_0\n\\]"
  },
  {
    "objectID": "lectures/6.html#estimation",
    "href": "lectures/6.html#estimation",
    "title": "Simple Linear Regression",
    "section": "Estimation",
    "text": "Estimation\nTo estimate \\(\\hat \\beta_0\\), we minimize the follow function:\n\\[\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n\\]\n\nThis is known as the sum squared errors, SSE"
  },
  {
    "objectID": "lectures/6.html#residuals",
    "href": "lectures/6.html#residuals",
    "title": "Simple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nThe residuals are known as the observed errors from the data in the model:\n\\[\nr_i = Y_i - \\hat Y_i\n\\]"
  },
  {
    "objectID": "lectures/6.html#estimation-in-r",
    "href": "lectures/6.html#estimation-in-r",
    "title": "Simple Linear Regression",
    "section": "Estimation in R",
    "text": "Estimation in R\n\nlm(Y ~ 1, data = DATA)\n\n\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/6.html#modeling-body-mass-in-penguins",
    "href": "lectures/6.html#modeling-body-mass-in-penguins",
    "title": "Simple Linear Regression",
    "section": "Modeling Body Mass in Penguins",
    "text": "Modeling Body Mass in Penguins\n\nlm(body_mass_g ~ 1, data = penguins)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ 1, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)  \n#&gt;        4207\n\n\n\n\\[\n\\hat Y = 4207\n\\]"
  },
  {
    "objectID": "lectures/6.html#visualize",
    "href": "lectures/6.html#visualize",
    "title": "Simple Linear Regression",
    "section": "Visualize",
    "text": "Visualize\n\n\nCode\nggplot(penguins, aes(body_mass_g)) +\n  geom_density() +\n  geom_vline(xintercept = 4207)"
  },
  {
    "objectID": "lectures/6.html#linear-model-1",
    "href": "lectures/6.html#linear-model-1",
    "title": "Simple Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\nThe goal of Statistics is to develop models the have a better explanation of the outcome \\(Y\\).\n\nIn particularly, reduce the sum of squared errors.\n\n\nBy utilizing a bit more of information, \\(X\\), we can increase the predicting capabilities of the model.\n\n\nThus, the linear model is born."
  },
  {
    "objectID": "lectures/6.html#visualization",
    "href": "lectures/6.html#visualization",
    "title": "Simple Linear Regression",
    "section": "Visualization",
    "text": "Visualization\n\n1-Dimensional2-Dimensional\n\n\n\n\nCode\nggplot(penguins, aes(body_mass_g)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, fill = after_stat(level))) +\n  stat_density_2d(geom = \"polygon\")"
  },
  {
    "objectID": "lectures/6.html#linear-model-2",
    "href": "lectures/6.html#linear-model-2",
    "title": "Simple Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP_3\n\\]"
  },
  {
    "objectID": "lectures/6.html#scatter-plot",
    "href": "lectures/6.html#scatter-plot",
    "title": "Simple Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\n\nCode\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/6.html#imposing-a-line",
    "href": "lectures/6.html#imposing-a-line",
    "title": "Simple Linear Regression",
    "section": "Imposing a Line",
    "text": "Imposing a Line\n\n\nCode\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) + \n  geom_point() +\n  stat_smooth(method = \"lm\", se = F)"
  },
  {
    "objectID": "lectures/6.html#modelling-the-data",
    "href": "lectures/6.html#modelling-the-data",
    "title": "Simple Linear Regression",
    "section": "Modelling the Data",
    "text": "Modelling the Data\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "lectures/6.html#linear-model-3",
    "href": "lectures/6.html#linear-model-3",
    "title": "Simple Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\n\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 X_i\n\\]\n\nGoal is to obtain numerical values for \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) that will minimize the SSE."
  },
  {
    "objectID": "lectures/6.html#sse",
    "href": "lectures/6.html#sse",
    "title": "Simple Linear Regression",
    "section": "SSE",
    "text": "SSE\n\\[\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n\\]\n\\[\n\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 X_i\n\\]"
  },
  {
    "objectID": "lectures/6.html#fitting-a-model-in-r",
    "href": "lectures/6.html#fitting-a-model-in-r",
    "title": "Simple Linear Regression",
    "section": "Fitting a Model in R",
    "text": "Fitting a Model in R\n\nlm(Y ~ X, data = DATA)\n\n\nX: Name Predictor Variable of Interest in data frame DATA\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/6.html#example",
    "href": "lectures/6.html#example",
    "title": "Simple Linear Regression",
    "section": "Example",
    "text": "Example\nY: “body_mass_g”; X: “flipper_length_mm”\n\nlm(body_mass_g ~ flipper_length_mm, data = penguins)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm  \n#&gt;          -5872.09              50.15\n\n\n\\[\n\\hat Y_i = -5872.09 + 50.15 X_i\n\\]"
  },
  {
    "objectID": "lectures/6.html#interpretation-of-hatbeta_0",
    "href": "lectures/6.html#interpretation-of-hatbeta_0",
    "title": "Simple Linear Regression",
    "section": "Interpretation of \\(\\hat\\beta_0\\)",
    "text": "Interpretation of \\(\\hat\\beta_0\\)\nThe intercept \\(\\hat \\beta_0\\) can be interpreted as the base value when \\(X\\) is set to 0.\n\nSome times the intercept can be interpretable to real world scenarios.\n\n\nOther times it cannot."
  },
  {
    "objectID": "lectures/6.html#interpreting-example",
    "href": "lectures/6.html#interpreting-example",
    "title": "Simple Linear Regression",
    "section": "Interpreting Example",
    "text": "Interpreting Example\n\\[\n\\hat Y_i = -5872.09 + 50.15 X_i\n\\]\nWhen flipper length is 0 mm, the body mass is -5872 grams."
  },
  {
    "objectID": "lectures/6.html#interpretation-of-hat-beta_1",
    "href": "lectures/6.html#interpretation-of-hat-beta_1",
    "title": "Simple Linear Regression",
    "section": "Interpretation of \\(\\hat \\beta_1\\)",
    "text": "Interpretation of \\(\\hat \\beta_1\\)\nThe slope \\(\\hat \\beta_1\\) indicates how will y change when x increases by 1 unit.\n\nIt will demonstrate if there is, on average, a positive or negative relationship based on the sign provided."
  },
  {
    "objectID": "lectures/6.html#interpreting-example-1",
    "href": "lectures/6.html#interpreting-example-1",
    "title": "Simple Linear Regression",
    "section": "Interpreting Example",
    "text": "Interpreting Example\n\\[\n\\hat Y_i = -5872.09 + 50.15 X_i\n\\]\nWhen flipper length increases by 1 mm, the body mass will increase by 50.15 grams."
  },
  {
    "objectID": "lectures/6.html#correlation",
    "href": "lectures/6.html#correlation",
    "title": "Simple Linear Regression",
    "section": "Correlation",
    "text": "Correlation\nCorrelation is a statistics that can be used to describe the strength of the relationship between 2 continuous variables.\n\n\\[\nr = \\frac{1}{n-1}\\sum^n_{i=1}\\frac{x_i - \\bar x}{s_x}\\frac{y_i - \\bar y}{s_y}\n\\]\n\n\\(\\bar x\\), \\(\\bar y\\): sample means\n\\(s_x\\), \\(s_y\\): sample standard deviations\n\n\n\n\\[\n-1 \\leq r \\leq 1\n\\]"
  },
  {
    "objectID": "lectures/6.html#correlations",
    "href": "lectures/6.html#correlations",
    "title": "Simple Linear Regression",
    "section": "Correlations",
    "text": "Correlations\n\nFrom IMS 2e"
  },
  {
    "objectID": "lectures/6.html#coefficient-of-determination",
    "href": "lectures/6.html#coefficient-of-determination",
    "title": "Simple Linear Regression",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\nThe coefficient of determination evaluates the strength between an outcome \\(Y\\) and the linear model, which includes \\(X\\).\n\n\\[\nR^2 = r^2\n\\]\n\n\n\\[\n0 \\leq R^2 \\leq 1\n\\]\n\n\nThe coefficient of determination measures the total variation explained by the linear model. The closer to 1, the better the linear model."
  },
  {
    "objectID": "lectures/6.html#correlation-in-r",
    "href": "lectures/6.html#correlation-in-r",
    "title": "Simple Linear Regression",
    "section": "Correlation in R",
    "text": "Correlation in R\n\ncor(DATA$Y, DATA$X)\n\n\nX: Name Predictor Variable of Interest in data frame DATA\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/6.html#example-1",
    "href": "lectures/6.html#example-1",
    "title": "Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\ncor(penguins$body_mass_g, penguins$flipper_length_mm)\n\n#&gt; [1] 0.8729789"
  },
  {
    "objectID": "lectures/6.html#coefficient-of-determination-in-r",
    "href": "lectures/6.html#coefficient-of-determination-in-r",
    "title": "Simple Linear Regression",
    "section": "Coefficient of Determination in R",
    "text": "Coefficient of Determination in R\n\nxlm &lt;- lm(Y ~ X, data = DATA)\nr2(xlm)\n\n\nX: Name Predictor Variable of Interest in data frame DATA\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/6.html#example-2",
    "href": "lectures/6.html#example-2",
    "title": "Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nxlm &lt;- lm(body_mass_g ~ species, penguins)\nr2(xlm)\n\n#&gt; [1] 0.6744887"
  },
  {
    "objectID": "lectures/6.html#statistical-model",
    "href": "lectures/6.html#statistical-model",
    "title": "Simple Linear Regression",
    "section": "Statistical Model",
    "text": "Statistical Model\n\\[\n\\hat Y = \\hat \\beta_0 + \\hat \\beta_1 X\n\\]\n\n\\(X\\): Input\n\\(\\hat Y\\): Output"
  },
  {
    "objectID": "lectures/6.html#prediction-1",
    "href": "lectures/6.html#prediction-1",
    "title": "Simple Linear Regression",
    "section": "Prediction",
    "text": "Prediction\nUsing the equation \\(\\hat Y\\), we can give it a value of \\(X\\) and then, in return, a value of \\(\\hat Y\\) that predicts the true value \\(Y\\)."
  },
  {
    "objectID": "lectures/6.html#prediction-in-r",
    "href": "lectures/6.html#prediction-in-r",
    "title": "Simple Linear Regression",
    "section": "Prediction in R",
    "text": "Prediction in R\n\nxlm &lt;- lm(Y ~ X,\n            data = DATA)\n\npredict_df &lt;- data.frame(X = VAL)\n\npredict(xlm,\n        predict_df)\n\n\nX: Name Predictor Variable of Interest in data frame DATA\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame\nVAL: Value for the Predictor Variable"
  },
  {
    "objectID": "lectures/6.html#example-1-1",
    "href": "lectures/6.html#example-1-1",
    "title": "Simple Linear Regression",
    "section": "Example 1",
    "text": "Example 1\n\nExampleCode\n\n\nPredict the body mass for a gentoo penguin.\n\n\n\nxlm &lt;- lm(body_mass_g ~ species,\n            data = penguins)\n\nxlm\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;      (Intercept)  speciesChinstrap     speciesGentoo  \n#&gt;          3706.16             26.92           1386.27\n\npredict_df &lt;- data.frame(species = \"Gentoo\")\n\npredict(xlm,\n        predict_df)\n\n#&gt;        1 \n#&gt; 5092.437"
  },
  {
    "objectID": "lectures/6.html#example-2-1",
    "href": "lectures/6.html#example-2-1",
    "title": "Simple Linear Regression",
    "section": "Example 2",
    "text": "Example 2\n\nExampleCode\n\n\nPredict the body mass for a penguin with a flipper length of 190.\n\n\n\nxlm &lt;- lm(body_mass_g ~ flipper_length_mm,\n            data = penguins)\n\n\nxlm\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm  \n#&gt;          -5872.09              50.15\n\npredict_df &lt;- data.frame(flipper_length_mm = 190)\n\npredict(xlm,\n        predict_df)\n\n#&gt;        1 \n#&gt; 3657.028"
  },
  {
    "objectID": "lectures/6.html#interpolation",
    "href": "lectures/6.html#interpolation",
    "title": "Simple Linear Regression",
    "section": "Interpolation",
    "text": "Interpolation\nInterpolation is the process of estimating a value within the range of the observed input data \\(X\\)."
  },
  {
    "objectID": "lectures/6.html#extrapolation",
    "href": "lectures/6.html#extrapolation",
    "title": "Simple Linear Regression",
    "section": "Extrapolation",
    "text": "Extrapolation\nExtrapolation is the process of estimating a value beyond the range of observed input data \\(X\\). It’s about venturing into the unknown, using what we know as a guide."
  },
  {
    "objectID": "lectures/6.html#extrapolation-1",
    "href": "lectures/6.html#extrapolation-1",
    "title": "Simple Linear Regression",
    "section": "Extrapolation",
    "text": "Extrapolation\n\n\nCode\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) + \n  xlim(160, 250) +\n  ylim(2600, 7000) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F)"
  },
  {
    "objectID": "lectures/4.html#r-packages",
    "href": "lectures/4.html#r-packages",
    "title": "Numerical Data",
    "section": "R Packages",
    "text": "R Packages\n\ncsucistats\ntidyverse"
  },
  {
    "objectID": "lectures/4.html#motivating-example",
    "href": "lectures/4.html#motivating-example",
    "title": "Numerical Data",
    "section": "Motivating Example",
    "text": "Motivating Example"
  },
  {
    "objectID": "lectures/4.html#what-is-numerical-data",
    "href": "lectures/4.html#what-is-numerical-data",
    "title": "Numerical Data",
    "section": "What is numerical data?",
    "text": "What is numerical data?\n\ntrashwheel$PlasticBottles\n\n#&gt;   [1] 1450 1120 2450 2380  980 1430  910 3580 2400 1340  740  950  530  840 1130\n#&gt;  [16] 1640 1350 1640 1730 5960 2170 1930 3200 2500 2140 1630 3640 1430  570 4800\n#&gt;  [31]  550 2240 4220 1400 2820 1900 3650  760 1250  880 1800 1370  550  640 1160\n#&gt;  [46] 1570 1340 1740 2870 3120 2780 2240 2550 1470 1240 1150 2850  960  870 1340\n#&gt;  [61] 2630 1850 2830 1740 2780 1260 2340 3240  940  830  960  870  920 1130 1060\n#&gt;  [76]  740  950 2340 2660 2430 2690 1540 1240 2870 2450 1830 2360 2640 2840 2640\n#&gt;  [91] 3070 2820 2640  870 3220 1980  410  640 2730 1360  760 1240  800 2540 2130\n#&gt; [106] 2670 1940 2140 1960 1640 1850 2370 2530 2720 2250 3240 3530 2340 2460 1340\n#&gt; [121]  950 2240 3050 1630 1120 3340 3560 2460 3860 3230 3760 3640 4230 3590 3740\n#&gt; [136] 4200 3840 4350 3960 4150 4450 3460 2850 3560 4130 4350 4420 2130 2450 2300\n#&gt; [151] 2850 2640 3250 2530 2780 2740 2530 1876 2340 2244 2980 3460 1840 1360 1880\n#&gt; [166] 2460 2260 2890 2140 2570 2030 1940 1870 1920 1920 2230  980  650  430  390\n#&gt; [181]  490  210  560  470  390  460  530  460  340  360 2540 2870 2230 2340 2560\n#&gt; [196] 2340 2480 2540 2980 3250 3340 2850 3220 2110 3670 2890 1460 3240 3530 2760\n#&gt; [211] 2980 2430 1500 1270 1340 2780 2910 2760 2950 2550 2720 2540 2370 2250 1260\n#&gt; [226]  950 1020 2250 1130 1240 1950 1460 2210 2470 2150  970  840  790  750  810\n#&gt; [241]  790  670  710  730  820  760  670  730  750  820 1580 2130 2350 2460 1220\n#&gt; [256]  950  400 1050 1480 1300 1650 2130 2340 1890 1420 1280 1140  870  940  620\n#&gt; [271] 1560 1720 1890 1760  890  710  620  780  540  670  820  540  730  910  710\n#&gt; [286] 1750 1320 2740 1120 2100 3110 1200 1390  940 1220 1960 2150 2380 1440 1080\n#&gt; [301] 1840 2100 1500  660  590 1200 2610 1820 1080  790 1240  780  480  300  990\n#&gt; [316] 1110  580 2200 1900 2800  840  680  300  240 1015 2150  580 1400  800 1110\n#&gt; [331] 1980 1040  940  600  460  980 1880  900 1740 1140 2040 2800 1800 3300 1880\n#&gt; [346] 2000 2680  950 3700 1900 3600 2400 4400  980 1980 3600 2200 2800 4200  800\n#&gt; [361] 1400 1800 2210  980 2800 1900 2780 2440 3400  920 1260 1980 1000  960  640\n#&gt; [376]  860  590 1800 3600 2300 3900 2900 1850 3400  980  750  500 3200 3800 1850\n#&gt; [391] 2800  980 3200 2800 2640 1200  510  660  480 2900 2300 1980 1040  900 3400\n#&gt; [406] 1800 3000 2800  950 3800  800  760  540  340  600  480 2100 1840 1100 2650\n#&gt; [421] 1900  900 1880 1240 3000 2300 1400 2050 3250 2100 2450 4050 3800 3000 2200\n#&gt; [436] 3500 1800 1250 2200 1640 1080 1790 2080 1600 2200 2000 1200  980  800 1200\n#&gt; [451] 1800 1440 1600 2400 1900 2100 1200  720  800 2200 1600 1000 3000 2700 2300\n#&gt; [466] 1900 5800 1800 2200 3400 4400 3400 2500 1600 1200 1040 4400 4800 1800 3600\n#&gt; [481] 3000 4200 1400 2800 1800 3400 2600 3300 4100 1800 3500 2900  850  680 3100\n#&gt; [496] 1750  980 2800 1200  840 1040  500  800  640  900  660  750 3800 2900 1800\n#&gt; [511] 2700 4200 3000 1200  600 4400 3100 2800 1100 3400 2800  800 1800  540 3300\n#&gt; [526] 2800  900 2100 3200 1440  800  540 2900 2000  960 3200 1250 2900 2100 3600\n#&gt; [541] 1400 3900 3000 1800 2700 2400 2900 1400 3900 4200 2800 1900  960  640  750\n#&gt; [556]  500 3500 2900 2100 3200 1800 1500 2100 1000 2900 1600 2100 1700 2400 3600\n#&gt; [571] 2700 2900 1800 2200 3000 2100 1000 2700 2200 3100 3400 4000 2100 1900 2300\n#&gt; [586] 3200 1200 3400 2100  980  540 1100  750 1800 2300 3400 2700 3000 3800 4200\n#&gt; [601] 1800 2000 3300 4200 1200 2200  850  980  300  450   80 1800 2200 3400 3100\n#&gt; [616] 2600 1800 1200 2200 1400 2100 1600 2000 3200 2100 1400  880 2000 1200 1950\n#&gt; [631] 9540 8350 8590 7830 8210 9830 9240 9540 8230 7540 8490 9610 7960 8870 7420\n#&gt; [646] 8250 8472 7530 2340 2210 2670  657 2290 2110 7340 9230 1020  900 1140 1350\n#&gt; [661] 1200 1090  920 3150 6970 9100 8400 6500 4400 3600 4000 3700 4057 8800 6800\n#&gt; [676] 5900 7500 8000 4400 6600 3800 4800 4400 5200 6400 5900 4800 5600 7200 4400\n#&gt; [691] 5400 4800 6700 6000 6800 5950 7200 6400 5300 4900 4400 5000 4900 4800 5800\n#&gt; [706] 3600 4200 5000 5400 6400 4000 4300 5100 3900 2700 3000 4100 3900 4400 4900\n#&gt; [721] 3600 4500 5000 5400 3400 4800 3800 2900 3200 4200 2900   NA 3400 4100 3900\n#&gt; [736] 4900 3400 5100 4200 2900 3800 3200 4000 1150 2320 3450 1490  920 1010  680\n#&gt; [751] 1800 2700  950  103 1850 1200 1050 1860 1180 1050 1420 1800 1500 1200  860\n#&gt; [766]  640 1400  980 2100 1100 2400 1500 2200 1200 2000 1800 1000 2100 2400 2700\n#&gt; [781] 3000  980  240  840 1800 1000  500   12 2400 1000 1800  300 2100 2400 1200\n#&gt; [796] 1800 2100  280  480  150 1400  980   40  640  800 1000  810  500 1800  840\n#&gt; [811]  500 1500  480 1000 1200  450  280  150    0    0 2100 1800 1400  980 3000\n#&gt; [826] 2400 1100  340 1200  600 2300 1200 3000 4200 1800  980 2100 3200 1050 2700\n#&gt; [841]  980 1400  800  640  240 2100 2800 1600 1200 2200  980  540  320  270 1200\n#&gt; [856]  360  120  480    0 2400 3200 2500 1800  250  240  640  180  540    0    0\n#&gt; [871] 2800 1600  980  480  720    0  360 2100 2900 2100 1800 2900 3200 1800 1600\n#&gt; [886] 2800 1200 2100 2500 1800 1600 2100 1200  980 2500 1900 2100 1200  900  800\n#&gt; [901]  480  720 1600 2400  800  500  640  900 1400 2100 1900  960 2400 1800 2700\n#&gt; [916] 1500  930 2100 3000 1600 1200 4100 5400 4400 5000 3400 4900 2500 2700 1300\n#&gt; [931]  800  440 2100 3200 1100  800  540 4100 3300 1500  850  440 2800 1200 1400\n#&gt; [946]  880  750 1800  400   10 1400  600 2500 1100 2300 1000 1300  500  700   20\n#&gt; [961]    0 1800  680 2800 3300 2900 1800 2800 1000 2500  950  360    0 3400 2700\n#&gt; [976] 3200 2800 1600 2900 2000 2500 1400  800 1500 2100 1400  980 1800  500 1200\n#&gt; [991]  180    0    0"
  },
  {
    "objectID": "lectures/4.html#summary-statistics-1",
    "href": "lectures/4.html#summary-statistics-1",
    "title": "Numerical Data",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nSummary statistics are used to describe the distribution of data."
  },
  {
    "objectID": "lectures/4.html#central-tendency",
    "href": "lectures/4.html#central-tendency",
    "title": "Numerical Data",
    "section": "Central Tendency",
    "text": "Central Tendency\nCentral tendency is a statistical concept that refers to the central or typical value around which a set of data points tends to cluster. It is used to summarize and describe a data set by identifying a single representative value that provides insights into the data’s overall characteristics."
  },
  {
    "objectID": "lectures/4.html#variation",
    "href": "lectures/4.html#variation",
    "title": "Numerical Data",
    "section": "Variation",
    "text": "Variation\nVariation in statistics refers to the extent to which data points in a dataset deviate or differ from a central tendency measure. Understanding variation is crucial for making informed decisions, drawing meaningful conclusions, and assessing the reliability of statistical analyses."
  },
  {
    "objectID": "lectures/4.html#minimum",
    "href": "lectures/4.html#minimum",
    "title": "Numerical Data",
    "section": "Minimum",
    "text": "Minimum\nThe minimum (min) is the smallest value in the data."
  },
  {
    "objectID": "lectures/4.html#maximum",
    "href": "lectures/4.html#maximum",
    "title": "Numerical Data",
    "section": "Maximum",
    "text": "Maximum\nThe maximum (max) is the largest value in the data."
  },
  {
    "objectID": "lectures/4.html#quartiles",
    "href": "lectures/4.html#quartiles",
    "title": "Numerical Data",
    "section": "Quartiles",
    "text": "Quartiles\nQuartiles are three values (Q1, Q2, Q3) that divides the data into four subsets."
  },
  {
    "objectID": "lectures/4.html#q1",
    "href": "lectures/4.html#q1",
    "title": "Numerical Data",
    "section": "Q1",
    "text": "Q1\nQ1 is the value signifying that a quarter of the data is lower than it."
  },
  {
    "objectID": "lectures/4.html#q2---median",
    "href": "lectures/4.html#q2---median",
    "title": "Numerical Data",
    "section": "Q2 - Median",
    "text": "Q2 - Median\nQ2 is the value signifying that half of the data is below it.\n\nThe median also represents the central tendency of the data."
  },
  {
    "objectID": "lectures/4.html#q3",
    "href": "lectures/4.html#q3",
    "title": "Numerical Data",
    "section": "Q3",
    "text": "Q3\nQ3 is the value signifying that 3 quarters of the data is below it."
  },
  {
    "objectID": "lectures/4.html#interquartile-range",
    "href": "lectures/4.html#interquartile-range",
    "title": "Numerical Data",
    "section": "Interquartile Range",
    "text": "Interquartile Range\n\n\\[\nIQR = Q_3 - Q_1\n\\]"
  },
  {
    "objectID": "lectures/4.html#range",
    "href": "lectures/4.html#range",
    "title": "Numerical Data",
    "section": "Range",
    "text": "Range\n\n\\[\nR = \\mathrm{max} - \\mathrm{min}\n\\]"
  },
  {
    "objectID": "lectures/4.html#how-to-identify-the-quartiles",
    "href": "lectures/4.html#how-to-identify-the-quartiles",
    "title": "Numerical Data",
    "section": "How to identify the quartiles?",
    "text": "How to identify the quartiles?\n\nSort the data\nID Max and Min\nFind the amount of data the makes a quarter:\n\n\\(K=N/4\\)\n\nCreate 4 groups using the sorted data\n\ngroup by data size\nIf \\(K\\) has a decimal, the \\(Kth\\) value is quartile of each group."
  },
  {
    "objectID": "lectures/4.html#mean",
    "href": "lectures/4.html#mean",
    "title": "Numerical Data",
    "section": "Mean",
    "text": "Mean\nDescribe how you will find the mean of these numbers:\n\n\n#&gt; [1] 10 14 22 24 10"
  },
  {
    "objectID": "lectures/4.html#mean-1",
    "href": "lectures/4.html#mean-1",
    "title": "Numerical Data",
    "section": "Mean",
    "text": "Mean\nThe mean is another measurement for central tendency.\n\\[\n\\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i\n\\]\n\n\\(n\\): total data points\n\\(X_i\\): data points\n\\(i\\): indexing data\n\\(\\sum\\): add all from first (bottom) to last (up)"
  },
  {
    "objectID": "lectures/4.html#variance",
    "href": "lectures/4.html#variance",
    "title": "Numerical Data",
    "section": "Variance",
    "text": "Variance\nThe variance is a measurement on the average squared distance the data points are from the central tendency.\n\\[\ns^2 = \\frac{1}{n-1}\\sum^n_{i=1}(X_i-\\bar X)^2\n\\]"
  },
  {
    "objectID": "lectures/4.html#standard-deviation",
    "href": "lectures/4.html#standard-deviation",
    "title": "Numerical Data",
    "section": "Standard Deviation",
    "text": "Standard Deviation\nThe standard deviation is a measurement on the average distance the data points are from the central tendency.\n\\[\ns=\\sqrt{s^2}\n\\]"
  },
  {
    "objectID": "lectures/4.html#outliers",
    "href": "lectures/4.html#outliers",
    "title": "Numerical Data",
    "section": "Outliers",
    "text": "Outliers\nThese are data points that seem to be highly distant from all other variables."
  },
  {
    "objectID": "lectures/4.html#numerical-statistics-in-r-1",
    "href": "lectures/4.html#numerical-statistics-in-r-1",
    "title": "Numerical Data",
    "section": "Numerical Statistics in R",
    "text": "Numerical Statistics in R"
  },
  {
    "objectID": "lectures/4.html#mean-2",
    "href": "lectures/4.html#mean-2",
    "title": "Numerical Data",
    "section": "Mean",
    "text": "Mean\n\nmean(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#median",
    "href": "lectures/4.html#median",
    "title": "Numerical Data",
    "section": "Median",
    "text": "Median\n\nmedian(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#standard-deviation-1",
    "href": "lectures/4.html#standard-deviation-1",
    "title": "Numerical Data",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\nsd(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#variance-1",
    "href": "lectures/4.html#variance-1",
    "title": "Numerical Data",
    "section": "Variance",
    "text": "Variance\n\nvar(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#quartiles-1",
    "href": "lectures/4.html#quartiles-1",
    "title": "Numerical Data",
    "section": "Quartiles",
    "text": "Quartiles\n\nquantile(DATA$VAR, probs = c(0.25, 0.5, 0.75))"
  },
  {
    "objectID": "lectures/4.html#max-and-min",
    "href": "lectures/4.html#max-and-min",
    "title": "Numerical Data",
    "section": "Max and Min",
    "text": "Max and Min\n\nmax(DATA$VAR)\nmin(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#summary-statistics-2",
    "href": "lectures/4.html#summary-statistics-2",
    "title": "Numerical Data",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\nnum_stats(DATA$VAR)"
  },
  {
    "objectID": "lectures/4.html#mr.-trash-wheel",
    "href": "lectures/4.html#mr.-trash-wheel",
    "title": "Numerical Data",
    "section": "Mr. Trash Wheel",
    "text": "Mr. Trash Wheel\n\nnum_stats(trashwheel$PlasticBottles)\n\n#&gt;   min   q25     mean median  q75  max       sd     var    iqr missing\n#&gt; 1   0 987.5 2219.331   1900 2900 9830 1650.449 2723984 1912.5       1"
  },
  {
    "objectID": "lectures/4.html#histogram",
    "href": "lectures/4.html#histogram",
    "title": "Numerical Data",
    "section": "Histogram",
    "text": "Histogram\nA histogram is a graphical representation of the distribution or frequency of data points in a dataset. It provides a visual way to understand the shape, central tendency, and spread of a dataset by dividing the data into intervals or bins and showing how many data points fall into each bin as a bar."
  },
  {
    "objectID": "lectures/4.html#histogram-r-code",
    "href": "lectures/4.html#histogram-r-code",
    "title": "Numerical Data",
    "section": "Histogram R Code",
    "text": "Histogram R Code\n\nggplot(DATA, aes(VARIABLE)) +\n  geom_histogram(bins = X)"
  },
  {
    "objectID": "lectures/4.html#histogram-1",
    "href": "lectures/4.html#histogram-1",
    "title": "Numerical Data",
    "section": "Histogram",
    "text": "Histogram\n\n\nCode\ny &lt;- rnorm(1000)\nggplot(tibble(y), aes(y)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "lectures/4.html#histogram-2",
    "href": "lectures/4.html#histogram-2",
    "title": "Numerical Data",
    "section": "Histogram",
    "text": "Histogram\n\n\nCode\ny &lt;- rgamma(1000, 2)\nggplot(tibble(y), aes(y)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "lectures/4.html#histograms",
    "href": "lectures/4.html#histograms",
    "title": "Numerical Data",
    "section": "Histograms",
    "text": "Histograms\n\n\nCode\ny &lt;- rbeta(1000, 5, 1)\nggplot(tibble(y), aes(y)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "lectures/4.html#histograms-1",
    "href": "lectures/4.html#histograms-1",
    "title": "Numerical Data",
    "section": "Histograms",
    "text": "Histograms\n\n\nCode\ny &lt;- rbinom(1000, 1, 0.4)\nz &lt;- (y == 0) * rnorm(1000, 23) + (y == 1) * rnorm(1000, 27)\nggplot(tibble(z), aes(z)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "lectures/4.html#mr.-trash-wheel-1",
    "href": "lectures/4.html#mr.-trash-wheel-1",
    "title": "Numerical Data",
    "section": "Mr. Trash Wheel",
    "text": "Mr. Trash Wheel\n\nggplot(trashwheel, aes(PlasticBottles)) +\n  geom_histogram()"
  },
  {
    "objectID": "lectures/4.html#box-plot",
    "href": "lectures/4.html#box-plot",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot\nA box plot, also known as a box-and-whisker plot, is a graphical representation of the distribution and key statistical characteristics of a dataset. It provides a visual summary of the data’s central tendency, spread, and potential outliers."
  },
  {
    "objectID": "lectures/4.html#box-plot-1",
    "href": "lectures/4.html#box-plot-1",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot"
  },
  {
    "objectID": "lectures/4.html#box-plot-r-code",
    "href": "lectures/4.html#box-plot-r-code",
    "title": "Numerical Data",
    "section": "Box Plot R Code",
    "text": "Box Plot R Code\n\nggplot(DATA, aes(VARIABLE)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/4.html#box-plot-2",
    "href": "lectures/4.html#box-plot-2",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot\n\nggplot(trashwheel, aes(PlasticBottles)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/4.html#box-plot-3",
    "href": "lectures/4.html#box-plot-3",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot\n\nggplot(trashwheel, aes(y = PlasticBottles)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/4.html#dot-plots",
    "href": "lectures/4.html#dot-plots",
    "title": "Numerical Data",
    "section": "Dot Plots",
    "text": "Dot Plots\nDot Plots are similar to histograms, but they incorporate dots to count how many data points fall within bins."
  },
  {
    "objectID": "lectures/4.html#dot-plots-in-r",
    "href": "lectures/4.html#dot-plots-in-r",
    "title": "Numerical Data",
    "section": "Dot Plots in R",
    "text": "Dot Plots in R\n\nggplot(DATA, aes(VARIABLE)) +\n  geom_dotplot(binwidth = X)"
  },
  {
    "objectID": "lectures/4.html#dot-plots-1",
    "href": "lectures/4.html#dot-plots-1",
    "title": "Numerical Data",
    "section": "Dot Plots",
    "text": "Dot Plots\n\n\nCode\nggplot(trashwheel, aes(PlasticBottles)) +\n  geom_dotplot(binwidth = 100)"
  },
  {
    "objectID": "lectures/4.html#scatter-plots-1",
    "href": "lectures/4.html#scatter-plots-1",
    "title": "Numerical Data",
    "section": "Scatter Plots",
    "text": "Scatter Plots\nScatter plots demonstrate how two variables behave with each other. They can tell you any postive or negative trends, if they exist, with the combination of the plots."
  },
  {
    "objectID": "lectures/4.html#positive-relationship",
    "href": "lectures/4.html#positive-relationship",
    "title": "Numerical Data",
    "section": "Positive Relationship",
    "text": "Positive Relationship"
  },
  {
    "objectID": "lectures/4.html#negative-relationship",
    "href": "lectures/4.html#negative-relationship",
    "title": "Numerical Data",
    "section": "Negative Relationship",
    "text": "Negative Relationship"
  },
  {
    "objectID": "lectures/4.html#no-relationship",
    "href": "lectures/4.html#no-relationship",
    "title": "Numerical Data",
    "section": "No Relationship",
    "text": "No Relationship"
  },
  {
    "objectID": "lectures/4.html#weak-positive-or-negative-relationship",
    "href": "lectures/4.html#weak-positive-or-negative-relationship",
    "title": "Numerical Data",
    "section": "Weak Positive or Negative Relationship",
    "text": "Weak Positive or Negative Relationship"
  },
  {
    "objectID": "lectures/4.html#scatter-plots-in-r",
    "href": "lectures/4.html#scatter-plots-in-r",
    "title": "Numerical Data",
    "section": "Scatter Plots in R",
    "text": "Scatter Plots in R\n\nggplot(DATA, aes(x = VAR1, y = VAR2)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/4.html#mr.-trash-wheel-2",
    "href": "lectures/4.html#mr.-trash-wheel-2",
    "title": "Numerical Data",
    "section": "Mr. Trash Wheel",
    "text": "Mr. Trash Wheel\n\nggplot(trashwheel, aes(PlasticBottles, PlasticBags)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/16.html#motivating-example-1",
    "href": "lectures/16.html#motivating-example-1",
    "title": "Model Inference",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nCode\nbladder1 |&gt; ggplot(aes(number, color = death2)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/16.html#hypothesis",
    "href": "lectures/16.html#hypothesis",
    "title": "Model Inference",
    "section": "Hypothesis",
    "text": "Hypothesis\n\n\n\\[H_0: \\beta = \\theta\\]\n\n\\[H_0: \\beta \\ne \\theta\\]"
  },
  {
    "objectID": "lectures/16.html#testing-beta_j",
    "href": "lectures/16.html#testing-beta_j",
    "title": "Model Inference",
    "section": "Testing \\(\\beta_j\\)",
    "text": "Testing \\(\\beta_j\\)\n\\[\n\\frac{\\hat\\beta_j - \\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "lectures/16.html#confidence-intervals",
    "href": "lectures/16.html#confidence-intervals",
    "title": "Model Inference",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\\[\nPE \\pm CV \\times SE\n\\]\n\nPE: Point Estimate\nCV: Critical Value \\(P(X&lt;CV) = 1-\\alpha/2\\)\n\\(\\alpha\\): significance level\nSE: Standard Error"
  },
  {
    "objectID": "lectures/16.html#conducting-ht-of-beta_j",
    "href": "lectures/16.html#conducting-ht-of-beta_j",
    "title": "Model Inference",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\n\n\nCode\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial())\nsummary(xlm)"
  },
  {
    "objectID": "lectures/16.html#example",
    "href": "lectures/16.html#example",
    "title": "Model Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(death ~ recur + number + size, bladder1, family = binomial())\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = death ~ recur + number + size, family = binomial(), \n#&gt;     data = bladder1)\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.8525259  0.4462559  -1.910 0.056082 .  \n#&gt; recur       -0.3897480  0.1062848  -3.667 0.000245 ***\n#&gt; number       0.0008451  0.1124503   0.008 0.994004    \n#&gt; size        -0.2240419  0.1626749  -1.377 0.168439    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 189.38  on 293  degrees of freedom\n#&gt; Residual deviance: 166.43  on 290  degrees of freedom\n#&gt; AIC: 174.43\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "lectures/16.html#confidence-interval",
    "href": "lectures/16.html#confidence-interval",
    "title": "Model Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n\nCode\nconfint(xlm, level = LEVEL)"
  },
  {
    "objectID": "lectures/16.html#example-1",
    "href": "lectures/16.html#example-1",
    "title": "Model Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nconfint(m1, level = 0.95)\n\n\n#&gt;                  2.5 %      97.5 %\n#&gt; (Intercept) -1.7353779  0.02529523\n#&gt; recur       -0.6217831 -0.20078281\n#&gt; number      -0.2421738  0.20731479\n#&gt; size        -0.5880581  0.06061498"
  },
  {
    "objectID": "lectures/16.html#confidence-interval-for-odds-ratio",
    "href": "lectures/16.html#confidence-interval-for-odds-ratio",
    "title": "Model Inference",
    "section": "Confidence Interval for Odds Ratio",
    "text": "Confidence Interval for Odds Ratio\n\n\nCode\nexp(confint(m1, level = 0.95))\n\n\n#&gt;                 2.5 %    97.5 %\n#&gt; (Intercept) 0.1763335 1.0256179\n#&gt; recur       0.5369861 0.8180901\n#&gt; number      0.7849197 1.2303698\n#&gt; size        0.5554048 1.0624898"
  },
  {
    "objectID": "lectures/16.html#model-inference",
    "href": "lectures/16.html#model-inference",
    "title": "Model Inference",
    "section": "Model inference",
    "text": "Model inference\nWe conduct model inference to determine if different models are better at explaining variation. A common example is to compare a linear model (\\(g(\\hat Y)=\\hat\\beta_0 + \\hat\\beta_1 X\\)) to the mean of Y (\\(\\hat \\mu_y\\)). We determine the significance of the variation explained using an Analysis of Variance (ANOVA) table and F test."
  },
  {
    "objectID": "lectures/16.html#model-inference-1",
    "href": "lectures/16.html#model-inference-1",
    "title": "Model Inference",
    "section": "Model Inference",
    "text": "Model Inference\nGiven 2 models:\n\\[\ng(\\hat Y) = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\cdots + \\hat\\beta_p X_p\n\\]\nor\n\\[\ng(\\hat Y) = \\bar y\n\\]\n\nIs the model with predictors do a better job than using the average?"
  },
  {
    "objectID": "lectures/16.html#likelihood-ratio-test",
    "href": "lectures/16.html#likelihood-ratio-test",
    "title": "Model Inference",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nThe Likelihood Ratio Test is a test to determine whether the likelihood of observing the outcome is significantly bigger in a larger, more complicated model, than a simpler model.\nIt conducts a hypothesis tests to see if models are significantly different from each other."
  },
  {
    "objectID": "lectures/16.html#conducting-an-lrt-in-r",
    "href": "lectures/16.html#conducting-an-lrt-in-r",
    "title": "Model Inference",
    "section": "Conducting an LRT in R",
    "text": "Conducting an LRT in R\n\n\nCode\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial)\nxlm0 &lt;- glm(Y ~ 1, data = DATA, family = binomial)\nanova(xlm0, xlm, test = \"LRT\")"
  },
  {
    "objectID": "lectures/16.html#example-2",
    "href": "lectures/16.html#example-2",
    "title": "Model Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nm0 &lt;- update(m1, formula. = ~ 1)\nanova(m0, m1, test = \"LRT\")\n\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: death ~ 1\n#&gt; Model 2: death ~ recur + number + size\n#&gt;   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \n#&gt; 1       293     189.38                         \n#&gt; 2       290     166.43  3   22.953 4.13e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/16.html#model-inference-2",
    "href": "lectures/16.html#model-inference-2",
    "title": "Model Inference",
    "section": "Model Inference",
    "text": "Model Inference\nModel inference can be extended to compare models that have different number of predictors."
  },
  {
    "objectID": "lectures/16.html#model-inference-3",
    "href": "lectures/16.html#model-inference-3",
    "title": "Model Inference",
    "section": "Model Inference",
    "text": "Model Inference\nGiven:\n\\[\nM1:\\ g(\\hat y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\n\\[\nM2:\\ g(\\hat y) = \\beta_0 + \\beta_1 X_1  \n\\]\nLet \\(M1\\) be the FULL (larger) model, and let \\(M2\\) be the RED (Reduced, smaller) model."
  },
  {
    "objectID": "lectures/16.html#model-inference-4",
    "href": "lectures/16.html#model-inference-4",
    "title": "Model Inference",
    "section": "Model Inference",
    "text": "Model Inference\nHe can test the following Hypothesis:\n\n\\(H_0\\): The error variations between the FULL and RED model are not different.\n\\(H_1\\): The error variations between the FULL and RED model are different."
  },
  {
    "objectID": "lectures/16.html#likelihood-ratio-test-in-r",
    "href": "lectures/16.html#likelihood-ratio-test-in-r",
    "title": "Model Inference",
    "section": "Likelihood Ratio Test in R",
    "text": "Likelihood Ratio Test in R\n\n\nCode\nfull &lt;- glm(Y  ~  X1 + X2 + X3 + X4, DATA, family = binomial())\nred &lt;- glm(Y ~ X1 + X2, DATA, family = binomial())\nanova(red, full, test = \"LRT\")"
  },
  {
    "objectID": "lectures/16.html#example-3",
    "href": "lectures/16.html#example-3",
    "title": "Model Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(death ~ number + size + recur, bladder1, family = binomial())\nm2 &lt;- glm(death ~ recur, bladder1, family = binomial())\nanova(m2, m1, test = \"LRT\")\n\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: death ~ recur\n#&gt; Model 2: death ~ number + size + recur\n#&gt;   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n#&gt; 1       292     168.72                     \n#&gt; 2       290     166.43  2   2.2883   0.3185"
  },
  {
    "objectID": "lectures/16.html#paired-t-test-as-linear-regression",
    "href": "lectures/16.html#paired-t-test-as-linear-regression",
    "title": "Model Inference",
    "section": "Paired t-Test as Linear Regression",
    "text": "Paired t-Test as Linear Regression\nA paired t-test compares the means of two related measurements (e.g., before and after). This can be expressed as a linear regression on the difference scores.\nRegression on difference scores (simplest method): \\[\n    D_i = Y_{i2} - Y_{i1} = \\beta_0 + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "lectures/16.html#t-test-equal-variances-as-linear-regression",
    "href": "lectures/16.html#t-test-equal-variances-as-linear-regression",
    "title": "Model Inference",
    "section": "t-Test (Equal Variances) as Linear Regression",
    "text": "t-Test (Equal Variances) as Linear Regression\nA two-sample t-test compares the means of two groups. The assumption is that the two groups have equal variance. This is equivalent to a linear regression model with a binary predictor:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\nWhere:\n\n\\(Y_i\\): Outcome variable\n\n\\(X_i\\): Group indicator (0 = Group A, 1 = Group B)\n\n\\(\\beta_1\\): Difference in means between groups\n\nKey Insight: The t-statistic in the regression output matches the t-test result exactly. Both test if the group means are equal."
  },
  {
    "objectID": "lectures/16.html#anova-as-linear-regression",
    "href": "lectures/16.html#anova-as-linear-regression",
    "title": "Model Inference",
    "section": "ANOVA as Linear Regression",
    "text": "ANOVA as Linear Regression\nANOVA compares means across three or more groups. This can also be expressed as a regression model using dummy variables for group membership:\n\\[\nY_i = \\beta_0 + \\beta_1 D_{1i} + \\beta_2 D_{2i} + \\varepsilon_i\n\\]\nWhere \\(D_{1i}, D_{2i}\\) are dummy variables for group categories.\nKey Insight:\nThe F-statistic in the ANOVA output is identical to the F-statistic from the regression model with a factor predictor."
  },
  {
    "objectID": "lectures/16.html#paired-t-test-as-linear-regression-1",
    "href": "lectures/16.html#paired-t-test-as-linear-regression-1",
    "title": "Model Inference",
    "section": "Paired t-Test as Linear Regression",
    "text": "Paired t-Test as Linear Regression\nA paired t-test compares the means of two related measurements (e.g., before and after). This can be expressed as a linear regression on the difference scores.\nRegression on difference scores (simplest method): \\[\nD_i = Y_{i2} - Y_{i1} = \\beta_0 + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "lectures/16.html#more-information",
    "href": "lectures/16.html#more-information",
    "title": "Model Inference",
    "section": "More Information",
    "text": "More Information\nVisit this site to get a more comprehensive picture on how common statistical tests can be done using regression models: https://lindeloev.github.io/tests-as-linear/#51_independent_t-test_and_mann-whitney_u"
  },
  {
    "objectID": "lectures/15.html#motivating-example-1",
    "href": "lectures/15.html#motivating-example-1",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nCode\nbladder1 |&gt; ggplot(aes(number, color = death2)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/15.html#hypothesis",
    "href": "lectures/15.html#hypothesis",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Hypothesis",
    "text": "Hypothesis\n\n\n\\[H_0: \\beta = \\theta\\]\n\n\\[H_0: \\beta \\ne \\theta\\]"
  },
  {
    "objectID": "lectures/15.html#testing-beta_j",
    "href": "lectures/15.html#testing-beta_j",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Testing \\(\\beta_j\\)",
    "text": "Testing \\(\\beta_j\\)\n\\[\n\\frac{\\hat\\beta_j - \\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "lectures/15.html#confidence-intervals",
    "href": "lectures/15.html#confidence-intervals",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\\[\nPE \\pm CV \\times SE\n\\]\n\nPE: Point Estimate\nCV: Critical Value \\(P(X&lt;CV) = 1-\\alpha/2\\)\n\\(\\alpha\\): significance level\nSE: Standard Error"
  },
  {
    "objectID": "lectures/15.html#conducting-ht-of-beta_j",
    "href": "lectures/15.html#conducting-ht-of-beta_j",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\n\n\nCode\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial())\nsummary(xlm)"
  },
  {
    "objectID": "lectures/15.html#example",
    "href": "lectures/15.html#example",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(death ~ recur + number + size, bladder1, family = binomial())\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = death ~ recur + number + size, family = binomial(), \n#&gt;     data = bladder1)\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.8525259  0.4462559  -1.910 0.056082 .  \n#&gt; recur       -0.3897480  0.1062848  -3.667 0.000245 ***\n#&gt; number       0.0008451  0.1124503   0.008 0.994004    \n#&gt; size        -0.2240419  0.1626749  -1.377 0.168439    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 189.38  on 293  degrees of freedom\n#&gt; Residual deviance: 166.43  on 290  degrees of freedom\n#&gt; AIC: 174.43\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "lectures/15.html#confidence-interval",
    "href": "lectures/15.html#confidence-interval",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n\nCode\nconfint(xlm, level = LEVEL)"
  },
  {
    "objectID": "lectures/15.html#example-1",
    "href": "lectures/15.html#example-1",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nconfint(m1, level = 0.95)\n\n\n#&gt;                  2.5 %      97.5 %\n#&gt; (Intercept) -1.7353779  0.02529523\n#&gt; recur       -0.6217831 -0.20078281\n#&gt; number      -0.2421738  0.20731479\n#&gt; size        -0.5880581  0.06061498"
  },
  {
    "objectID": "lectures/15.html#confidence-interval-for-odds-ratio",
    "href": "lectures/15.html#confidence-interval-for-odds-ratio",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Confidence Interval for Odds Ratio",
    "text": "Confidence Interval for Odds Ratio\n\n\nCode\nexp(confint(m1, level = 0.95))\n\n\n#&gt;                 2.5 %    97.5 %\n#&gt; (Intercept) 0.1763335 1.0256179\n#&gt; recur       0.5369861 0.8180901\n#&gt; number      0.7849197 1.2303698\n#&gt; size        0.5554048 1.0624898"
  },
  {
    "objectID": "lectures/15.html#model-inference",
    "href": "lectures/15.html#model-inference",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Model inference",
    "text": "Model inference\nWe conduct model inference to determine if different models are better at explaining variation. A common example is to compare a linear model (\\(g(\\hat Y)=\\hat\\beta_0 + \\hat\\beta_1 X\\)) to the mean of Y (\\(\\hat \\mu_y\\)). We determine the significance of the variation explained using an Analysis of Variance (ANOVA) table and F test."
  },
  {
    "objectID": "lectures/15.html#model-inference-1",
    "href": "lectures/15.html#model-inference-1",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven 2 models:\n\\[\ng(\\hat Y) = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\cdots + \\hat\\beta_p X_p\n\\]\nor\n\\[\ng(\\hat Y) = \\bar y\n\\]\n\nIs the model with predictors do a better job than using the average?"
  },
  {
    "objectID": "lectures/15.html#likelihood-ratio-test",
    "href": "lectures/15.html#likelihood-ratio-test",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nThe Likelihood Ratio Test is a test to determine whether the likelihood of observing the outcome is significantly bigger in a larger, more complicated model, than a simpler model.\nIt conducts a hypothesis tests to see if models are significantly different from each other."
  },
  {
    "objectID": "lectures/15.html#conducting-an-lrt-in-r",
    "href": "lectures/15.html#conducting-an-lrt-in-r",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Conducting an LRT in R",
    "text": "Conducting an LRT in R\n\n\nCode\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial)\nxlm0 &lt;- glm(Y ~ 1, data = DATA, family = binomial)\nanova(xlm0, xlm, test = \"LRT\")"
  },
  {
    "objectID": "lectures/15.html#example-2",
    "href": "lectures/15.html#example-2",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm0 &lt;- update(m1, formula. = ~ 1)\nanova(m0, m1, test = \"LRT\")\n\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: death ~ 1\n#&gt; Model 2: death ~ recur + number + size\n#&gt;   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \n#&gt; 1       293     189.38                         \n#&gt; 2       290     166.43  3   22.953 4.13e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/15.html#model-inference-2",
    "href": "lectures/15.html#model-inference-2",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Model Inference",
    "text": "Model Inference\nModel inference can be extended to compare models that have different number of predictors."
  },
  {
    "objectID": "lectures/15.html#model-inference-3",
    "href": "lectures/15.html#model-inference-3",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven:\n\\[\nM1:\\ g(\\hat y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\n\\[\nM2:\\ g(\\hat y) = \\beta_0 + \\beta_1 X_1  \n\\]\nLet \\(M1\\) be the FULL (larger) model, and let \\(M2\\) be the RED (Reduced, smaller) model."
  },
  {
    "objectID": "lectures/15.html#model-inference-4",
    "href": "lectures/15.html#model-inference-4",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Model Inference",
    "text": "Model Inference\nHe can test the following Hypothesis:\n\n\\(H_0\\): The error variations between the FULL and RED model are not different.\n\\(H_1\\): The error variations between the FULL and RED model are different."
  },
  {
    "objectID": "lectures/15.html#likelihood-ratio-test-in-r",
    "href": "lectures/15.html#likelihood-ratio-test-in-r",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Likelihood Ratio Test in R",
    "text": "Likelihood Ratio Test in R\n\n\nCode\nfull &lt;- glm(Y  ~  X1 + X2 + X3 + X4, DATA, family = binomial())\nred &lt;- glm(Y ~ X1 + X2, DATA, family = binomial())\nanova(red, full, test = \"LRT\")"
  },
  {
    "objectID": "lectures/15.html#example-3",
    "href": "lectures/15.html#example-3",
    "title": "Inference with Logistic and Poisson Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(death ~ number + size + recur, bladder1, family = binomial())\nm2 &lt;- glm(death ~ recur, bladder1, family = binomial())\nanova(m2, m1, test = \"LRT\")\n\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: death ~ recur\n#&gt; Model 2: death ~ number + size + recur\n#&gt;   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n#&gt; 1       292     168.72                     \n#&gt; 2       290     166.43  2   2.2883   0.3185"
  },
  {
    "objectID": "lectures/13.html#motivation-1",
    "href": "lectures/13.html#motivation-1",
    "title": "Inference",
    "section": "Motivation",
    "text": "Motivation\nThe ncbirths data set provides information on whether different predictors. The outcome of interest will be premie (“full term” or “premie”)."
  },
  {
    "objectID": "lectures/13.html#motivation-1-1",
    "href": "lectures/13.html#motivation-1-1",
    "title": "Inference",
    "section": "Motivation 1",
    "text": "Motivation 1\nModel the relationship between the outcome premie and habit (“smoker” and “nonsmoker”)."
  },
  {
    "objectID": "lectures/13.html#motivation-2",
    "href": "lectures/13.html#motivation-2",
    "title": "Inference",
    "section": "Motivation 2",
    "text": "Motivation 2\nModel the relationship between the outcome premie and visits (“smoker” and “nonsmoker”)."
  },
  {
    "objectID": "lectures/13.html#logistic-model-1",
    "href": "lectures/13.html#logistic-model-1",
    "title": "Inference",
    "section": "Logistic Model 1",
    "text": "Logistic Model 1\nUse a logistic regression to model the outcome premie and habit.\n\n\nCode\nglm(premie ~ habit, data = ncbirths, family = binomial())"
  },
  {
    "objectID": "lectures/13.html#odds-ratio-1",
    "href": "lectures/13.html#odds-ratio-1",
    "title": "Inference",
    "section": "Odds Ratio 1",
    "text": "Odds Ratio 1\n\n\nCode\nb(glm(premie ~ habit, data = ncbirths, family = binomial()), 1)\nexp(b(glm(premie ~ habit, data = ncbirths, family = binomial()), 1))"
  },
  {
    "objectID": "lectures/13.html#odds-ratio-1-1",
    "href": "lectures/13.html#odds-ratio-1-1",
    "title": "Inference",
    "section": "Odds Ratio 1",
    "text": "Odds Ratio 1\n\n\nCode\nb(glm(premie ~ habit, data = ncbirths, family = binomial()), 1)\n\n\n#&gt; [1] -0.01344105\n\n\nCode\nexp(b(glm(premie ~ habit, data = ncbirths, family = binomial()), 1))\n\n\n#&gt; [1] 0.9866489\n\n\nThe odds of a mother who smokes having a premature infant is 0.98 times lower than the odds of a mother who does not smoke."
  },
  {
    "objectID": "lectures/13.html#logistic-model-2",
    "href": "lectures/13.html#logistic-model-2",
    "title": "Inference",
    "section": "Logistic Model 2",
    "text": "Logistic Model 2\nUse a logistic regression to model the outcome premie and visits.\n\n\nCode\nglm(premie ~ visits, data = ncbirths, family = binomial())"
  },
  {
    "objectID": "lectures/13.html#odds-ratio-2",
    "href": "lectures/13.html#odds-ratio-2",
    "title": "Inference",
    "section": "Odds Ratio 2",
    "text": "Odds Ratio 2\n\n\nCode\nb(glm(premie ~ visits, data = ncbirths, family = binomial()), 1)\nexp(b(glm(premie ~ visits, data = ncbirths, family = binomial()), 1))"
  },
  {
    "objectID": "lectures/13.html#odds-ratio-2-1",
    "href": "lectures/13.html#odds-ratio-2-1",
    "title": "Inference",
    "section": "Odds Ratio 2",
    "text": "Odds Ratio 2\n\n\nCode\nb(glm(premie ~ visits, data = ncbirths, family = binomial()), 1)\n\n\n#&gt; [1] -0.1063097\n\n\nCode\nexp(b(glm(premie ~ visits, data = ncbirths, family = binomial()), 1))\n\n\n#&gt; [1] 0.8991461\n\n\nAs the number of hospital visits increases by 1, the odds of having a premature infant decreases by a factor 0.89."
  },
  {
    "objectID": "lectures/13.html#real-or-random",
    "href": "lectures/13.html#real-or-random",
    "title": "Inference",
    "section": "Real or Random",
    "text": "Real or Random\n\nDoes smoking have a protective effect on having a premature infant?\n\n\nDoes the number of hospital visits have a protective effect?\n\n\nWhat is real and what is random?"
  },
  {
    "objectID": "lectures/13.html#hypothesis-tests",
    "href": "lectures/13.html#hypothesis-tests",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nHypothesis tests are used to test whether claims are valid or not. This is conducted by collecting data, setting the Null and Alternative Hypothesis."
  },
  {
    "objectID": "lectures/13.html#null-hypothesis-h_0",
    "href": "lectures/13.html#null-hypothesis-h_0",
    "title": "Inference",
    "section": "Null Hypothesis \\(H_0\\)",
    "text": "Null Hypothesis \\(H_0\\)\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value."
  },
  {
    "objectID": "lectures/13.html#alternative-hypothesis-h_a",
    "href": "lectures/13.html#alternative-hypothesis-h_a",
    "title": "Inference",
    "section": "Alternative Hypothesis \\(H_a\\)",
    "text": "Alternative Hypothesis \\(H_a\\)\nThe alternative hypothesis contradicts the null hypothesis."
  },
  {
    "objectID": "lectures/13.html#example-of-null-and-alternative-hypothesis",
    "href": "lectures/13.html#example-of-null-and-alternative-hypothesis",
    "title": "Inference",
    "section": "Example of Null and Alternative Hypothesis",
    "text": "Example of Null and Alternative Hypothesis\nWe want to see if \\(\\beta\\) is different from \\(\\beta^*\\)\n\n\n\nNull Hypothesis\nAlternative Hypothesis\n\n\n\n\n\\(H_0: \\beta=\\beta^*\\)\n\\(H_a: \\beta\\ne\\beta^*\\)\n\n\n\\(H_0: \\beta\\le\\beta^*\\)\n\\(H_a: \\beta&gt;\\beta^*\\)\n\n\n\\(H_0: \\beta\\ge\\beta^*\\)\n\\(H_0: \\beta&lt;\\beta^*\\)"
  },
  {
    "objectID": "lectures/13.html#one-side-vs-two-side-hypothesis-tests",
    "href": "lectures/13.html#one-side-vs-two-side-hypothesis-tests",
    "title": "Inference",
    "section": "One-Side vs Two-Side Hypothesis Tests",
    "text": "One-Side vs Two-Side Hypothesis Tests\nNotice how there are 3 types of null and alternative hypothesis, The first type of hypothesis (\\(H_a:\\beta\\ne\\beta^*\\)) is considered a 2-sided hypothesis because the rejection region is located in 2 regions. The remaining two hypotheses are considered 1-sided because the rejection region is located on one side of the distribution.\n\n\n\nNull Hypothesis\nAlternative Hypothesis\nSide\n\n\n\n\n\\(H_0: \\beta=\\beta^*\\)\n\\(H_a: \\beta\\ne\\beta^*\\)\n2-Sided\n\n\n\\(H_0: \\beta\\le\\beta^*\\)\n\\(H_a: \\beta&gt;\\beta^*\\)\n1-Sided\n\n\n\\(H_0: \\beta\\ge\\beta^*\\)\n\\(H_0: \\beta&lt;\\beta^*\\)\n1-Sided"
  },
  {
    "objectID": "lectures/13.html#decision-making",
    "href": "lectures/13.html#decision-making",
    "title": "Inference",
    "section": "Decision Making",
    "text": "Decision Making\nHypothesis Testing will force you to make a decision: Reject \\(H_0\\) OR Fail to Reject \\(H_0\\)\n\nReject \\(H_0\\): The effect seen is not due to random chance, there is a process making contributing to the effect.\n\n\nFail to Reject \\(H_0\\): The effect seen is due to random chance. Random sampling is the reason why an effect is displayed, not an underlying process."
  },
  {
    "objectID": "lectures/13.html#decision-making-p-value",
    "href": "lectures/13.html#decision-making-p-value",
    "title": "Inference",
    "section": "Decision Making: P-Value",
    "text": "Decision Making: P-Value\nThe p-value approach is one of the most common methods to report significant results. It is easier to interpret the p-value because it provides the probability of observing our test statistics, or something more extreme, given that the null hypothesis is true.\n\nIf \\(p &lt; \\alpha\\), then you reject \\(H_0\\); otherwise, you will fail to reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/13.html#decision-making-confidence-interval-approach",
    "href": "lectures/13.html#decision-making-confidence-interval-approach",
    "title": "Inference",
    "section": "Decision Making: Confidence Interval Approach",
    "text": "Decision Making: Confidence Interval Approach\nThe confidence interval approach can evaluate a hypothesis test where the alternative hypothesis is \\(\\beta\\ne\\beta^*\\). The bootstrapping approach will result in a lower and upper bound denoted as: \\((LB, UB)\\).\n\nIf \\(\\beta^*\\) is in \\((LB, UB)\\), then you fail to reject \\(H_0\\). If \\(\\beta^*\\) is not in \\((LB,UB)\\), then you reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/13.html#significance-level-alpha",
    "href": "lectures/13.html#significance-level-alpha",
    "title": "Inference",
    "section": "Significance Level \\(\\alpha\\)",
    "text": "Significance Level \\(\\alpha\\)\nThe significance level \\(\\alpha\\) is the probability you will reject the null hypothesis given that it was true.\n\nIn other words, \\(\\alpha\\) is the error rate that a research controls.\n\n\nTypically, we want this error rate to be small (\\(\\alpha = 0.05\\))."
  },
  {
    "objectID": "lectures/13.html#hypothesis-test-process",
    "href": "lectures/13.html#hypothesis-test-process",
    "title": "Inference",
    "section": "Hypothesis Test Process",
    "text": "Hypothesis Test Process\n\nSet up the Null and Alternative Hypothesis.\n\n\\(H_0: \\beta = 0\\) and \\(H_a: \\beta \\ne 0\\)\n\nCompute p-value or confidence interval\nMake a decision\nInterpret the results"
  },
  {
    "objectID": "lectures/13.html#example",
    "href": "lectures/13.html#example",
    "title": "Inference",
    "section": "Example",
    "text": "Example\nUse a logistic regression to model the outcome premie and habit.\n\n\nCode\nglm(premie ~ habit, data = ncbirths, family = binomial())\nb(glm(premie ~ habit, data = ncbirths, family = binomial()), 1)\nexp(b(glm(premie ~ habit, data = ncbirths, family = binomial()), 1))"
  },
  {
    "objectID": "lectures/13.html#example-3",
    "href": "lectures/13.html#example-3",
    "title": "Inference",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "lectures/13.html#logistic-model-2-1",
    "href": "lectures/13.html#logistic-model-2-1",
    "title": "Inference",
    "section": "Logistic Model 2",
    "text": "Logistic Model 2\n\n\nCode\nglm(premie ~ visits, data = ncbirths, family = binomial())\n\nb(glm(premie ~ visits, data = ncbirths, family = binomial()), 1)\nexp(b(glm(premie ~ visits, data = ncbirths, family = binomial()), 1))"
  },
  {
    "objectID": "lectures/11.html#motivating-examples",
    "href": "lectures/11.html#motivating-examples",
    "title": "Statistical Inference",
    "section": "Motivating Examples",
    "text": "Motivating Examples\n\n\nCode\np1 &lt;- penguins |&gt; ggplot(aes(x=species, y = body_mass)) +\n  geom_jitter() + \n  geom_boxplot() + \n  labs(x = \"Species\", y = \"Body Mass\", title = \"Linear\")\n  \np2 &lt;- heart_disease |&gt; ggplot(aes(x=cp, fill = disease)) +\n  geom_bar(stat=\"count\", position=position_dodge()) +\n  labs(x = \"Chest Pain\", title = \"Logistic\") \np1 + p2"
  },
  {
    "objectID": "lectures/11.html#model-inference",
    "href": "lectures/11.html#model-inference",
    "title": "Statistical Inference",
    "section": "Model inference",
    "text": "Model inference\nWe conduct model inference to determine if different models are better at explaining variation. A common example is to compare a linear model (\\(\\hat Y=\\hat\\beta_0 + \\hat\\beta_1 X\\)) to the mean of Y (\\(\\hat \\mu_y\\)). We determine the significance of the variation explained using an Analysis of Variance (ANOVA) table and F test."
  },
  {
    "objectID": "lectures/11.html#linear-model-inference",
    "href": "lectures/11.html#linear-model-inference",
    "title": "Statistical Inference",
    "section": "Linear Model Inference",
    "text": "Linear Model Inference\nGiven 2 models:\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\cdots + \\hat\\beta_p X_p\n\\]\nor\n\\[\n\\hat Y = \\bar y\n\\]\n\nIs the model with predictors do a better job than using the average?"
  },
  {
    "objectID": "lectures/11.html#logistic-model-inference",
    "href": "lectures/11.html#logistic-model-inference",
    "title": "Statistical Inference",
    "section": "Logistic Model Inference",
    "text": "Logistic Model Inference\nGiven 2 models:\n\\[\ng(\\hat Y) = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\cdots + \\hat\\beta_p X_p\n\\]\nor\n\\[\ng(\\hat Y) = \\bar y\n\\]\n\nIs the model with predictors do a better job than using the average?"
  },
  {
    "objectID": "lectures/11.html#anova-linear",
    "href": "lectures/11.html#anova-linear",
    "title": "Statistical Inference",
    "section": "ANOVA (Linear)",
    "text": "ANOVA (Linear)"
  },
  {
    "objectID": "lectures/11.html#anova-table",
    "href": "lectures/11.html#anova-table",
    "title": "Statistical Inference",
    "section": "ANOVA Table",
    "text": "ANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nModel\n\\(DFR=k-1\\)\n\\(SSR\\)\n\\(MSR=\\frac{SSM}{DFR}\\)\n\\(\\hat F=\\frac{MSR}{MSE}\\)\n\n\nError\n\\(DFE=n-k\\)\n\\(SSE\\)\n\\(MSE=\\frac{SSE}{DFE}\\)\n\n\n\nTotal\n\\(TDF=n-1\\)\n\\(TSS=SSR+SSE\\)\n\n\n\n\n\n\\[\n\\hat F \\sim F(DFR, DFE)\n\\]"
  },
  {
    "objectID": "lectures/11.html#conducting-an-anova-in-r",
    "href": "lectures/11.html#conducting-an-anova-in-r",
    "title": "Statistical Inference",
    "section": "Conducting an ANOVA in R",
    "text": "Conducting an ANOVA in R\n\n\nCode\nxlm &lt;- lm(Y ~ X, data = DATA)\nanova(xlm)"
  },
  {
    "objectID": "lectures/11.html#likelihood-ratio-test-logistic",
    "href": "lectures/11.html#likelihood-ratio-test-logistic",
    "title": "Statistical Inference",
    "section": "Likelihood Ratio Test (Logistic)",
    "text": "Likelihood Ratio Test (Logistic)\nThe Likelihood Ratio Test is a test to determine whether the likelihood of observing the outcome is significantly bigger in a larger, more complicated model, than a simpler model.\nIt conducts a hypothesis tests to see if models are significantly different from each other."
  },
  {
    "objectID": "lectures/11.html#conducting-an-lrt-in-r",
    "href": "lectures/11.html#conducting-an-lrt-in-r",
    "title": "Statistical Inference",
    "section": "Conducting an LRT in R",
    "text": "Conducting an LRT in R\n\n\nCode\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial)\nanova(xlm)"
  },
  {
    "objectID": "lectures/1.html#statistical-warning",
    "href": "lectures/1.html#statistical-warning",
    "title": "Statistics",
    "section": "Statistical Warning",
    "text": "Statistical Warning"
  },
  {
    "objectID": "lectures/1.html#statistics-1",
    "href": "lectures/1.html#statistics-1",
    "title": "Statistics",
    "section": "Statistics",
    "text": "Statistics\n\n\n\n\n\n\n\n\n\nWith an increasing use of data to make decisions, Statistics has been essential for processing large amounts of data to byte-size information\nStatistics is also known as\n\nData Science\nMachine Learning\nArtificial Intelligence\n\nSo for today, we’re asking: what is Statistics?"
  },
  {
    "objectID": "lectures/1.html#what-does-google-say",
    "href": "lectures/1.html#what-does-google-say",
    "title": "Statistics",
    "section": "What does Google say?",
    "text": "What does Google say?\n\n\nUC Irvine\nStatistics is the science concerned with developing and studying methods for collecting, analyzing, interpreting and presenting empirical data.\n\nWikipedia\nStatistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data, or as a branch of mathematics."
  },
  {
    "objectID": "lectures/1.html#what-does-ai-say",
    "href": "lectures/1.html#what-does-ai-say",
    "title": "Statistics",
    "section": "What does AI say?",
    "text": "What does AI say?\n\n\nChatGPT\nStatistics is a branch of mathematics and a field of study that deals with the collection, analysis, interpretation, presentation, and organization of data.\n\nGoogle Gemini\nStatistics is the science of collecting, analyzing, interpreting, and presenting data."
  },
  {
    "objectID": "lectures/1.html#what-do-researchers-say",
    "href": "lectures/1.html#what-do-researchers-say",
    "title": "Statistics",
    "section": "What do researchers say?",
    "text": "What do researchers say?\n\nAnthropologistMicrobiologistSociologistPolitical ScientistEcologistMathematician\n\n\nObjectively interpreting data to make meaningful inferences about our predictions.\n\n\nWhatever the statistician says.\n\n\nGathering the narratives of individuals, groups, or society and telling a story about their past, present, or future. The numbers paint a picture worth many words.\n\n\nUsing numbers to try to explain behaviors and/or patterns in our world.\n\n\nStatistics is the way to make sense of the natural world by taking data we collect to identify patterns between variables, and applying statistical theory to make sure we are taking the right approach to data collection and analysis. Also, assess patterns to see if they are reproducible and provide a logical explanation that makes biological sense.\n\n\nStatistics is the study of data, patterns, and trends."
  },
  {
    "objectID": "lectures/1.html#what-is-it",
    "href": "lectures/1.html#what-is-it",
    "title": "Statistics",
    "section": "What is it?",
    "text": "What is it?\n\n\nMath\n\n\n\n\n\n\nScience"
  },
  {
    "objectID": "lectures/1.html#what-does-a-statistician-say",
    "href": "lectures/1.html#what-does-a-statistician-say",
    "title": "Statistics",
    "section": "What does a Statistician say?",
    "text": "What does a Statistician say?\nIt is the study of variation and randomness!\n\nUsing mathematics, we model randomness to characterizes commonality and variation!\n\n\nUsing science, we systematically refine models to better fit randomness in data!\n\n\nUsing art, when it all eventually fails!"
  },
  {
    "objectID": "lectures/1.html#when-it-fails",
    "href": "lectures/1.html#when-it-fails",
    "title": "Statistics",
    "section": "When it fails?!?!",
    "text": "When it fails?!?!"
  },
  {
    "objectID": "lectures/1.html#statistics-mantra---george-box",
    "href": "lectures/1.html#statistics-mantra---george-box",
    "title": "Statistics",
    "section": "Statistics Mantra - George Box",
    "text": "Statistics Mantra - George Box\n\n\n\n\n\n\n\n\nAll models are wrong,\nsome are useful!"
  },
  {
    "objectID": "lectures/1.html#what-is-the-formal-definition-of-statistics",
    "href": "lectures/1.html#what-is-the-formal-definition-of-statistics",
    "title": "Statistics",
    "section": "What is the formal definition of Statistics?",
    "text": "What is the formal definition of Statistics?\nStatistics is both the development of mathematical models to be used in real-world data and the analysis of data using existing models."
  },
  {
    "objectID": "lectures/1.html#probability-models",
    "href": "lectures/1.html#probability-models",
    "title": "Statistics",
    "section": "Probability Models",
    "text": "Probability Models\n\n\n\n\n\n\n\n\n\nModel observations that follow a new data generating process\nUnderstand its properties\nDevelop new probability distributions\nKnown as Probability Theory\nResearcher is a Probabilist or Mathematical Statistician"
  },
  {
    "objectID": "lectures/1.html#data-analysis",
    "href": "lectures/1.html#data-analysis",
    "title": "Statistics",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\n\nModel data with a known probability model\nAccount for sources of variation and bias\nAccount for violations of independence and randomness\nKnown as Statistician or Data Scientist"
  },
  {
    "objectID": "lectures/1.html#whats-the-goal-of-a-statistician",
    "href": "lectures/1.html#whats-the-goal-of-a-statistician",
    "title": "Statistics",
    "section": "What’s the goal of a Statistician?",
    "text": "What’s the goal of a Statistician?\n\nINFERENCE\n\n\nUse our sample data to understand the larger population.\n\n\nThe data will tell us how the population generally behaves.\n\n\nThe data will guide us in the differences in units.\n\n\nData will tell us if there is a signal or just noise."
  },
  {
    "objectID": "lectures/1.html#word-cloud",
    "href": "lectures/1.html#word-cloud",
    "title": "Statistics",
    "section": "Word Cloud",
    "text": "Word Cloud"
  },
  {
    "objectID": "lectures/1.html#conducting-inference",
    "href": "lectures/1.html#conducting-inference",
    "title": "Statistics",
    "section": "Conducting Inference",
    "text": "Conducting Inference"
  },
  {
    "objectID": "lectures/1.html#are-we-seeing-something-or-is-it-just-noise",
    "href": "lectures/1.html#are-we-seeing-something-or-is-it-just-noise",
    "title": "Statistics",
    "section": "Are we seeing something or is it just noise???",
    "text": "Are we seeing something or is it just noise???\nAre we seeing something different from what was expected? Or is it due to random chance?"
  },
  {
    "objectID": "lectures/1.html#hypothesis-testing",
    "href": "lectures/1.html#hypothesis-testing",
    "title": "Statistics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nSet up the Null and Alternative Hypothesis\nConstruct a test statistic based on the null hypothesis\nConstruct a distribution of the test statistic based on probability theory\nCompute the probability of observing the test statistic\nMake a decision based on the probability"
  },
  {
    "objectID": "lectures/1.html#what-if-we-cannot-construct-the-distribution",
    "href": "lectures/1.html#what-if-we-cannot-construct-the-distribution",
    "title": "Statistics",
    "section": "What if we cannot construct the distribution?!?!",
    "text": "What if we cannot construct the distribution?!?!\n\n\n\n\n\n\n\n\n\n\n\nWe bring out the Monte Carlo methods!"
  },
  {
    "objectID": "lectures/1.html#monte-carlo-methods",
    "href": "lectures/1.html#monte-carlo-methods",
    "title": "Statistics",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods\n\nMonte Carlo Methods are used to construct a distribution function of an obscure test statistic\nWe simulate a large number of data sets based on the null hypothesis\nWe construct a test statistic for each fake data set and the real one\nWe count how many data sets produce a test statistic that is more extreme than the real test statistic\n\\(p=\\#\\ of\\ extreme\\ data\\ sets\\ /\\ all\\ data\\ sets\\)"
  },
  {
    "objectID": "lectures/1.html#overview-of-research",
    "href": "lectures/1.html#overview-of-research",
    "title": "Statistics",
    "section": "Overview of Research",
    "text": "Overview of Research\n\nAsk a question about a population\nCollect data from a sample\nConstruct and test a hypothesis\nDraw conclusion about the population\nRefine your question and methodology"
  },
  {
    "objectID": "lectures/1.html#so-what-is-statistics",
    "href": "lectures/1.html#so-what-is-statistics",
    "title": "Statistics",
    "section": "So, what is Statistics?",
    "text": "So, what is Statistics?\n\n\n\n\n\n\nFor you, I’ll be anything"
  },
  {
    "objectID": "lectures/1.html#whats-statistics-without-a-little",
    "href": "lectures/1.html#whats-statistics-without-a-little",
    "title": "Statistics",
    "section": "What’s Statistics without a little …",
    "text": "What’s Statistics without a little …"
  },
  {
    "objectID": "lectures/1.html#train-of-thoughts",
    "href": "lectures/1.html#train-of-thoughts",
    "title": "Statistics",
    "section": "Train of Thoughts",
    "text": "Train of Thoughts\n\nThere are two train of thoughts on how to interpret estimates and probability.\n\n\nOne approach is the Frequentist approach.\n\n\nThe other approach is the Bayesian approach.\n\n\nBoth sides hate each other."
  },
  {
    "objectID": "lectures/1.html#frequentists",
    "href": "lectures/1.html#frequentists",
    "title": "Statistics",
    "section": "Frequentists",
    "text": "Frequentists"
  },
  {
    "objectID": "lectures/1.html#frequentists-1",
    "href": "lectures/1.html#frequentists-1",
    "title": "Statistics",
    "section": "Frequentists",
    "text": "Frequentists\nA frequentist, in the context of statistics, is an individual who adheres to the frequentist interpretation of probability and statistical inference.\n\nMeaning probability is obtained by the repetition of multiple experiments."
  },
  {
    "objectID": "lectures/1.html#bayesians",
    "href": "lectures/1.html#bayesians",
    "title": "Statistics",
    "section": "Bayesians",
    "text": "Bayesians"
  },
  {
    "objectID": "lectures/1.html#bayesians-1",
    "href": "lectures/1.html#bayesians-1",
    "title": "Statistics",
    "section": "Bayesians",
    "text": "Bayesians\nA Bayesians, in the context of statistics, is an individual who adheres to the Bayesian interpretation of probability and statistical inference.\n\nProbability is obtained by likelihood of an event to occur, given data and prior knowledge."
  },
  {
    "objectID": "lectures/1.html#what-am-i-and-people-that-have-lives",
    "href": "lectures/1.html#what-am-i-and-people-that-have-lives",
    "title": "Statistics",
    "section": "What am I (and people that have lives)?",
    "text": "What am I (and people that have lives)?\n\n\nWhatever gets the job done!"
  },
  {
    "objectID": "lectures/1.html#there-is-more-much-more-but-i-will-say-this-in-my-statistical-journey",
    "href": "lectures/1.html#there-is-more-much-more-but-i-will-say-this-in-my-statistical-journey",
    "title": "Statistics",
    "section": "There is more, much more, but I will say this, in my statistical journey",
    "text": "There is more, much more, but I will say this, in my statistical journey"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Elementary Statistics!",
    "section": "",
    "text": "Brief Introduction\n\n\n\n\n\nWelcome to the course! This is the home page of the course where I will provide a recap on what was covered in the week. Here I will post any documents or videos for your reference. If you have any questions, please email me at isaac.qs@csuci.edu."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Categorical Data\n\n\n\n\n\nProvides an overview descriptive statistics and data visualization techniques for categorical data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup Regression Models\n\n\n\n\n\nDiscuss using categorical predictor variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\nBegins the discussion on Inference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference with Linear Regression\n\n\n\n\n\nBegins the discussion for linear regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference with Logistic Regression\n\n\n\n\n\nBegins the discussion on inference for logistic and Poisson regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference with Logistic and Poisson Regression\n\n\n\n\n\nBegins the discussion on inference for logistic and Poisson regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling R and Rstudio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Inference\n\n\n\n\n\nMake the connection between traditional statisitcal inference an regression inference.\n\n\n\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariable Linear Regression\n\n\n\n\n\nContinues the discussion with multi linear regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical Data\n\n\n\n\n\nProvides an overview descriptive statistics and data visualization techniques for numerical data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\nBegins the discussion for linear regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\n\n\n\n\nStatistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\n\nTesting Different Groups\n\n\nTesting the difference in groups.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\nA Math? A Science? An Art? Or Something Else?\n\n\n\n\n\n\n\n\nIsaac Quintanilla Salinas\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/10.html#what-is-statistical-inference",
    "href": "lectures/10.html#what-is-statistical-inference",
    "title": "Statistical Inference",
    "section": "What is Statistical Inference?",
    "text": "What is Statistical Inference?\n\nDrawing conclusions about a population based on a sample\nPopulation = entire group\nSample = subset\n\n\nIntroduce the big idea: We want to make st"
  },
  {
    "objectID": "lectures/10.html#two-main-types-of-inference",
    "href": "lectures/10.html#two-main-types-of-inference",
    "title": "Statistical Inference",
    "section": "Two Main Types of Inference",
    "text": "Two Main Types of Inference\n\nEstimation\nHypothesis Testing\n\n\nWe’ll be focusing on two fundamental techniques in inference. First, estimating population values (like the mean), and second, testing claims about the population."
  },
  {
    "objectID": "lectures/10.html#estimation",
    "href": "lectures/10.html#estimation",
    "title": "Statistical Inference",
    "section": "Estimation",
    "text": "Estimation\n\nPoint Estimate: Single best guess (e.g., \\(\\hat \\beta_1\\))\nInterval Estimate: Range likely to contain the true value\n\n\nPoint estimates are easy but not very informative. Intervals give us a sense of uncertainty, which is critical in inference."
  },
  {
    "objectID": "lectures/10.html#hypothesis-testing",
    "href": "lectures/10.html#hypothesis-testing",
    "title": "Statistical Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(H_0\\): No effect or difference\n\n\\(H_1\\): Some effect or difference\n\nWe use sample data to support or reject \\(H_0\\)\n\n\nMention that \\(H_0\\) is the default assumption. We only reject it if the data give us strong enough evidence."
  },
  {
    "objectID": "lectures/10.html#key-concepts-and-tools",
    "href": "lectures/10.html#key-concepts-and-tools",
    "title": "Statistical Inference",
    "section": "Key Concepts and Tools",
    "text": "Key Concepts and Tools\n\nSampling Distribution\nCentral Limit Theorem\nStandard Error\n\n\nThese three concepts are foundational. Understanding them helps us assess how reliable our estimates are."
  },
  {
    "objectID": "lectures/10.html#p-values",
    "href": "lectures/10.html#p-values",
    "title": "Statistical Inference",
    "section": "p-values",
    "text": "p-values\n\nProbability of observing data as extreme as this if \\(H_0\\) is true\n\nMisinterpretation of p-values is common. Emphasize: low p-value means data is unusual under \\(H_0\\)."
  },
  {
    "objectID": "lectures/10.html#confidence-intervals",
    "href": "lectures/10.html#confidence-intervals",
    "title": "Statistical Inference",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nA range where we expect the true value to fall\n\n\nClarify interpretation: it’s not about the probability the parameter is inside the interval, but about the method producing accurate intervals in the long run."
  },
  {
    "objectID": "lectures/10.html#hypothesis-tests",
    "href": "lectures/10.html#hypothesis-tests",
    "title": "Statistical Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nHypothesis tests are used to test whether claims are valid or not. This is conducted by collecting data, setting the Null and Alternative Hypothesis."
  },
  {
    "objectID": "lectures/10.html#null-hypothesis-h_0",
    "href": "lectures/10.html#null-hypothesis-h_0",
    "title": "Statistical Inference",
    "section": "Null Hypothesis \\(H_0\\)",
    "text": "Null Hypothesis \\(H_0\\)\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value."
  },
  {
    "objectID": "lectures/10.html#alternative-hypothesis-h_a",
    "href": "lectures/10.html#alternative-hypothesis-h_a",
    "title": "Statistical Inference",
    "section": "Alternative Hypothesis \\(H_a\\)",
    "text": "Alternative Hypothesis \\(H_a\\)\nThe alternative hypothesis contradicts the null hypothesis."
  },
  {
    "objectID": "lectures/10.html#example-of-null-and-alternative-hypothesis",
    "href": "lectures/10.html#example-of-null-and-alternative-hypothesis",
    "title": "Statistical Inference",
    "section": "Example of Null and Alternative Hypothesis",
    "text": "Example of Null and Alternative Hypothesis\nWe want to see if \\(\\beta\\) is different from \\(\\beta^*\\)\n\n\n\nNull Hypothesis\nAlternative Hypothesis\n\n\n\n\n\\(H_0: \\beta=\\beta^*\\)\n\\(H_a: \\beta\\ne\\beta^*\\)\n\n\n\\(H_0: \\beta\\le\\beta^*\\)\n\\(H_a: \\beta&gt;\\beta^*\\)\n\n\n\\(H_0: \\beta\\ge\\beta^*\\)\n\\(H_0: \\beta&lt;\\beta^*\\)"
  },
  {
    "objectID": "lectures/10.html#one-side-vs-two-side-hypothesis-tests",
    "href": "lectures/10.html#one-side-vs-two-side-hypothesis-tests",
    "title": "Statistical Inference",
    "section": "One-Side vs Two-Side Hypothesis Tests",
    "text": "One-Side vs Two-Side Hypothesis Tests\nNotice how there are 3 types of null and alternative hypothesis, The first type of hypothesis (\\(H_a:\\beta\\ne\\beta^*\\)) is considered a 2-sided hypothesis because the rejection region is located in 2 regions. The remaining two hypotheses are considered 1-sided because the rejection region is located on one side of the distribution.\n\n\n\nNull Hypothesis\nAlternative Hypothesis\nSide\n\n\n\n\n\\(H_0: \\beta=\\beta^*\\)\n\\(H_a: \\beta\\ne\\beta^*\\)\n2-Sided\n\n\n\\(H_0: \\beta\\le\\beta^*\\)\n\\(H_a: \\beta&gt;\\beta^*\\)\n1-Sided\n\n\n\\(H_0: \\beta\\ge\\beta^*\\)\n\\(H_0: \\beta&lt;\\beta^*\\)\n1-Sided"
  },
  {
    "objectID": "lectures/10.html#hypothesis-testing-steps",
    "href": "lectures/10.html#hypothesis-testing-steps",
    "title": "Statistical Inference",
    "section": "Hypothesis Testing Steps",
    "text": "Hypothesis Testing Steps\n\nState \\(H_0\\) and \\(H_1\\)\nChoose \\(\\alpha\\)\nCompute confidence interval/p-value\nMake a decision\n\n\nWalk through the steps slowly with an example in mind. Emphasize that \\(\\alpha\\) is a threshold, not the actual probability of error."
  },
  {
    "objectID": "lectures/10.html#rejection-region",
    "href": "lectures/10.html#rejection-region",
    "title": "Statistical Inference",
    "section": "Rejection Region",
    "text": "Rejection Region"
  },
  {
    "objectID": "lectures/10.html#rejection-region-1",
    "href": "lectures/10.html#rejection-region-1",
    "title": "Statistical Inference",
    "section": "Rejection Region",
    "text": "Rejection Region\n\n\nCode\nalpha &lt;- 0.05\n\n# Critical values for two-tailed test\nz_critical &lt;- qnorm(1 - alpha / 2)\n\n# Create data for the normal curve\nx &lt;- seq(-4, 4, length = 1000)\ny &lt;- dnorm(x)\n\ndf &lt;- data.frame(x = x, y = y)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line(color = \"deepskyblue\", linewidth = 1) +\n  geom_area(data = subset(df, x &lt;= -z_critical), aes(y = y), fill = \"firebrick\", alpha = 0.5) +\n  geom_area(data = subset(df, x &gt;= z_critical), aes(y = y), fill = \"firebrick\", alpha = 0.5) +\n  geom_vline(xintercept = c(-z_critical, z_critical), linetype = \"dashed\", color = \"black\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/10.html#decision-making-1",
    "href": "lectures/10.html#decision-making-1",
    "title": "Statistical Inference",
    "section": "Decision Making",
    "text": "Decision Making\nHypothesis Testing will force you to make a decision: Reject \\(H_0\\) OR Fail to Reject \\(H_0\\)\n\nReject \\(H_0\\): The effect seen is not due to random chance, there is a process making contributing to the effect.\n\n\nFail to Reject \\(H_0\\): The effect seen is due to random chance. Random sampling is the reason why an effect is displayed, not an underlying process."
  },
  {
    "objectID": "lectures/10.html#decision-making-p-value",
    "href": "lectures/10.html#decision-making-p-value",
    "title": "Statistical Inference",
    "section": "Decision Making: P-Value",
    "text": "Decision Making: P-Value\nThe p-value approach is one of the most common methods to report significant results. It is easier to interpret the p-value because it provides the probability of observing our test statistics, or something more extreme, given that the null hypothesis is true.\n\nIf \\(p &lt; \\alpha\\), then you reject \\(H_0\\); otherwise, you will fail to reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/10.html#significance-level-alpha",
    "href": "lectures/10.html#significance-level-alpha",
    "title": "Statistical Inference",
    "section": "Significance Level \\(\\alpha\\)",
    "text": "Significance Level \\(\\alpha\\)\nThe significance level \\(\\alpha\\) is the probability you will reject the null hypothesis given that it was true.\n\nIn other words, \\(\\alpha\\) is the error rate that a research controls.\n\n\nTypically, we want this error rate to be small (\\(\\alpha = 0.05\\))."
  },
  {
    "objectID": "lectures/10.html#confidence-intervals-2",
    "href": "lectures/10.html#confidence-intervals-2",
    "title": "Statistical Inference",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nA confidence interval gives a range of plausible values for a population parameter.\nIt reflects uncertainty in point estimates from sample data.\n\n\nIntroduce confidence intervals as the natural next step after understanding sampling variability and standard error. Emphasize that point estimates are useful, but intervals give a more complete picture."
  },
  {
    "objectID": "lectures/10.html#interpretation",
    "href": "lectures/10.html#interpretation",
    "title": "Statistical Inference",
    "section": "Interpretation",
    "text": "Interpretation\n\n“We are 95% confident that the true mean lies between A and B.”\n\n\nThis does not mean there’s a 95% chance the mean is in that interval.\nIt means: if we repeated the sampling process many times, 95% of the intervals would contain the true value.\n\n\nThis is one of the most common misconceptions. Clarify that the confidence is in the method, not any one interval."
  },
  {
    "objectID": "lectures/10.html#factors-affecting-ci-width",
    "href": "lectures/10.html#factors-affecting-ci-width",
    "title": "Statistical Inference",
    "section": "Factors Affecting CI Width",
    "text": "Factors Affecting CI Width\n\nSample size (\\(n\\)): larger \\(n\\) → narrower CI\n\nStandard deviation (\\(s\\) or \\(\\sigma\\)): more variability → wider CI\n\nConfidence level: higher confidence → wider CI\n\n\nUse this to summarize what controls how “precise” our confidence interval is. Give examples of each."
  },
  {
    "objectID": "lectures/10.html#decision-making-confidence-interval-approach",
    "href": "lectures/10.html#decision-making-confidence-interval-approach",
    "title": "Statistical Inference",
    "section": "Decision Making: Confidence Interval Approach",
    "text": "Decision Making: Confidence Interval Approach\nThe confidence interval approach can evaluate a hypothesis test where the alternative hypothesis is \\(\\beta\\ne\\beta^*\\). The confidence interval approach will result in a lower and upper bound denoted as: \\((LB, UB)\\).\n\nIf \\(\\beta^*\\) is in \\((LB, UB)\\), then you fail to reject \\(H_0\\). If \\(\\beta^*\\) is not in \\((LB,UB)\\), then you reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/10.html#conducting-ht-of-beta_j",
    "href": "lectures/10.html#conducting-ht-of-beta_j",
    "title": "Statistical Inference",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\nxlm &lt;- lm(Y ~ X, data = DATA)\ntidy(xlm)\n\nxlm: Object where the model is stored\nY: Name of the outcome variable in DATA\nX: Name of the Predictor Variable(s) in DATA\nDATA: Name of the data set"
  },
  {
    "objectID": "lectures/10.html#example",
    "href": "lectures/10.html#example",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\nIs there a significant relationship between penguin body mass (outcome; body_mass) and flipper length (predictor; flipper_len)? Use the penguins data set to determine a significant association."
  },
  {
    "objectID": "lectures/10.html#example-1",
    "href": "lectures/10.html#example-1",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- lm(body_mass ~ flipper_len, penguins)\ntidy(m1)\n\n\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)  -5872.     310.       -18.9 1.18e- 54\n#&gt; 2 flipper_len     50.2      1.54      32.6 3.13e-105"
  },
  {
    "objectID": "lectures/10.html#confidence-interval",
    "href": "lectures/10.html#confidence-interval",
    "title": "Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nconfint(xlm, level = LEVEL)\n\nxlm: Name of the model saved in R\nLEVEL: A number between 0 and 1 to specify confidence level"
  },
  {
    "objectID": "lectures/10.html#example-2",
    "href": "lectures/10.html#example-2",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\ntidy(m1, conf.int = TRUE, conf.level = 0.9)\n\n\n#&gt; # A tibble: 2 × 7\n#&gt;   term        estimate std.error statistic   p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)  -5872.     310.       -18.9 1.18e- 54  -6384.    -5360. \n#&gt; 2 flipper_len     50.2      1.54      32.6 3.13e-105     47.6      52.7"
  },
  {
    "objectID": "lectures/10.html#mammals-sleep-data-set",
    "href": "lectures/10.html#mammals-sleep-data-set",
    "title": "Statistical Inference",
    "section": "Mammals Sleep Data Set",
    "text": "Mammals Sleep Data Set\nThe msleep data set contains information on sleeping patterns for mammals. We are interested in understanding the relationship of the length of sleep cycle (sleep_cycle; in hours) and rem sleep (sleep_rem; rapid eye movement; in hours)."
  },
  {
    "objectID": "lectures/10.html#red-wine-data",
    "href": "lectures/10.html#red-wine-data",
    "title": "Statistical Inference",
    "section": "Red Wine Data",
    "text": "Red Wine Data\nThe Wine Quality data set contains data on information on both red and white wine from North Portugal. We are interested in seeing if pH of the red wine (predictor variable) affects the quality (outcome variable).\n\n\nCode\nurl &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\nwine &lt;- read_delim(url, delim = \";\")"
  },
  {
    "objectID": "lectures/10.html#conducting-ht-of-beta_j-1",
    "href": "lectures/10.html#conducting-ht-of-beta_j-1",
    "title": "Statistical Inference",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial())\ntidy(xlm)\n\nxlm: Object where the model is stored\nY: Name of the outcome variable in DATA\nX: Name of the Predictor Variable(s) in DATA\nDATA: Name of the data set"
  },
  {
    "objectID": "lectures/10.html#example-3",
    "href": "lectures/10.html#example-3",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\nIs there a significant association between heart disease (outcome; disease) and resting blood pressure (predictor; trestbps). Use the heart_disease data set to determine a significant association."
  },
  {
    "objectID": "lectures/10.html#example-4",
    "href": "lectures/10.html#example-4",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(disease ~ trestbps, heart_disease, family = binomial())\ntidy(m1)\n\n\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 (Intercept)  -2.49     0.905       -2.76 0.00586\n#&gt; 2 trestbps      0.0177   0.00681      2.61 0.00914"
  },
  {
    "objectID": "lectures/10.html#confidence-interval-1",
    "href": "lectures/10.html#confidence-interval-1",
    "title": "Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\ntidy(xlm, conf.int = TRUE, conf.level = LEVEL)\n\nxlm: Name of the model saved in R\nLEVEL: A number between 0 and 1 to specify confidence level\ndefaults to 0.95"
  },
  {
    "objectID": "lectures/10.html#example-5",
    "href": "lectures/10.html#example-5",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\ntidy(m1, conf.int = TRUE)\n\n\n#&gt; # A tibble: 2 × 7\n#&gt;   term        estimate std.error statistic p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)  -2.49     0.905       -2.76 0.00586 -4.31      -0.747 \n#&gt; 2 trestbps      0.0177   0.00681      2.61 0.00914  0.00461    0.0314"
  },
  {
    "objectID": "lectures/10.html#odds-ratio",
    "href": "lectures/10.html#odds-ratio",
    "title": "Statistical Inference",
    "section": "Odds Ratio",
    "text": "Odds Ratio"
  },
  {
    "objectID": "lectures/10.html#odds-ratio-confidence-intervat",
    "href": "lectures/10.html#odds-ratio-confidence-intervat",
    "title": "Statistical Inference",
    "section": "Odds Ratio & Confidence Intervat",
    "text": "Odds Ratio & Confidence Intervat\ntidy(xlm, exponentiate = TRUE, conf.int = TRUE)\n\nxlm: Name of the model saved in R"
  },
  {
    "objectID": "lectures/10.html#example-6",
    "href": "lectures/10.html#example-6",
    "title": "Statistical Inference",
    "section": "Example",
    "text": "Example\n\n\nCode\ntidy(m1, exponentiate = TRUE, conf.int = TRUE)\n\n\n#&gt; # A tibble: 2 × 7\n#&gt;   term        estimate std.error statistic p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)   0.0826   0.905       -2.76 0.00586   0.0135     0.474\n#&gt; 2 trestbps      1.02     0.00681      2.61 0.00914   1.00       1.03"
  },
  {
    "objectID": "lectures/10.html#breast-cancer-data",
    "href": "lectures/10.html#breast-cancer-data",
    "title": "Statistical Inference",
    "section": "Breast Cancer Data",
    "text": "Breast Cancer Data\nThe Breast Cancer data set contains information about image diagnosis of individuals from Wisconsin. We are interested if breast cancer diagnosis (outcome variable; Benign or Malignant), is affected by tumor radius.\n\n\nCode\nurl &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\nbc &lt;- read.csv(url, header = FALSE)\n\n# Add column names\ncolnames(bc) &lt;- c(\"id\", \"diagnosis\", \"radius\", \"texture\", \"perimeter\", \"area\", \"smoothness\",\n                  \"compactness\", \"concavity\", paste0(\"V\", 10:32))\n\n# Convert diagnosis to factor\nbc$diagnosis &lt;- factor(bc$diagnosis, levels = c(\"B\", \"M\"), labels = c(\"Benign\", \"Malignant\"))"
  },
  {
    "objectID": "lectures/10.html#bank-note-classification",
    "href": "lectures/10.html#bank-note-classification",
    "title": "Statistical Inference",
    "section": "Bank Note Classification",
    "text": "Bank Note Classification\nThe Bank Note data set contains information about bank note authentication based on images. We are interested in seeing if class (outcome variable; real or fake) is associated by image entropy (predictor).\n\n\nCode\nurl &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\nbank &lt;- read.csv(url, header = FALSE)\n\ncolnames(bank) &lt;- c(\"variance\", \"skewness\", \"curtosis\", \"entropy\", \"class\")\nbank$class &lt;- factor(bank$class, levels = c(0, 1), labels = c(\"Genuine\", \"Forged\"))"
  },
  {
    "objectID": "lectures/12.html#motivating-example-1",
    "href": "lectures/12.html#motivating-example-1",
    "title": "Inference with Logistic Regression",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nCode\nbladder1 |&gt; ggplot(aes(number, color = death2)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/12.html#hypothesis",
    "href": "lectures/12.html#hypothesis",
    "title": "Inference with Logistic Regression",
    "section": "Hypothesis",
    "text": "Hypothesis\n\n\n\\[H_0: \\beta = \\theta\\]\n\n\\[H_0: \\beta \\ne \\theta\\]"
  },
  {
    "objectID": "lectures/12.html#testing-beta_j",
    "href": "lectures/12.html#testing-beta_j",
    "title": "Inference with Logistic Regression",
    "section": "Testing \\(\\beta_j\\)",
    "text": "Testing \\(\\beta_j\\)\n\\[\n\\frac{\\hat\\beta_j - \\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "lectures/12.html#confidence-intervals",
    "href": "lectures/12.html#confidence-intervals",
    "title": "Inference with Logistic Regression",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\\[\nPE \\pm CV \\times SE\n\\]\n\nPE: Point Estimate\nCV: Critical Value \\(P(X&lt;CV) = 1-\\alpha/2\\)\n\\(\\alpha\\): significance level\nSE: Standard Error"
  },
  {
    "objectID": "lectures/12.html#conducting-ht-of-beta_j",
    "href": "lectures/12.html#conducting-ht-of-beta_j",
    "title": "Inference with Logistic Regression",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\n\n\nCode\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial())\nsummary(xlm)"
  },
  {
    "objectID": "lectures/12.html#example",
    "href": "lectures/12.html#example",
    "title": "Inference with Logistic Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(death ~ recur + number + size, bladder1, family = binomial())\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = death ~ recur + number + size, family = binomial(), \n#&gt;     data = bladder1)\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.8525259  0.4462559  -1.910 0.056082 .  \n#&gt; recur       -0.3897480  0.1062848  -3.667 0.000245 ***\n#&gt; number       0.0008451  0.1124503   0.008 0.994004    \n#&gt; size        -0.2240419  0.1626749  -1.377 0.168439    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 189.38  on 293  degrees of freedom\n#&gt; Residual deviance: 166.43  on 290  degrees of freedom\n#&gt; AIC: 174.43\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "lectures/12.html#confidence-interval",
    "href": "lectures/12.html#confidence-interval",
    "title": "Inference with Logistic Regression",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n\nCode\nconfint(xlm, level = LEVEL)"
  },
  {
    "objectID": "lectures/12.html#example-1",
    "href": "lectures/12.html#example-1",
    "title": "Inference with Logistic Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nconfint(m1, level = 0.95)\n\n\n#&gt;                  2.5 %      97.5 %\n#&gt; (Intercept) -1.7353779  0.02529523\n#&gt; recur       -0.6217831 -0.20078281\n#&gt; number      -0.2421738  0.20731479\n#&gt; size        -0.5880581  0.06061498"
  },
  {
    "objectID": "lectures/12.html#confidence-interval-for-odds-ratio",
    "href": "lectures/12.html#confidence-interval-for-odds-ratio",
    "title": "Inference with Logistic Regression",
    "section": "Confidence Interval for Odds Ratio",
    "text": "Confidence Interval for Odds Ratio\n\n\nCode\nexp(confint(m1, level = 0.95))\n\n\n#&gt;                 2.5 %    97.5 %\n#&gt; (Intercept) 0.1763335 1.0256179\n#&gt; recur       0.5369861 0.8180901\n#&gt; number      0.7849197 1.2303698\n#&gt; size        0.5554048 1.0624898"
  },
  {
    "objectID": "lectures/12.html#model-inference",
    "href": "lectures/12.html#model-inference",
    "title": "Inference with Logistic Regression",
    "section": "Model inference",
    "text": "Model inference\nWe conduct model inference to determine if different models are better at explaining variation. A common example is to compare a linear model (\\(g(\\hat Y)=\\hat\\beta_0 + \\hat\\beta_1 X\\)) to the mean of Y (\\(\\hat \\mu_y\\)). We determine the significance of the variation explained using an Analysis of Variance (ANOVA) table and F test."
  },
  {
    "objectID": "lectures/12.html#conducting-an-lrt-in-r",
    "href": "lectures/12.html#conducting-an-lrt-in-r",
    "title": "Inference with Logistic Regression",
    "section": "Conducting an LRT in R",
    "text": "Conducting an LRT in R\n\n\nCode\nxlm &lt;- glm(Y ~ X, data = DATA, family = binomial)\nxlm0 &lt;- glm(Y ~ 1, data = DATA, family = binomial)\nanova(xlm0, xlm, test = \"LRT\")"
  },
  {
    "objectID": "lectures/12.html#example-2",
    "href": "lectures/12.html#example-2",
    "title": "Inference with Logistic Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm0 &lt;- update(m1, formula. = ~ 1)\nanova(m0, m1, test = \"LRT\")\n\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: death ~ 1\n#&gt; Model 2: death ~ recur + number + size\n#&gt;   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \n#&gt; 1       293     189.38                         \n#&gt; 2       290     166.43  3   22.953 4.13e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/12.html#model-inference-1",
    "href": "lectures/12.html#model-inference-1",
    "title": "Inference with Logistic Regression",
    "section": "Model Inference",
    "text": "Model Inference\nModel inference can be extended to compare models that have different number of predictors."
  },
  {
    "objectID": "lectures/12.html#model-inference-2",
    "href": "lectures/12.html#model-inference-2",
    "title": "Inference with Logistic Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven:\n\\[\nM1:\\ g(\\hat y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\n\\[\nM2:\\ g(\\hat y) = \\beta_0 + \\beta_1 X_1  \n\\]\nLet \\(M1\\) be the FULL (larger) model, and let \\(M2\\) be the RED (Reduced, smaller) model."
  },
  {
    "objectID": "lectures/12.html#model-inference-3",
    "href": "lectures/12.html#model-inference-3",
    "title": "Inference with Logistic Regression",
    "section": "Model Inference",
    "text": "Model Inference\nHe can test the following Hypothesis:\n\n\\(H_0\\): The error variations between the FULL and RED model are not different.\n\\(H_1\\): The error variations between the FULL and RED model are different."
  },
  {
    "objectID": "lectures/12.html#likelihood-ratio-test-in-r",
    "href": "lectures/12.html#likelihood-ratio-test-in-r",
    "title": "Inference with Logistic Regression",
    "section": "Likelihood Ratio Test in R",
    "text": "Likelihood Ratio Test in R\n\n\nCode\nfull &lt;- glm(Y  ~  X1 + X2 + X3 + X4, DATA, family = binomial())\nred &lt;- glm(Y ~ X1 + X2, DATA, family = binomial())\nanova(red, full, test = \"LRT\")"
  },
  {
    "objectID": "lectures/12.html#example-3",
    "href": "lectures/12.html#example-3",
    "title": "Inference with Logistic Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- glm(death ~ number + size + recur, bladder1, family = binomial())\nm2 &lt;- glm(death ~ recur, bladder1, family = binomial())\nanova(m2, m1, test = \"LRT\")\n\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: death ~ recur\n#&gt; Model 2: death ~ number + size + recur\n#&gt;   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n#&gt; 1       292     168.72                     \n#&gt; 2       290     166.43  2   2.2883   0.3185"
  },
  {
    "objectID": "lectures/14.html#motivating-example-1",
    "href": "lectures/14.html#motivating-example-1",
    "title": "Inference with Linear Regression",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nCode\np1 &lt;- penguins |&gt; ggplot(aes(x=species, y = body_mass_g)) +\n  geom_jitter() + \n  geom_boxplot() + \n  labs(x = \"Species\", y = \"Body Mass\")\n  \np2 &lt;- penguins |&gt; ggplot(aes(x=flipper_length_mm, y = body_mass_g)) +\n  geom_point() + \n  labs(x = \"Flipper Length\", y = \"Body Mass\")  \n\np1 + p2"
  },
  {
    "objectID": "lectures/14.html#mathematical-models-1",
    "href": "lectures/14.html#mathematical-models-1",
    "title": "Inference with Linear Regression",
    "section": "Mathematical Models",
    "text": "Mathematical Models"
  },
  {
    "objectID": "lectures/14.html#standard-normal-distribution",
    "href": "lectures/14.html#standard-normal-distribution",
    "title": "Inference with Linear Regression",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\n\n\\[\n{\\frac{1}{\\sqrt{2 \\pi}}} e^{-\\frac{1}{2}x^2}\n\\]\n\n\n\nCode\ndata.frame(x = seq(-5,5, length.out = 100), \n           y1 = dt(seq(-5,5, length.out = 100), 1),\n           y2 = dt(seq(-5,5, length.out = 100), 10),\n           y3 = dt(seq(-5,5, length.out = 100), 30),\n           y4 = dt(seq(-5,5, length.out = 100), 100),\n           y5 = dnorm((seq(-5,5, length.out = 100)))) |&gt; \n  ggplot() +\n  # geom_line(aes(x, y1, color = \"1\")) +\n  # geom_line(aes(x, y2, color = \"10\")) +\n  # geom_line(aes(x, y3, color = \"30\")) +\n  # geom_line(aes(x, y4, color = \"100\")) +\n  geom_line(aes(x, y5)) +\n  ylab(\"y\")"
  },
  {
    "objectID": "lectures/14.html#t-distribution",
    "href": "lectures/14.html#t-distribution",
    "title": "Inference with Linear Regression",
    "section": "t Distribution",
    "text": "t Distribution\n\n\n\\[\n\\frac{\\Gamma \\left(\\frac{v+1}{2}\\right)}{\\sqrt{\\pi v}\\Gamma\\left(\\frac{v}{2}\\right)} \\left(1 + \\frac{x^2}{v}\\right)^{-\\frac{v+1}{2}}\n\\]\n\n\n\nCode\ndata.frame(x = seq(-5,5, length.out = 100), \n           y1 = dt(seq(-5,5, length.out = 100), 1),\n           y2 = dt(seq(-5,5, length.out = 100), 10),\n           y3 = dt(seq(-5,5, length.out = 100), 30),\n           y4 = dt(seq(-5,5, length.out = 100), 100),\n           y5 = dnorm((seq(-5,5, length.out = 100)))) |&gt; \n  ggplot() +\n  geom_line(aes(x, y1, color = \"1\")) +\n  geom_line(aes(x, y2, color = \"10\")) +\n  geom_line(aes(x, y3, color = \"30\")) +\n  geom_line(aes(x, y4, color = \"100\")) +\n  geom_line(aes(x, y5, color = \"Normal\")) +\n  ylab(\"y\")"
  },
  {
    "objectID": "lectures/14.html#hypothesis",
    "href": "lectures/14.html#hypothesis",
    "title": "Inference with Linear Regression",
    "section": "Hypothesis",
    "text": "Hypothesis\n\n\n\\[H_0: \\beta = \\theta\\]\n\n\\[H_0: \\beta \\ne \\theta\\]"
  },
  {
    "objectID": "lectures/14.html#testing-beta_j",
    "href": "lectures/14.html#testing-beta_j",
    "title": "Inference with Linear Regression",
    "section": "Testing \\(\\beta_j\\)",
    "text": "Testing \\(\\beta_j\\)\n\\[\nT = \\frac{\\hat\\beta_j-\\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n\\]\n\n\\(n\\): sample size\n\\(p^\\prime\\): number of \\(\\beta\\)s"
  },
  {
    "objectID": "lectures/14.html#p-value",
    "href": "lectures/14.html#p-value",
    "title": "Inference with Linear Regression",
    "section": "P-Value",
    "text": "P-Value\n\n\n\nAlternative Hypothesis\np-value\n\n\n\n\n\\(\\beta&gt;\\theta\\)\n\\(P(\\hat\\beta &gt;T)=p\\)\n\n\n\\(\\beta&lt;\\theta\\)\n\\(P(\\hat\\beta &lt; T)=p\\)\n\n\n\\(\\beta\\ne\\theta\\)\n\\(2\\times P(\\hat\\beta &gt;|T|)=p\\)"
  },
  {
    "objectID": "lectures/14.html#confidence-intervals",
    "href": "lectures/14.html#confidence-intervals",
    "title": "Inference with Linear Regression",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\\[\n\\hat \\beta_j \\pm CV \\times se(\\hat\\beta_j)\n\\]\n\n\\(CV\\): Critical Value \\(P(X&lt;CV) = 1-\\alpha/2\\)\n\\(\\alpha\\): significance level\n\\(se\\): Standard Error Function"
  },
  {
    "objectID": "lectures/14.html#conducting-ht-of-beta_j",
    "href": "lectures/14.html#conducting-ht-of-beta_j",
    "title": "Inference with Linear Regression",
    "section": "Conducting HT of \\(\\beta_j\\)",
    "text": "Conducting HT of \\(\\beta_j\\)\n\n\nCode\nxlm &lt;- lm(Y ~ X, data = DATA)\nsummary(xlm)"
  },
  {
    "objectID": "lectures/14.html#example",
    "href": "lectures/14.html#example",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- lm(body_mass_g ~ species + flipper_length_mm, penguins)\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species + flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -898.8 -252.0  -24.8  229.8 1191.6 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -4013.18     586.25  -6.846 3.74e-11 ***\n#&gt; speciesChinstrap   -205.38      57.57  -3.568 0.000414 ***\n#&gt; speciesGentoo       284.52      95.43   2.981 0.003083 ** \n#&gt; flipper_length_mm    40.61       3.08  13.186  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 373.3 on 329 degrees of freedom\n#&gt; Multiple R-squared:  0.787,  Adjusted R-squared:  0.7851 \n#&gt; F-statistic: 405.3 on 3 and 329 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/14.html#confidence-interval",
    "href": "lectures/14.html#confidence-interval",
    "title": "Inference with Linear Regression",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n\nCode\nconfint(xlm, level = LEVEL)"
  },
  {
    "objectID": "lectures/14.html#example-1",
    "href": "lectures/14.html#example-1",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nconfint(m1, level = 0.90)\n\n\n#&gt;                           5 %        95 %\n#&gt; (Intercept)       -4980.19064 -3046.16714\n#&gt; speciesChinstrap   -300.33123  -110.41973\n#&gt; speciesGentoo       127.11143   441.93578\n#&gt; flipper_length_mm    35.52645    45.68588"
  },
  {
    "objectID": "lectures/14.html#model-inference",
    "href": "lectures/14.html#model-inference",
    "title": "Inference with Linear Regression",
    "section": "Model inference",
    "text": "Model inference\nWe conduct model inference to determine if different models are better at explaining variation. A common example is to compare a linear model (\\(\\hat Y=\\hat\\beta_0 + \\hat\\beta_1 X\\)) to the mean of Y (\\(\\hat \\mu_y\\)). We determine the significance of the variation explained using an Analysis of Variance (ANOVA) table and F test."
  },
  {
    "objectID": "lectures/14.html#model-inference-1",
    "href": "lectures/14.html#model-inference-1",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven 2 models:\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\cdots + \\hat\\beta_p X_p\n\\]\nor\n\\[\n\\hat Y = \\bar y\n\\]\n\nIs the model with predictors do a better job than using the average?"
  },
  {
    "objectID": "lectures/14.html#anova",
    "href": "lectures/14.html#anova",
    "title": "Inference with Linear Regression",
    "section": "ANOVA",
    "text": "ANOVA"
  },
  {
    "objectID": "lectures/14.html#anova-table",
    "href": "lectures/14.html#anova-table",
    "title": "Inference with Linear Regression",
    "section": "ANOVA Table",
    "text": "ANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nModel\n\\(DFR=k-1\\)\n\\(SSR\\)\n\\(MSR=\\frac{SSM}{DFR}\\)\n\\(\\hat F=\\frac{MSR}{MSE}\\)\n\n\nError\n\\(DFE=n-k\\)\n\\(SSE\\)\n\\(MSE=\\frac{SSE}{DFE}\\)\n\n\n\nTotal\n\\(TDF=n-1\\)\n\\(TSS=SSR+SSE\\)\n\n\n\n\n\n\\[\n\\hat F \\sim F(DFR, DFE)\n\\]"
  },
  {
    "objectID": "lectures/14.html#conducting-an-anova-in-r",
    "href": "lectures/14.html#conducting-an-anova-in-r",
    "title": "Inference with Linear Regression",
    "section": "Conducting an ANOVA in R",
    "text": "Conducting an ANOVA in R\n\n\nCode\nxlm &lt;- lm(Y ~ X, data = DATA)\nsummary(xlm)"
  },
  {
    "objectID": "lectures/14.html#example-2",
    "href": "lectures/14.html#example-2",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nsummary(m1)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species + flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -898.8 -252.0  -24.8  229.8 1191.6 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -4013.18     586.25  -6.846 3.74e-11 ***\n#&gt; speciesChinstrap   -205.38      57.57  -3.568 0.000414 ***\n#&gt; speciesGentoo       284.52      95.43   2.981 0.003083 ** \n#&gt; flipper_length_mm    40.61       3.08  13.186  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 373.3 on 329 degrees of freedom\n#&gt; Multiple R-squared:  0.787,  Adjusted R-squared:  0.7851 \n#&gt; F-statistic: 405.3 on 3 and 329 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/14.html#model-inference-2",
    "href": "lectures/14.html#model-inference-2",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nModel inference can be extended to compare models that have different number of predictors."
  },
  {
    "objectID": "lectures/14.html#model-inference-3",
    "href": "lectures/14.html#model-inference-3",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nGiven:\n\\[\nM1:\\ \\hat y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\n\\[\nM2:\\ \\hat y = \\beta_0 + \\beta_1 X_1  \n\\]\nLet \\(M1\\) be the FULL (larger) model, and let \\(M2\\) be the RED (Reduced, smaller) model."
  },
  {
    "objectID": "lectures/14.html#model-inference-4",
    "href": "lectures/14.html#model-inference-4",
    "title": "Inference with Linear Regression",
    "section": "Model Inference",
    "text": "Model Inference\nHe can test the following Hypothesis:\n\n\\(H_0\\): The error variations between the FULL and RED model are not different.\n\\(H_1\\): The error variations between the FULL and RED model are different."
  },
  {
    "objectID": "lectures/14.html#test-statistic",
    "href": "lectures/14.html#test-statistic",
    "title": "Inference with Linear Regression",
    "section": "Test Statistic",
    "text": "Test Statistic\n\\[\n\\hat F = \\frac{[SSE(RED) - SSE(FULL)]/[DFE(RED)-DFE(FULL)]}{MSE(FULL)}\n\\]\n\\[\n\\hat F \\sim F[DFE(RED) - DFE(FULL), DFE(FULL)]\n\\]"
  },
  {
    "objectID": "lectures/14.html#anova-in-r",
    "href": "lectures/14.html#anova-in-r",
    "title": "Inference with Linear Regression",
    "section": "ANOVA in R",
    "text": "ANOVA in R\n\n\nCode\nfull &lt;- lm(Y  ~  X1 + X2 + X3 + X4)\nred &lt;- lm(Y ~ X1 + X2)\nanova(red, full)"
  },
  {
    "objectID": "lectures/14.html#example-3",
    "href": "lectures/14.html#example-3",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nm1 &lt;- lm(body_mass_g ~ species + island + flipper_length_mm, penguins)\nm2 &lt;- lm(body_mass_g ~ island + flipper_length_mm, penguins)\nanova(m2, m1)\n\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: body_mass_g ~ island + flipper_length_mm\n#&gt; Model 2: body_mass_g ~ species + island + flipper_length_mm\n#&gt;   Res.Df      RSS Df Sum of Sq      F    Pr(&gt;F)    \n#&gt; 1    329 47774435                                  \n#&gt; 2    327 45552857  2   2221579 7.9738 0.0004157 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/14.html#model-assumptions-1",
    "href": "lectures/14.html#model-assumptions-1",
    "title": "Inference with Linear Regression",
    "section": "Model Assumptions",
    "text": "Model Assumptions\nWhen we are conducting inference linear regression, we will have to check the following conditions:\n\nLinearity\nIndependence\nNormality\nEqual Variances\nMulticollinearity (for MLR)"
  },
  {
    "objectID": "lectures/14.html#linearity",
    "href": "lectures/14.html#linearity",
    "title": "Inference with Linear Regression",
    "section": "Linearity",
    "text": "Linearity\nProbably considered the most important assumption, but there must be a linear relationship between both the outcome variable (y) and a set of predictors (\\(x_1\\), \\(x_2\\), …)."
  },
  {
    "objectID": "lectures/14.html#independence",
    "href": "lectures/14.html#independence",
    "title": "Inference with Linear Regression",
    "section": "Independence",
    "text": "Independence\nThe data points must not influence each other."
  },
  {
    "objectID": "lectures/14.html#normality",
    "href": "lectures/14.html#normality",
    "title": "Inference with Linear Regression",
    "section": "Normality",
    "text": "Normality\nThe model errors (also known as residuals) must follow a normal distribution."
  },
  {
    "objectID": "lectures/14.html#equal-variances",
    "href": "lectures/14.html#equal-variances",
    "title": "Inference with Linear Regression",
    "section": "Equal Variances",
    "text": "Equal Variances\nThe variability of the data points must be the same for all predictor values."
  },
  {
    "objectID": "lectures/14.html#residuals",
    "href": "lectures/14.html#residuals",
    "title": "Inference with Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nResiduals are the errors between the observed value and the estimated model. Common residuals include\n\nRaw Residual\nStandardized Residual\nJackknife (studentized) Residuals"
  },
  {
    "objectID": "lectures/14.html#influential-measurements",
    "href": "lectures/14.html#influential-measurements",
    "title": "Inference with Linear Regression",
    "section": "Influential Measurements",
    "text": "Influential Measurements\nInfluential measures are statistics that determine how much a data point affects the model. Common influential measures are\n\nLeverages\nCook’s Distance"
  },
  {
    "objectID": "lectures/14.html#raw-residuals",
    "href": "lectures/14.html#raw-residuals",
    "title": "Inference with Linear Regression",
    "section": "Raw Residuals",
    "text": "Raw Residuals\n\\[\n\\hat r_i = y_i - \\hat y_i\n\\]"
  },
  {
    "objectID": "lectures/14.html#residual-analysis",
    "href": "lectures/14.html#residual-analysis",
    "title": "Inference with Linear Regression",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nA residual analysis is used to test the assumptions of linear regression."
  },
  {
    "objectID": "lectures/14.html#qq-plot",
    "href": "lectures/14.html#qq-plot",
    "title": "Inference with Linear Regression",
    "section": "QQ Plot",
    "text": "QQ Plot\nA qq (quantile-quantile) plot will plot the estimated quantiles of the residuals against the theoretical quantiles from a normal distribution function. If the points from the qq-plot lie on the \\(y=x\\) line, it is said that the residuals follow a normal distribution."
  },
  {
    "objectID": "lectures/14.html#residual-vs-fitted-plot",
    "href": "lectures/14.html#residual-vs-fitted-plot",
    "title": "Inference with Linear Regression",
    "section": "Residual vs Fitted Plot",
    "text": "Residual vs Fitted Plot\nThis plot allows you to assess the linearity, constant variance, and identify potential outliers. Create a scatter plot between the fitted values (x-axis) and the raw/standardized residuals (y-axis)."
  },
  {
    "objectID": "lectures/14.html#variance-inflation-factor",
    "href": "lectures/14.html#variance-inflation-factor",
    "title": "Inference with Linear Regression",
    "section": "Variance Inflation Factor",
    "text": "Variance Inflation Factor\nThe variance inflation factor is a measurement on how much variables are collinear with each other. A value greater than 10 is a cause for concern and action should be taken."
  },
  {
    "objectID": "lectures/14.html#residual-analysis-in-r",
    "href": "lectures/14.html#residual-analysis-in-r",
    "title": "Inference with Linear Regression",
    "section": "Residual Analysis in R",
    "text": "Residual Analysis in R\nUse the resid_df function to obtain the residuals of a model.\n\n\nCode\nrdf &lt;- resid_df(LM_OBJECT)"
  },
  {
    "objectID": "lectures/14.html#residual-vs-fitted-plot-1",
    "href": "lectures/14.html#residual-vs-fitted-plot-1",
    "title": "Inference with Linear Regression",
    "section": "Residual vs Fitted Plot",
    "text": "Residual vs Fitted Plot\n\n\nCode\nggplot(RDF, aes(fitted, resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")"
  },
  {
    "objectID": "lectures/14.html#qq-plot-1",
    "href": "lectures/14.html#qq-plot-1",
    "title": "Inference with Linear Regression",
    "section": "QQ Plot",
    "text": "QQ Plot\n\n\nCode\nggplot(RDF, aes(sample = resid)) + \n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "lectures/14.html#example-4",
    "href": "lectures/14.html#example-4",
    "title": "Inference with Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nCode\nxlm &lt;- lm(body_mass_g ~   island + species + flipper_length_mm,\n          penguins)\ndfxlm &lt;- resid_df(xlm)\n\nggplot(dfxlm, aes(fitted, resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, col = \"red\")\n\nggplot(dfxlm, aes(sample = resid)) + \n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "lectures/15b.html#announcements",
    "href": "lectures/15b.html#announcements",
    "title": "Installing R and Rstudio",
    "section": "Announcements",
    "text": "Announcements\n\nFill Out Course Evaluations\nThird Exam Next Week\n\n12 PM Class (Sec 01): Monday Math 12 10:30 AM to 12:30 PM\n1:30 PM Class (Sec 02): Wednesday May 14 1-3 PM"
  },
  {
    "objectID": "lectures/15b.html#traditional-statistics",
    "href": "lectures/15b.html#traditional-statistics",
    "title": "Installing R and Rstudio",
    "section": "Traditional Statistics",
    "text": "Traditional Statistics\nTraditional Statistics methods can conducted using either linear or logistic regression.\nVisit Here for more information."
  },
  {
    "objectID": "lectures/15b.html#what-is-statistics",
    "href": "lectures/15b.html#what-is-statistics",
    "title": "Installing R and Rstudio",
    "section": "What is Statistics?",
    "text": "What is Statistics?\n\nIt is the study of variation and randomness!"
  },
  {
    "objectID": "lectures/15b.html#whats-the-goal-of-statistics",
    "href": "lectures/15b.html#whats-the-goal-of-statistics",
    "title": "Installing R and Rstudio",
    "section": "What’s the goal of Statistics?",
    "text": "What’s the goal of Statistics?\n\nINFERENCE\n\n\nUse our sample data to understand the larger population.\n\n\nThe data will tell us how the population generally behaves.\n\n\nThe data will guide us in the differences in units.\n\n\nData will tell us if there is a signal or just noise."
  },
  {
    "objectID": "lectures/15b.html#final-thoughts",
    "href": "lectures/15b.html#final-thoughts",
    "title": "Installing R and Rstudio",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nWhen conducting a study, literature review and study design are as equally important as statistics.\n\n\nIf you don’t see variability in the data, something is wrong.\n\n\nFocus on consistency in the methodology, not consistency in data.\n\n\nUnderstand that you can be wrong, and that is okay.\n\n\nDon’t let data influence the methodology during a study/experiment."
  },
  {
    "objectID": "lectures/15b.html#statistics-mantra",
    "href": "lectures/15b.html#statistics-mantra",
    "title": "Installing R and Rstudio",
    "section": "Statistics Mantra",
    "text": "Statistics Mantra\n\n\n\n\n\n\n\n\nAll models are wrong,\nsome are useful!"
  },
  {
    "objectID": "lectures/15b.html#final-survey-1",
    "href": "lectures/15b.html#final-survey-1",
    "title": "Installing R and Rstudio",
    "section": "Final Survey",
    "text": "Final Survey\nSurvery"
  },
  {
    "objectID": "lectures/15b.html#installing-r",
    "href": "lectures/15b.html#installing-r",
    "title": "Installing R and Rstudio",
    "section": "Installing R",
    "text": "Installing R\nhttps://cloud.r-project.org/"
  },
  {
    "objectID": "lectures/15b.html#installing-rstudio",
    "href": "lectures/15b.html#installing-rstudio",
    "title": "Installing R and Rstudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nhttps://posit.co/download/rstudio-desktop/"
  },
  {
    "objectID": "lectures/15b.html#installing-rtools-windows",
    "href": "lectures/15b.html#installing-rtools-windows",
    "title": "Installing R and Rstudio",
    "section": "Installing RTools (Windows)",
    "text": "Installing RTools (Windows)\nhttps://cran.r-project.org/bin/windows/Rtools/"
  },
  {
    "objectID": "lectures/15b.html#tidyverse",
    "href": "lectures/15b.html#tidyverse",
    "title": "Installing R and Rstudio",
    "section": "Tidyverse",
    "text": "Tidyverse\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "lectures/15b.html#csucistats",
    "href": "lectures/15b.html#csucistats",
    "title": "Installing R and Rstudio",
    "section": "csucistats",
    "text": "csucistats\n\ninstall.packages('csucistats', \n                 repos = c('https://inqs909.r-universe.dev', \n                           'https://cloud.r-project.org'))"
  },
  {
    "objectID": "lectures/15b.html#other-packages",
    "href": "lectures/15b.html#other-packages",
    "title": "Installing R and Rstudio",
    "section": "Other Packages",
    "text": "Other Packages\n\ninstall.packages(c(\"ggthemes\", \"statmod\", \"car\", \"ggpubr\", \"lmtest\", \"rms\", \"palmerpenguins\"))\ncsucistats::install_plots()\ncsucistats::install_themes()"
  },
  {
    "objectID": "lectures/15b.html#load-packages",
    "href": "lectures/15b.html#load-packages",
    "title": "Installing R and Rstudio",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(tidyverse)\nlibrary(csucistats)\nlibrary(ggthemes)\nlibrary(waffle)\nlibrary(ggmosaic)\nlibrary(ggtricks)\nlibrary(ggtext)\nlibrary(ggpubr)\nlibrary(ThemePark)\nlibrary(car)\nlibrary(lmtest)\nlibrary(rms)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "lectures/15b.html#rstudio-layout-1",
    "href": "lectures/15b.html#rstudio-layout-1",
    "title": "Installing R and Rstudio",
    "section": "RStudio Layout",
    "text": "RStudio Layout\n\nScripts\nConsole\nEnvironment\nFiles\nEverything else"
  },
  {
    "objectID": "lectures/15b.html#downloading-data-data",
    "href": "lectures/15b.html#downloading-data-data",
    "title": "Installing R and Rstudio",
    "section": "Downloading Data Data",
    "text": "Downloading Data Data\n\nu1 &lt;- \"https://www.inqs.info/p/plotathon/owenWilsonWows.csv\"\nu2 &lt;- \"https://www.inqs.info/p/plotathon/owenWilsonWows.xlsx\"\n\ndownload.file(u1,\n              file.path(getwd(), basename(u1)))\ndownload.file(u2,\n              file.path(getwd(), basename(u2)))"
  },
  {
    "objectID": "lectures/3.html#r-packages",
    "href": "lectures/3.html#r-packages",
    "title": "Categorical Data",
    "section": "R Packages",
    "text": "R Packages\n\ncsucistats\ntidyverse"
  },
  {
    "objectID": "lectures/3.html#background",
    "href": "lectures/3.html#background",
    "title": "Categorical Data",
    "section": "Background",
    "text": "Background\nIn October 2023, James Hoffman and Cometeer held the “Great American Coffee Taste Test” on YouTube, asking viewers to fill out a survey and coffee ordered from Cometeer."
  },
  {
    "objectID": "lectures/3.html#data",
    "href": "lectures/3.html#data",
    "title": "Categorical Data",
    "section": "Data",
    "text": "Data\nThe data is part of DSLC Tidy Tuesday program where data sets are provided to help data science learners how to create graphics.\nInformation on the data sets variables (columns) can be found here."
  },
  {
    "objectID": "lectures/3.html#data-1",
    "href": "lectures/3.html#data-1",
    "title": "Categorical Data",
    "section": "Data",
    "text": "Data\n\n\nCode\nlibrary(csucistats)\nlibrary(ggtricks)\nlibrary(tidyverse)\nlibrary(ThemePark)\nlibrary(DT)\ncoffee &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-05-14/coffee_survey.csv\")\ndatatable(slice_sample(coffee, n = 10), options = list(dom = \"p\", pageLength = 5))"
  },
  {
    "objectID": "lectures/3.html#categorical-data-1",
    "href": "lectures/3.html#categorical-data-1",
    "title": "Categorical Data",
    "section": "Categorical Data",
    "text": "Categorical Data\nCategorical data are data recordings that represented a category.\n\nData may be recorded as a “character” or “string” data.\n\n\nData may be recorded as a whole number, with an attached code book indicating the categories each number belongs to."
  },
  {
    "objectID": "lectures/3.html#examples-of-categorical-data",
    "href": "lectures/3.html#examples-of-categorical-data",
    "title": "Categorical Data",
    "section": "Examples of Categorical Data",
    "text": "Examples of Categorical Data\n\nAre you a student?\nWhat city do you live in?\nWhat is your major?"
  },
  {
    "objectID": "lectures/3.html#likert-scale",
    "href": "lectures/3.html#likert-scale",
    "title": "Categorical Data",
    "section": "Likert Scale",
    "text": "Likert Scale\nLikert scales are the rating systems you may have answered in surveys.\n\n\nStrongly Disagree\nDisagree\nNeutral\nAgree\nStrongly Agree"
  },
  {
    "objectID": "lectures/3.html#likert-scales",
    "href": "lectures/3.html#likert-scales",
    "title": "Categorical Data",
    "section": "Likert Scales",
    "text": "Likert Scales\nLikert scales may be treated as numerical data if the jumps between scales are equal."
  },
  {
    "objectID": "lectures/3.html#summarizing-categorical-data",
    "href": "lectures/3.html#summarizing-categorical-data",
    "title": "Categorical Data",
    "section": "Summarizing Categorical Data",
    "text": "Summarizing Categorical Data\nOnce we have the data, how do we summarize it to other people."
  },
  {
    "objectID": "lectures/3.html#continguency-tables-1",
    "href": "lectures/3.html#continguency-tables-1",
    "title": "Categorical Data",
    "section": "Continguency Tables",
    "text": "Continguency Tables\nContinguency tables display how often a category is seen in the data.\n\nThere are two types of statistics that are reported in a table, the frequency and proportion."
  },
  {
    "objectID": "lectures/3.html#frequencey",
    "href": "lectures/3.html#frequencey",
    "title": "Categorical Data",
    "section": "Frequencey",
    "text": "Frequencey\nFrequency represents the count of observing a specific category in your sample.\n\n\n#&gt;  [1] \"1\"           \"1\"           \"Less than 1\" \"2\"           \"More than 4\"\n#&gt;  [6] \"3\"           \"1\"           \"3\"           \"2\"           \"1\""
  },
  {
    "objectID": "lectures/3.html#proportions-relative-frequencey",
    "href": "lectures/3.html#proportions-relative-frequencey",
    "title": "Categorical Data",
    "section": "Proportions (relative frequencey)",
    "text": "Proportions (relative frequencey)\nProportions represent the percentage that the category represents the sample.\n\nThis allows you to generalize your sample to the population, regardless of sample size."
  },
  {
    "objectID": "lectures/3.html#continguency-tables-in-r",
    "href": "lectures/3.html#continguency-tables-in-r",
    "title": "Categorical Data",
    "section": "Continguency Tables in R",
    "text": "Continguency Tables in R\nThe variable caffeine indicates how much caffeine a participant prefers.\n\ncat_stats(coffee$caffeine)"
  },
  {
    "objectID": "lectures/3.html#plotting-in-r",
    "href": "lectures/3.html#plotting-in-r",
    "title": "Categorical Data",
    "section": "Plotting in R",
    "text": "Plotting in R\nPlotting in R can be done via the ggplot2, a powerful library based on the Grammar of Graphics."
  },
  {
    "objectID": "lectures/3.html#plotting-in-r-1",
    "href": "lectures/3.html#plotting-in-r-1",
    "title": "Categorical Data",
    "section": "Plotting in R",
    "text": "Plotting in R\n\nYou need to create a base plot using the ggplot()\nUse the + to change the look of the base plot\nIndicate how to transform the base plot to the desired plot\ngeom_*\nstat_*\nChange the look of the plot with other functions\nUse a theme_* function to add a theme to the plot"
  },
  {
    "objectID": "lectures/3.html#bar-plots-1",
    "href": "lectures/3.html#bar-plots-1",
    "title": "Categorical Data",
    "section": "Bar Plots",
    "text": "Bar Plots\nBar Plots can be used to display the frequency or proportions on the data."
  },
  {
    "objectID": "lectures/3.html#frequency-bar-plots-in-r",
    "href": "lectures/3.html#frequency-bar-plots-in-r",
    "title": "Categorical Data",
    "section": "Frequency Bar Plots in R",
    "text": "Frequency Bar Plots in R\n\nggplot(data = DATA, aes(x = VARIABLE)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#frequency-bar-plots-in-r-1",
    "href": "lectures/3.html#frequency-bar-plots-in-r-1",
    "title": "Categorical Data",
    "section": "Frequency Bar Plots in R",
    "text": "Frequency Bar Plots in R\n\n\nCode\nggplot(coffee, aes(caffeine)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#relative-frequency-bar-plots-in-r",
    "href": "lectures/3.html#relative-frequency-bar-plots-in-r",
    "title": "Categorical Data",
    "section": "Relative Frequency Bar Plots in R",
    "text": "Relative Frequency Bar Plots in R\n\nggplot(data = DATA, aes(x = VARIABLE, y = after_stat(prop), group = 1)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#relative-frequency-bar-plots-in-r-1",
    "href": "lectures/3.html#relative-frequency-bar-plots-in-r-1",
    "title": "Categorical Data",
    "section": "Relative Frequency Bar Plots in R",
    "text": "Relative Frequency Bar Plots in R\n\n\nCode\nggplot(coffee, aes(caffeine, after_stat(prop), group = 1)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#data-2",
    "href": "lectures/3.html#data-2",
    "title": "Categorical Data",
    "section": "Data",
    "text": "Data\nThe variable taste indicates if the participants like the taste of coffee.\n\ncat_stats(coffee$taste)"
  },
  {
    "objectID": "lectures/3.html#cross-tabulation-1",
    "href": "lectures/3.html#cross-tabulation-1",
    "title": "Categorical Data",
    "section": "Cross-Tabulation",
    "text": "Cross-Tabulation\nCross-tabulations, also known as contingency tables, are statistical tools used to analyze the relationship between two or more categorical variables by displaying their frequency distribution in a table format. Each cell in the table represents the count or frequency of observations that fall into a particular combination of categories for the variables."
  },
  {
    "objectID": "lectures/3.html#key-features-of-cross-tabulations",
    "href": "lectures/3.html#key-features-of-cross-tabulations",
    "title": "Categorical Data",
    "section": "Key Features of Cross-Tabulations",
    "text": "Key Features of Cross-Tabulations\n\nRows and Columns:\n\nRows represent the categories of one variable.\nColumns represent the categories of another variable.\n\nCells:\n\nEach cell displays the frequency or count of data points that belong to the intersection of a row and column category.\n\nMargins:\n\nRow and column totals provide summaries for each variable.\nThe grand total shows the overall sample size."
  },
  {
    "objectID": "lectures/3.html#types-of-proportions-in-cross-tabulations",
    "href": "lectures/3.html#types-of-proportions-in-cross-tabulations",
    "title": "Categorical Data",
    "section": "Types of Proportions in Cross-Tabulations",
    "text": "Types of Proportions in Cross-Tabulations\n\nRow Proportions: Show the percentage of each row total represented by a cell.\nColumn Proportions: Show the percentage of each column total represented by a cell.\nTable Proportions: Show the percentage of the overall total represented by a cell."
  },
  {
    "objectID": "lectures/3.html#table-proportions",
    "href": "lectures/3.html#table-proportions",
    "title": "Categorical Data",
    "section": "Table Proportions",
    "text": "Table Proportions\nTable proportions in cross-tabulations refer to the relative frequency or percentage of counts within the entire table, calculated by dividing each cell’s count by the total sum of all counts in the table. These proportions allow you to examine the contribution of each cell to the overall data set."
  },
  {
    "objectID": "lectures/3.html#table-proportions-1",
    "href": "lectures/3.html#table-proportions-1",
    "title": "Categorical Data",
    "section": "Table Proportions",
    "text": "Table Proportions\n\ncat_stats(coffee$caffeine, coffee$taste, prop = \"table\")"
  },
  {
    "objectID": "lectures/3.html#row-proportions",
    "href": "lectures/3.html#row-proportions",
    "title": "Categorical Data",
    "section": "Row Proportions",
    "text": "Row Proportions\nRow proportions refer to the relative frequency or percentage of counts within each row of a contingency table. In a cross-tabulation, row proportions allow you to compare how the distribution of one variable varies within each category of another variable, within a row."
  },
  {
    "objectID": "lectures/3.html#row-proportions-1",
    "href": "lectures/3.html#row-proportions-1",
    "title": "Categorical Data",
    "section": "Row Proportions",
    "text": "Row Proportions\n\ncat_stats(coffee$caffeine, coffee$taste, prop = \"row\")"
  },
  {
    "objectID": "lectures/3.html#column-proportions",
    "href": "lectures/3.html#column-proportions",
    "title": "Categorical Data",
    "section": "Column Proportions",
    "text": "Column Proportions\nColumn proportions refer to the relative frequency or percentage of counts within each column of a contingency table. These proportions allow you to compare how the distribution of one variable varies across different categories of another variable, within a column."
  },
  {
    "objectID": "lectures/3.html#column-proportions-1",
    "href": "lectures/3.html#column-proportions-1",
    "title": "Categorical Data",
    "section": "Column Proportions",
    "text": "Column Proportions\n\ncat_stats(coffee$caffeine, coffee$taste, prop = \"col\")"
  },
  {
    "objectID": "lectures/3.html#stacked-bar-plot-in-r",
    "href": "lectures/3.html#stacked-bar-plot-in-r",
    "title": "Categorical Data",
    "section": "Stacked Bar Plot in R",
    "text": "Stacked Bar Plot in R\n\nggplot(DATA, aes(x = VAR1, y = after_stat(count), fill = VAR2)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#stacked-bar-plot-in-r-1",
    "href": "lectures/3.html#stacked-bar-plot-in-r-1",
    "title": "Categorical Data",
    "section": "Stacked Bar Plot in R",
    "text": "Stacked Bar Plot in R\n\nggplot(coffee, aes(x = caffeine, y = after_stat(count), fill = taste)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#stacked-bar-plot-in-r-2",
    "href": "lectures/3.html#stacked-bar-plot-in-r-2",
    "title": "Categorical Data",
    "section": "Stacked Bar Plot in R",
    "text": "Stacked Bar Plot in R\n\nggplot(coffee, aes(y = caffeine, x = after_stat(count), fill = taste)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/3.html#pie-charts",
    "href": "lectures/3.html#pie-charts",
    "title": "Categorical Data",
    "section": "Pie Charts",
    "text": "Pie Charts\nA pie chart is a circular statistical graphic divided into slices, where each slice represents a proportion or percentage of the whole. The size of each slice is proportional to the relative frequency or magnitude of the category it represents."
  },
  {
    "objectID": "lectures/3.html#pie-charts-1",
    "href": "lectures/3.html#pie-charts-1",
    "title": "Categorical Data",
    "section": "Pie Charts",
    "text": "Pie Charts"
  },
  {
    "objectID": "lectures/3.html#key-features-of-pie-charts",
    "href": "lectures/3.html#key-features-of-pie-charts",
    "title": "Categorical Data",
    "section": "Key Features of Pie Charts",
    "text": "Key Features of Pie Charts\n\nCircular Format:\n\nThe chart is shaped like a circle, symbolizing a whole (100% or 1).\n\nSlices:\n\nEach slice corresponds to a category and its size represents the contribution of that category to the total.\n\nLabels:\n\nSlices are often labeled with the category name and the percentage or value they represent."
  },
  {
    "objectID": "lectures/3.html#pie-chart-in-r",
    "href": "lectures/3.html#pie-chart-in-r",
    "title": "Categorical Data",
    "section": "Pie Chart in R",
    "text": "Pie Chart in R\n\ndf_pie &lt;- cat_stats(coffee$caffeine, tbl_df = TRUE)$table\nggplot(df_pie, aes(cat = Category, val = n, fill = Category)) +\n  geom_pie()"
  },
  {
    "objectID": "lectures/3.html#pie-chart-in-r-1",
    "href": "lectures/3.html#pie-chart-in-r-1",
    "title": "Categorical Data",
    "section": "Pie Chart in R",
    "text": "Pie Chart in R\n\ncoffee_pie &lt;- cat_stats(coffee$caffeine, tbl_df = TRUE)$table\nggplot(coffee_pie, aes(cat = Category, val = n, fill = Category)) +\n  geom_pie()"
  },
  {
    "objectID": "lectures/3.html#mosaic-plots",
    "href": "lectures/3.html#mosaic-plots",
    "title": "Categorical Data",
    "section": "Mosaic Plots",
    "text": "Mosaic Plots\nA mosaic plot is a graphical representation of two categorical variables. It uses rectangles to visualize the proportions of data categories while simultaneously showing the relationships between multiple variables. The size of each rectangle corresponds to the relative frequency or proportion of the data in a particular category combination."
  },
  {
    "objectID": "lectures/3.html#key-features-of-mosaic-plots",
    "href": "lectures/3.html#key-features-of-mosaic-plots",
    "title": "Categorical Data",
    "section": "Key Features of Mosaic Plots",
    "text": "Key Features of Mosaic Plots\n\nRectangular Tiles:\n\nThe plot is divided into rectangles, with each tile representing a unique combination of categories.\n\nProportional Areas:\n\nThe area of each rectangle is proportional to the frequency or proportion of the data it represents.\n\nHierarchical Arrangement:\n\nVariables are arranged hierarchically along the axes, with the tiles subdivided to represent relationships between variables.\n\nColor Coding (Optional):\n\nDifferent colors can be used to highlight specific patterns, emphasize groups, or indicate significance."
  },
  {
    "objectID": "lectures/3.html#themes",
    "href": "lectures/3.html#themes",
    "title": "Categorical Data",
    "section": "Themes",
    "text": "Themes\nThe R packages ThemePark and ggthemes allows you to change the overall look of a plot.\n\nAll you need to do is add the theme to the plot."
  },
  {
    "objectID": "lectures/5.html#what-is-probability",
    "href": "lectures/5.html#what-is-probability",
    "title": "Distribution Functions",
    "section": "What is Probability?",
    "text": "What is Probability?\nProbability is the measure of how likely an event is to occur. It ranges from 0 to 1:\n\n\\(P(A) = 0\\): The event \\(A\\) will definitely not happen.\n\\(P(A) = 1\\): The event \\(A\\) will definitely happen.\nValues between 0 and 1 represent varying degrees of likelihood."
  },
  {
    "objectID": "lectures/5.html#everyday-examples",
    "href": "lectures/5.html#everyday-examples",
    "title": "Distribution Functions",
    "section": "Everyday Examples",
    "text": "Everyday Examples\n\nWhat’s the probability it will rain tomorrow?\nWhat are the chances of rolling a 6 on a standard die?\nHow likely is it that a randomly chosen student has a GPA above 3.0?"
  },
  {
    "objectID": "lectures/5.html#key-terms-and-definitions",
    "href": "lectures/5.html#key-terms-and-definitions",
    "title": "Distribution Functions",
    "section": "Key Terms and Definitions",
    "text": "Key Terms and Definitions\n\nExperimentSample SpaceEventProb. of an Event\n\n\nAn action or process that generates outcomes.\n- Example: Rolling a die.\n\n\nThe set of all possible outcomes of an experiment.\n- Example: For rolling a die, \\(S = \\{1, 2, 3, 4, 5, 6\\}\\).\n\n\nA subset of the sample space, representing outcomes of interest.\n\nExample: Rolling an even number (\\(A = \\{2, 4, 6\\}\\)).\n\n\n\nThe proportion of times an event is expected to occur if the experiment is repeated many times."
  },
  {
    "objectID": "lectures/5.html#the-probability-formula",
    "href": "lectures/5.html#the-probability-formula",
    "title": "Distribution Functions",
    "section": "The Probability Formula",
    "text": "The Probability Formula\nThe probability of an event \\(A\\) is defined as:\n\\[\nP(A) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}}\n\\]"
  },
  {
    "objectID": "lectures/5.html#example",
    "href": "lectures/5.html#example",
    "title": "Distribution Functions",
    "section": "Example",
    "text": "Example\nIf we roll a die, what is the probability of rolling a 4?\n\nFavorable outcomes: 1 (rolling a 4).\nTotal outcomes: 6 (since \\(S = \\{1, 2, 3, 4, 5, 6\\}\\)).\n\n\\[\nP(\\text{rolling a 4}) = \\frac{1}{6} \\approx 0.167\n\\]"
  },
  {
    "objectID": "lectures/5.html#rules-of-probability",
    "href": "lectures/5.html#rules-of-probability",
    "title": "Distribution Functions",
    "section": "Rules of Probability",
    "text": "Rules of Probability\n\nRule 1Rule 2Rule 3\n\n\nProbability of the Sample Space\nThe probability of the sample space is always 1: \\[\nP(S) = 1\n\\]\n\n\nProbability of Impossible Events\nThe probability of an event that cannot happen is 0:\n\\[\nP(\\emptyset) = 0\n\\]\n\n\nComplement Rule\nThe probability of the complement of an event \\(A\\) (not \\(A\\)) is:\n\\[\nP(A^c) = 1 - P(A)\n\\]\n\nExample: If \\(P(\\text{rain}) = 0.3\\), then \\(P(\\text{no rain}) = 1 - 0.3 = 0.7\\)."
  },
  {
    "objectID": "lectures/5.html#applications",
    "href": "lectures/5.html#applications",
    "title": "Distribution Functions",
    "section": "Applications",
    "text": "Applications\n\nDrawing a CardTossing a Coin TwiceTraffic Lights\n\n\nIf you draw a card from a standard deck of 52 cards, what is the probability of drawing:\n\nA heart?\n\\[\nP(\\text{heart}) = \\frac{13}{52} = 0.25\n  \\]\nA red card (heart or diamond)?\n\\[\nP(\\text{red card}) = \\frac{26}{52} = 0.5\n  \\]\n\n\n\nWhat is the probability of getting:\n\nExactly one head?\nSample space: \\(S = \\{\\text{HH, HT, TH, TT}\\}\\).\nEvent: \\(A = \\{\\text{HT, TH}\\}\\).\n\\[\nP(A) = \\frac{2}{4} = 0.5\n\\]\nAt least one head?\nEvent: \\(B = \\{\\text{HH, HT, TH}\\}\\).\n\\[\nP(B) = \\frac{3}{4} = 0.75\n\\]\n\n\n\nA commuter encounters three traffic lights, each with a 70% chance of being green. Assuming independence, what is the probability that all three lights are green?\n\\[\nP(\\text{all green}) = 0.7 \\cdot 0.7 \\cdot 0.7 = 0.343\n\\]"
  },
  {
    "objectID": "lectures/5.html#more-problems",
    "href": "lectures/5.html#more-problems",
    "title": "Distribution Functions",
    "section": "More Problems",
    "text": "More Problems\n\nRolling a Die1234\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\n\n\n\n\n\n\\[\nP(X = 2)\n\\]\n\n\n\\[\nP(1 \\le X \\le 3)\n\\]\n\n\n\\[\nP(1 &lt; X &lt; 3)\n\\]\n\n\n\\[\nP(X &gt; 1)\n\\]"
  },
  {
    "objectID": "lectures/5.html#joint-probability-1",
    "href": "lectures/5.html#joint-probability-1",
    "title": "Distribution Functions",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nOften, we’re interested in the probabilities of multiple events occurring.\nThis presentation focuses on calculating probabilities involving two events.\nWe’ll explore key concepts like:\n\nJoint Probability\nUnion of Events\nConditional Probability\nIndependence"
  },
  {
    "objectID": "lectures/5.html#joint-probability-2",
    "href": "lectures/5.html#joint-probability-2",
    "title": "Distribution Functions",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nThe probability of both events A and B occurring.\nDenoted as \\(P(A\\ \\mathrm{and}\\ B)\\) or \\(P(A \\cap B)\\).\nExamples:\n\nDrawing a King and a Heart from a deck of cards.\nFlipping two coins and getting heads on both."
  },
  {
    "objectID": "lectures/5.html#union-of-events",
    "href": "lectures/5.html#union-of-events",
    "title": "Distribution Functions",
    "section": "Union of Events",
    "text": "Union of Events\n\nThe probability of either event A or event B (or both) occurring.\nDenoted as \\(P(A\\ \\mathrm{or}\\ B)\\) or \\(P(A \\cup B)\\).\nExamples:\n\nRolling a 3 or a 5 on a die.\nDrawing a red card or a face card."
  },
  {
    "objectID": "lectures/5.html#calculating-union-probability",
    "href": "lectures/5.html#calculating-union-probability",
    "title": "Distribution Functions",
    "section": "Calculating Union Probability",
    "text": "Calculating Union Probability\n\nGeneral Case: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) (Inclusion-Exclusion Principle)\nMutually Exclusive Events: If A and B are mutually exclusive (cannot both occur), then \\(P(A \\cap B) = 0\\), and \\(P(A \\cup B) = P(A) + P(B)\\)"
  },
  {
    "objectID": "lectures/5.html#example-union-probability",
    "href": "lectures/5.html#example-union-probability",
    "title": "Distribution Functions",
    "section": "Example: Union Probability",
    "text": "Example: Union Probability\n\nWhat is the probability of rolling a number greater than 4 or an even number on a six-sided die?\n\nEvent A: Rolling a number greater than 4 (5 or 6).\nEvent B: Rolling an even number (2, 4, or 6).\n\n\n\n\n\\(P(A) = 2/6\\)\n\\(P(B) = 3/6\\)\n\\(P(A \\cap B) = 1/6\\) (rolling a 6)\n\\(P(A \\cup B) = (2/6) + (3/6) - (1/6) = 4/6 = 2/3\\)"
  },
  {
    "objectID": "lectures/5.html#conditional-probability",
    "href": "lectures/5.html#conditional-probability",
    "title": "Distribution Functions",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nThe conditional probability of event A occurring given that event B has occurred: \\(P(A|B)\\) (read as “the probability of A given B”).\nFormula: \\(P(A|B) =\\frac{P(A \\cap B)}{P(B)}\\)\n\n\\(P(A|B)\\): Conditional probability of A given B.\n\\(P(A \\cap B)\\): Probability of both A and B occurring.\n\\(P(B)\\): Probability of B occurring."
  },
  {
    "objectID": "lectures/5.html#independence",
    "href": "lectures/5.html#independence",
    "title": "Distribution Functions",
    "section": "Independence",
    "text": "Independence\n\nTwo events are independent if the occurrence of one does not affect the probability of the other.\nIf A and B are independent:\n\n\\(P(A|B) = P(A)\\)\n\\(P(B|A) = P(B)\\)\n\\(P(A \\cap B) = P(A) P(B)\\)"
  },
  {
    "objectID": "lectures/5.html#bayes-theorem",
    "href": "lectures/5.html#bayes-theorem",
    "title": "Distribution Functions",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\nReverses the conditioning: Relates P(A|B) to P(B|A).\n\\(P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}\\)\nUseful when we know P(B|A) but want P(A|B)."
  },
  {
    "objectID": "lectures/5.html#random-variable",
    "href": "lectures/5.html#random-variable",
    "title": "Distribution Functions",
    "section": "Random Variable",
    "text": "Random Variable\nA random variable is a variable whose value is a numerical outcome of a random phenomenon. It’s a way to map the outcomes of a probabilistic event to numbers. This allows us to analyze these outcomes mathematically."
  },
  {
    "objectID": "lectures/5.html#rv-key-concepts",
    "href": "lectures/5.html#rv-key-concepts",
    "title": "Distribution Functions",
    "section": "RV: Key Concepts",
    "text": "RV: Key Concepts\n\nRandom Phenomenon: This is any process or experiment whose outcome is uncertain.\nOutcomes: The possible results of a random phenomenon are called outcomes.\nNumerical Value: A random variable assigns a numerical value to each outcome."
  },
  {
    "objectID": "lectures/5.html#types-of-random-variables",
    "href": "lectures/5.html#types-of-random-variables",
    "title": "Distribution Functions",
    "section": "Types of Random Variables",
    "text": "Types of Random Variables\n\nDiscrete Random VariableContinuous Random Variable\n\n\nA discrete random variable can only take on a finite number of values or a countably infinite number of values. The values are often integers. Examples:\n\nThe number of heads in three coin flips (0, 1, 2, or 3).\nThe number of defective light bulbs in a box of 100.\nThe number of customers entering a store in an hour.\n\n\n\nA continuous random variable can take on any value within a given range. Examples:\n\nThe height of a person.\nThe temperature of a room.\nThe time it takes to complete a task."
  },
  {
    "objectID": "lectures/5.html#distribution-functions-1",
    "href": "lectures/5.html#distribution-functions-1",
    "title": "Distribution Functions",
    "section": "Distribution Functions",
    "text": "Distribution Functions\nA distribution function describes the probabilities of a random variable across its possible values. It answers questions like:\n\nHow likely is a random variable to take on a specific value or fall within a range?\nWhat is the overall “shape” of the distribution of outcomes?"
  },
  {
    "objectID": "lectures/5.html#cumulative-distribution-function",
    "href": "lectures/5.html#cumulative-distribution-function",
    "title": "Distribution Functions",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\nThe Cumulative Distribution Function (CDF) describes the probability that a random variable \\(X\\) is less than or equal to a certain value \\(x\\): \\[\nF_X(x) = P(X \\leq x)\n\\]"
  },
  {
    "objectID": "lectures/5.html#properties-of-the-cdf",
    "href": "lectures/5.html#properties-of-the-cdf",
    "title": "Distribution Functions",
    "section": "Properties of the CDF",
    "text": "Properties of the CDF\n\nRange: The CDF is always between 0 and 1: \\[\n0 \\leq F_X(x) \\leq 1\n\\]\nNon-Decreasing: The CDF never decreases as \\(x\\) increases.\nLimits:\n\n\\(\\lim_{x \\to -\\infty} F_X(x) = 0\\)\n\\(\\lim_{x \\to \\infty} F_X(x) = 1\\)"
  },
  {
    "objectID": "lectures/5.html#cdf-example",
    "href": "lectures/5.html#cdf-example",
    "title": "Distribution Functions",
    "section": "CDF Example",
    "text": "CDF Example\nFor a die roll (discrete case):\n\n\\(F_X(2) = P(X \\leq 2) = P(X = 1) + P(X = 2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}\\).\n\nFor a normal distribution (continuous case):\n\nUse the CDF to find probabilities, typically provided via tables or software."
  },
  {
    "objectID": "lectures/5.html#probability-density-function-pdf",
    "href": "lectures/5.html#probability-density-function-pdf",
    "title": "Distribution Functions",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\nThe Probability Density Function (PDF) is used for continuous random variables and describes the likelihood of the variable falling within a small interval.\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, dx\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\\[\nP(X = a) = P(a \\leq X \\leq a) = \\int_a^a f_X(x) \\, dx = 0\n\\]"
  },
  {
    "objectID": "lectures/5.html#properties-of-the-pdf",
    "href": "lectures/5.html#properties-of-the-pdf",
    "title": "Distribution Functions",
    "section": "Properties of the PDF:",
    "text": "Properties of the PDF:\n\nNon-Negative: \\[\nf_X(x) \\geq 0 \\quad \\forall x\n\\]\nTotal Area Under Curve: \\[\n\\int_{-\\infty}^\\infty f_X(x) \\, dx = 1\n\\]"
  },
  {
    "objectID": "lectures/5.html#pdf-examples",
    "href": "lectures/5.html#pdf-examples",
    "title": "Distribution Functions",
    "section": "PDF Examples",
    "text": "PDF Examples\nThe PDF of the normal distribution is: \\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]\nHere, \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation."
  },
  {
    "objectID": "lectures/5.html#probability-mass-function-pmf",
    "href": "lectures/5.html#probability-mass-function-pmf",
    "title": "Distribution Functions",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nThe Probability Mass Function (PMF) is used for discrete random variables and gives the probability of each possible value: \\[\nP(X = x) = p_X(x)\n\\]"
  },
  {
    "objectID": "lectures/5.html#properties-of-the-pmf",
    "href": "lectures/5.html#properties-of-the-pmf",
    "title": "Distribution Functions",
    "section": "Properties of the PMF",
    "text": "Properties of the PMF\n\nNon-Negative: \\[\np_X(x) \\geq 0 \\quad \\forall x\n\\]\nSum of Probabilities: \\[\n\\sum_x p_X(x) = 1\n\\]"
  },
  {
    "objectID": "lectures/5.html#pmf-example",
    "href": "lectures/5.html#pmf-example",
    "title": "Distribution Functions",
    "section": "PMF Example",
    "text": "PMF Example\nFor a fair die, the PMF is: \\[\np_X(x) = \\frac{1}{6}, \\quad x \\in \\{1, 2, 3, 4, 5, 6\\}\n\\]"
  },
  {
    "objectID": "lectures/5.html#percentiles",
    "href": "lectures/5.html#percentiles",
    "title": "Distribution Functions",
    "section": "Percentiles",
    "text": "Percentiles\nA percentile is a measure indicating the value below which a given percentage of observations in a group of observations falls. It is used in statistics to provide information about the relative standing of a particular value within a dataset."
  },
  {
    "objectID": "lectures/5.html#percentile-example",
    "href": "lectures/5.html#percentile-example",
    "title": "Distribution Functions",
    "section": "Percentile Example",
    "text": "Percentile Example\nSuppose we have a random variable X representing the height of adults in a population. If the 90th percentile of X is 180 cm, it means that the probability of a randomly selected adult being 180 cm or shorter is 90%."
  },
  {
    "objectID": "lectures/5.html#precentile",
    "href": "lectures/5.html#precentile",
    "title": "Distribution Functions",
    "section": "Precentile",
    "text": "Precentile\n\n\nCode\ndata &lt;- rnorm(500000, mean = 100, sd = 15) |&gt; data.frame(x = _)  # Example: normally distributed data\n\n# Calculate the 75th percentile\npercentile_75 &lt;- qnorm(0.75, 100, 15)\n\nggplot(data = data, aes(x = x)) +\n  geom_density(fill = \"skyblue\", alpha = 0.5) +  # Density curve\n  geom_vline(xintercept = percentile_75, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Density Plot with 75th Percentile Highlighted\", x = \"Data Value\", y = \"Density\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/5.html#applications-1",
    "href": "lectures/5.html#applications-1",
    "title": "Distribution Functions",
    "section": "Applications",
    "text": "Applications\n\nPMF of a Coin TossCDF of an Exp. Dist.Apps. in Real Life\n\n\nFor a fair coin tossed once:\n\n\\(X = 0\\): Tails, \\(P(X = 0) = 0.5\\)\n\\(X = 1\\): Heads, \\(P(X = 1) = 0.5\\)\n\nThe PMF is: \\[\np_X(x) =\n\\begin{cases}\n0.5 & x = 0 \\text{ or } x = 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\nAn exponential random variable with rate \\(\\lambda &gt; 0\\) has a CDF: \\[\nF_X(x) =\n\\begin{cases}\n1 - e^{-\\lambda x}, & x \\geq 0 \\\\\n0, & x &lt; 0\n\\end{cases}\n\\]\nThis can be used to model waiting times between events.\n\n\n\nPMF:\n\nNumber of emails received per hour.\n\nPDF:\n\nHeights of students in a class.\n\nCDF:\n\nThe likelihood of completing a task within a certain time frame."
  },
  {
    "objectID": "lectures/5.html#common-contisuous-rv",
    "href": "lectures/5.html#common-contisuous-rv",
    "title": "Distribution Functions",
    "section": "Common Contisuous RV",
    "text": "Common Contisuous RV\n\n\n\n\n\n\n\n\n\nContinuous RV\nParameters\nNotation\nSupport\n\n\n\n\nUniform\na (minimum), b (maximum)\nUnif(a, b)\n[a, b]\n\n\nNormal (Gaussian)\nμ (mean), σ (standard deviation)\nN(μ, σ²)\n(-∞, ∞)\n\n\nExponential\nλ (rate)\nExp(λ)\n[0, ∞)\n\n\nGamma\nk (shape), θ (scale)\nΓ(k, θ)\n[0, ∞)\n\n\nBeta\nα (shape 1), β (shape 2)\nBeta(α, β)\n[0,1]\n\n\nChi-Squared\nk (degrees of freedom)\nχ²(k)\n[0, ∞)\n\n\nt-distribution\nν (degrees of freedom)\nt(ν)\n(-∞, ∞)\n\n\nF-distribution\nν₁ (degrees of freedom 1), ν₂ (degrees of freedom 2)\nF(ν₁, ν₂)\n[0, ∞)\n\n\nWeibull\nk (shape), λ (scale)\nWeibull(k, λ)\n[0, ∞)\n\n\nLognormal\nμ (location), σ (scale)\nLognormal(μ, σ)\n[0, ∞)"
  },
  {
    "objectID": "lectures/5.html#common-discrete-rv",
    "href": "lectures/5.html#common-discrete-rv",
    "title": "Distribution Functions",
    "section": "Common Discrete RV",
    "text": "Common Discrete RV\n\n\n\n\n\n\n\n\n\nDiscrete RV\nParameters\nNotation\nSupport\n\n\n\n\nBernoulli\np (probability of success)\nBernoulli(p)\n{0, 1}\n\n\nBinomial\nn (number of trials), p (probability of success)\nBin(n, p)\n{0, 1, 2, …, n}\n\n\nGeometric\np (probability of success)\nGeo(p)\n{1, 2, 3, …}\n\n\nPoisson\nλ (average rate of events)\nPois(λ)\n{0, 1, 2, …}\n\n\nNegative Binomial\nr (number of successes), p (probability of success)\nNB(r, p)\n{r, r+1, r+2, …}\n\n\nDiscrete Uniform\nn (number of possible outcomes)\nDUnif(1, n)\n{1, 2, …, n} or a set of n values"
  },
  {
    "objectID": "lectures/5.html#probability-and-pdmf",
    "href": "lectures/5.html#probability-and-pdmf",
    "title": "Distribution Functions",
    "section": "Probability and PD/MF",
    "text": "Probability and PD/MF\n\nContinuousDiscrete\n\n\n\n\nCode\nmean &lt;- 0  # Mean of the distribution\nsd &lt;- 1   # Standard deviation of the distribution\n\n# Generate data points for the curve (more points = smoother curve)\nx &lt;- seq(mean - 3*sd, mean + 3*sd, length.out = 100)  # Range covering most of the distribution\ny &lt;- dnorm(x, mean = mean, sd = sd)  # Calculate the density at each x value\n\n# Create the plot\nggplot(data = data.frame(x, y), aes(x = x, y = y)) +\n  geom_line(color = \"blue\", size = 1) +  # Line for the distribution curve\n  labs(x = \"X\",\n       y = \"Probability Density\") +\n  theme_bw() + # A clean theme\n  scale_x_continuous(breaks = seq(mean - 3*sd, mean + 3*sd, by = sd)) # Ticks at standard deviations\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nn &lt;- 10   # Number of trials\np &lt;- 0.3  # Probability of success on each trial\n\n# Generate the data for the plot\nx &lt;- 0:n  # Possible values for the random variable (number of successes)\ny &lt;- dbinom(x, size = n, prob = p)  # Calculate the probabilities\n\n# Create the plot using geom_col (for discrete data)\nggplot(data = data.frame(x, y), aes(x = factor(x), y = y)) +  # x as factor for discrete bars\n  geom_col(fill = \"skyblue\", color = \"black\") +\n  labs(title = paste(\"Binomial Distribution (n =\", n, \", p =\", p, \")\"),\n       x = \"Number of Successes (x)\",\n       y = \"Probability P(X = x)\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/5.html#normal-distribution-1",
    "href": "lectures/5.html#normal-distribution-1",
    "title": "Distribution Functions",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe Normal Distribution is a probability distribution that is symmetric, with most of the data points clustering around the mean.\n\nIt’s bell-shaped and is defined mathematically by two parameters:\n\nMean (\\(\\mu\\)): The center or peak of the distribution.\nStandard Deviation (\\(\\sigma\\)): Controls the spread of the distribution.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nFor any normally distributed data, the highest probability density is at the mean, and as you move away from the mean, the probability density gradually decreases."
  },
  {
    "objectID": "lectures/5.html#properties",
    "href": "lectures/5.html#properties",
    "title": "Distribution Functions",
    "section": "Properties",
    "text": "Properties\n\nSymmetry: It is perfectly symmetric about the mean, meaning the left side is a mirror image of the right.\nUnimodal: There is a single peak at the mean.\nMean, Median, and Mode are Equal: In a normal distribution, these three measures of central tendency are located at the same point.\n68-95-99.7 Rule (Empirical Rule)\n\n\nThis rule helps us understand how data is distributed in a normal curve and provides a quick way to estimate probabilities for normally distributed data."
  },
  {
    "objectID": "lectures/5.html#normal-distribution-2",
    "href": "lectures/5.html#normal-distribution-2",
    "title": "Distribution Functions",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\\[\nX\\sim N(\\mu, \\sigma)\n\\]\n\\[\n-\\infty &lt; X &lt; \\infty\n\\]\n\n\\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "lectures/5.html#the-standard-normal-distribution",
    "href": "lectures/5.html#the-standard-normal-distribution",
    "title": "Distribution Functions",
    "section": "The Standard Normal Distribution",
    "text": "The Standard Normal Distribution\nThe Standard Normal Distribution is a special type of normal distribution with a mean of 0 and a standard deviation of 1. It’s often used as a reference to convert any normal distribution to a standard form."
  },
  {
    "objectID": "lectures/5.html#z-scores",
    "href": "lectures/5.html#z-scores",
    "title": "Distribution Functions",
    "section": "Z-Scores",
    "text": "Z-Scores\nA Z-score (or standard score) tells us how many standard deviations an individual data point is from the mean. It’s calculated as:\n\\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]\n\nIf \\(Z\\) is positive, the data point is above the mean.\nIf \\(Z\\) is negative, the data point is below the mean.\nUsing Z-scores, we can compare values across different normal distributions or find the probability associated with a particular score."
  },
  {
    "objectID": "lectures/5.html#why-the-normal-distribution-is-important",
    "href": "lectures/5.html#why-the-normal-distribution-is-important",
    "title": "Distribution Functions",
    "section": "Why the Normal Distribution is Important",
    "text": "Why the Normal Distribution is Important\n\nIt Describes Many Natural Phenomena: Heights, weights, test scores, measurement errors, and countless other variables follow a normal distribution, especially when influenced by many small, random factors.\nPredictive Power: With normally distributed data, we can make predictions and infer probabilities, thanks to the 68-95-99.7 rule.\nCentral Limit Theorem: The normal distribution is foundational to the Central Limit Theorem, which tells us that, regardless of the original data distribution, the sampling distribution of the sample mean will approach a normal distribution as sample size increases.\nEase of Use in Statistical Methods: Many statistical tests and methods assume normality, allowing for simplified calculations and reliable inferences."
  },
  {
    "objectID": "lectures/5.html#apps-of-the-normal-dist.",
    "href": "lectures/5.html#apps-of-the-normal-dist.",
    "title": "Distribution Functions",
    "section": "Apps of the Normal Dist.",
    "text": "Apps of the Normal Dist.\n\nStandardized TestingFinance\n\n\nScores on standardized tests, such as IQ tests or the SAT, are often designed to follow a normal distribution. By knowing a student’s score in terms of Z-scores, we can determine their percentile or how they compare to other test-takers.\n\n\nIn finance, stock returns and other economic factors are often modeled with a normal distribution to estimate risk, forecast trends, and make informed investment decisions."
  },
  {
    "objectID": "lectures/5.html#finding-probability",
    "href": "lectures/5.html#finding-probability",
    "title": "Distribution Functions",
    "section": "Finding Probability",
    "text": "Finding Probability\n\\[\nX \\sim N(\\mu, \\sigma)\n\\]\n\n\\[\nP(a \\leq X \\leq b) = P(a &lt; X &lt; b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx\n\\]"
  },
  {
    "objectID": "lectures/5.html#what-is-the-empirical-rule",
    "href": "lectures/5.html#what-is-the-empirical-rule",
    "title": "Distribution Functions",
    "section": "What is the Empirical Rule?",
    "text": "What is the Empirical Rule?\nThe Empirical Rule provides a way to understand the spread of data in a normal distribution by describing how data points cluster around the mean. According to this rule:\n\nApproximately 68% of data points fall within one standard deviation of the mean.\nApproximately 95% of data points fall within two standard deviations of the mean.\nApproximately 99.7% of data points fall within three standard deviations of the mean.\n\nThe empirical rule is very helpful because, with just the mean and standard deviation, we can quickly estimate how data is distributed within a normal curve."
  },
  {
    "objectID": "lectures/5.html#empirical-rule-to-the-normal-dist.",
    "href": "lectures/5.html#empirical-rule-to-the-normal-dist.",
    "title": "Distribution Functions",
    "section": "Empirical Rule to the Normal Dist.",
    "text": "Empirical Rule to the Normal Dist.\nLet’s define the key terms and apply the empirical rule to the normal distribution.\n\nMean (μ): This is the central point of the normal distribution where the data clusters around.\nStandard Deviation (σ): This is a measure of how spread out the data points are from the mean."
  },
  {
    "objectID": "lectures/5.html#empirical-rule-to-the-normal-dist.-1",
    "href": "lectures/5.html#empirical-rule-to-the-normal-dist.-1",
    "title": "Distribution Functions",
    "section": "Empirical Rule to the Normal Dist.",
    "text": "Empirical Rule to the Normal Dist.\nIn a normal distribution:\n\n68% of data lies between \\((\\mu - \\sigma)\\) and \\((\\mu + \\sigma)\\).\n95% of data lies between \\((\\mu - 2\\sigma)\\) and \\((\\mu + 2\\sigma)\\).\n99.7% of data lies between \\((\\mu - 3\\sigma)\\) and \\((\\mu + 3\\sigma)\\).\n\nThese intervals allow us to estimate probabilities for data within each range without needing to calculate exact probabilities."
  },
  {
    "objectID": "lectures/5.html#visualizing-the-empirical-rule",
    "href": "lectures/5.html#visualizing-the-empirical-rule",
    "title": "Distribution Functions",
    "section": "Visualizing the Empirical Rule",
    "text": "Visualizing the Empirical Rule\nTo better understand the empirical rule, imagine a symmetric, bell-shaped normal curve. Here’s how it would look based on the empirical rule:\n\nThe 68% region represents the middle of the curve, starting one standard deviation left of the mean and ending one standard deviation right.\nThe 95% region stretches further out, covering almost the entire curve except for the outer tails.\nThe 99.7% region includes nearly all data points, covering the entire curve except for a tiny fraction at each extreme.\n\nThis visualization shows how the data is most concentrated around the mean, with less data appearing as we move further away."
  },
  {
    "objectID": "lectures/5.html#using-the-empirical-rule-for-probabilities",
    "href": "lectures/5.html#using-the-empirical-rule-for-probabilities",
    "title": "Distribution Functions",
    "section": "Using the Empirical Rule for Probabilities",
    "text": "Using the Empirical Rule for Probabilities\nThe empirical rule helps us answer questions like:\n\nWhat percentage of data points fall within a certain range?\nHow unusual is a data point located far from the mean?"
  },
  {
    "objectID": "lectures/5.html#examples",
    "href": "lectures/5.html#examples",
    "title": "Distribution Functions",
    "section": "Examples",
    "text": "Examples\n\nIf we know that a data point lies more than two standard deviations away from the mean, we know it’s in the outer 5% of the distribution, making it relatively rare.\nUsing the rule, we can estimate that around 95% of values should lie within two standard deviations of the mean. If we observe data points outside of this range, we might consider them outliers."
  },
  {
    "objectID": "lectures/5.html#more-examples",
    "href": "lectures/5.html#more-examples",
    "title": "Distribution Functions",
    "section": "More Examples",
    "text": "More Examples\n\nExam ScoresHeights of Adults\n\n\nSuppose exam scores are normally distributed with a mean of 70 and a standard deviation of 10.\n\n68% of students scored between 60 and 80 (70 ± 10).\n95% of students scored between 50 and 90 (70 ± 20).\n99.7% of students scored between 40 and 100 (70 ± 30).\n\n\n\nAssume that adult heights follow a normal distribution with a mean height of 170 cm and a standard deviation of 8 cm.\n\n68% of adults have heights between 162 cm and 178 cm (170 ± 8).\n95% of adults have heights between 154 cm and 186 cm (170 ± 16).\n99.7% of adults have heights between 146 cm and 194 cm (170 ± 24)."
  },
  {
    "objectID": "lectures/5.html#binomial-distribution-1",
    "href": "lectures/5.html#binomial-distribution-1",
    "title": "Distribution Functions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nThe Binomial Distribution is a probability distribution that models the number of successes in a fixed number of trials, where each trial has:\n\nTwo possible outcomes: typically called “success” and “failure.”\nA constant probability of success, \\(p\\), on each trial."
  },
  {
    "objectID": "lectures/5.html#real-world-examples",
    "href": "lectures/5.html#real-world-examples",
    "title": "Distribution Functions",
    "section": "Real-World Examples:",
    "text": "Real-World Examples:\n\nTossing a coin \\(n\\) times and counting how many heads you get.\nRolling a die \\(n\\) times and counting how many times you roll a 6.\nAdministering a medical treatment to \\(n\\) patients and recording how many recover.\n\n\nThe binomial distribution answers questions like:\n\n“What is the probability of getting exactly 3 heads in 5 coin tosses?”\n“What is the likelihood of at least 4 successes in 10 trials?”"
  },
  {
    "objectID": "lectures/5.html#conditions-for-a-binomial-experiment",
    "href": "lectures/5.html#conditions-for-a-binomial-experiment",
    "title": "Distribution Functions",
    "section": "Conditions for a Binomial Experiment",
    "text": "Conditions for a Binomial Experiment\n\nFixed Number of Trials (\\(n\\)):\n\nThe experiment consists of a set number of trials.\n\nTwo Possible Outcomes:\n\nEach trial results in either a success (e.g., heads) or a failure (e.g., tails).\n\nConstant Probability of Success (\\(p\\)):\n\nThe probability of success remains the same for each trial.\n\nIndependence:\n\nThe outcome of one trial does not affect the outcomes of other trials."
  },
  {
    "objectID": "lectures/5.html#the-binomial-probability-formula",
    "href": "lectures/5.html#the-binomial-probability-formula",
    "title": "Distribution Functions",
    "section": "The Binomial Probability Formula",
    "text": "The Binomial Probability Formula\n\\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\nWhere:\n\n\\(P(X = k)\\): Probability of exactly \\(k\\) successes.\n\\(n\\): Number of trials.\n\\(k\\): Number of successes.\n\\(p\\): Probability of success on a single trial.\n\\(1-p\\): Probability of failure.\n\\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\): Represents the number of ways to choose \\(k\\) successes from \\(n\\) trials"
  },
  {
    "objectID": "lectures/5.html#example-calculation",
    "href": "lectures/5.html#example-calculation",
    "title": "Distribution Functions",
    "section": "Example Calculation",
    "text": "Example Calculation\nSuppose you flip a fair coin 5 times (\\(n = 5, p = 0.5\\)) and want to know the probability of getting exactly 3 heads (\\(k = 3\\)).\n\n\\[\nP(X = 3) = \\binom{5}{3} (0.5)^3 (1-0.5)^{5-3}\n\\]"
  },
  {
    "objectID": "lectures/5.html#properties-of-the-binomial-dist.",
    "href": "lectures/5.html#properties-of-the-binomial-dist.",
    "title": "Distribution Functions",
    "section": "Properties of the Binomial Dist.",
    "text": "Properties of the Binomial Dist.\n\nMean and VarianceShape of the Distribution\n\n\nFor a binomial random variable \\(X\\):\n\nMean (Expected Value): \\(\\mu = n \\cdot p\\)\nVariance: \\(\\sigma^2 = n \\cdot p \\cdot (1-p)\\)\nStandard Deviation: \\(\\sigma = \\sqrt{n \\cdot p \\cdot (1-p)}\\)\n\n\n\n\nIf \\(p = 0.5\\), the distribution is symmetric.\nIf \\(p &gt; 0.5\\), the distribution is skewed left.\nIf \\(p &lt; 0.5\\), the distribution is skewed right."
  },
  {
    "objectID": "lectures/5.html#applications-2",
    "href": "lectures/5.html#applications-2",
    "title": "Distribution Functions",
    "section": "Applications",
    "text": "Applications\n\nQuality ControlSportsClinical Trials\n\n\nA factory produces lightbulbs, and 95% of them meet quality standards. If you randomly test 10 bulbs, what is the probability that exactly 8 bulbs pass the test?\nHere:\n\n\\(n = 10\\), \\(p = 0.95\\), \\(k = 8\\).\n\n\\[\nP(X = 8) = \\binom{10}{8} (0.95)^8 (0.05)^2\n\\]\n\n\nA basketball player has a 60% chance of making a free throw. If they take 5 shots, what is the probability of making at least 3 shots?\nHere: - \\(n = 5\\), \\(p = 0.6\\).\n\\[\nP(X \\geq 3) = P(X = 3) + P(X = 4) + P(X = 5)\n\\]\n\n\nIn a clinical trial, a new drug has a 70% success rate. If 15 patients are treated, what is the probability that exactly 10 respond positively to the treatment?"
  },
  {
    "objectID": "lectures/5.html#poisson-distribution-1",
    "href": "lectures/5.html#poisson-distribution-1",
    "title": "Distribution Functions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe Poisson Distribution is a discrete probability distribution that models the number of events occurring within a fixed interval. These events must happen independently and at a constant average rate."
  },
  {
    "objectID": "lectures/5.html#real-world-examples-1",
    "href": "lectures/5.html#real-world-examples-1",
    "title": "Distribution Functions",
    "section": "Real-World Examples",
    "text": "Real-World Examples\n\nThe number of emails you receive in an hour.\nThe number of cars passing through a toll booth in 10 minutes.\nThe number of defects in a square meter of fabric.\n\n\nThe Poisson distribution helps us answer questions like:\n\n“What is the probability of receiving 5 emails in the next hour?”\n“How likely is it to have 2 defects in a single square meter?”"
  },
  {
    "objectID": "lectures/5.html#conditions-for-the-poisson-dist.",
    "href": "lectures/5.html#conditions-for-the-poisson-dist.",
    "title": "Distribution Functions",
    "section": "Conditions for the Poisson Dist.",
    "text": "Conditions for the Poisson Dist.\n\nEvents Occur Randomly:\n\nThe events are random and unpredictable.\n\nIndependence:\n\nThe occurrence of one event does not affect the probability of another event occurring.\n\nConstant Average Rate (\\(\\lambda\\)):\n\nThe average number of events (\\(\\lambda\\)) over a fixed interval remains constant.\n\nNon-Overlapping Intervals:\n\nEvents in one interval do not influence events in another."
  },
  {
    "objectID": "lectures/5.html#the-poisson-probability-formula",
    "href": "lectures/5.html#the-poisson-probability-formula",
    "title": "Distribution Functions",
    "section": "The Poisson Probability Formula",
    "text": "The Poisson Probability Formula\n\\[\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\]\nWhere:\n\n\\(P(X = k)\\): Probability of \\(k\\) events occurring.\n\\(\\lambda\\): Average number of events in the interval.\n\\(e \\approx 2.718\\)\n\\(k!\\): Factorial of \\(k\\), calculated as \\(k \\times (k-1) \\times \\dots \\times 1\\)."
  },
  {
    "objectID": "lectures/5.html#example-calculation-1",
    "href": "lectures/5.html#example-calculation-1",
    "title": "Distribution Functions",
    "section": "Example Calculation",
    "text": "Example Calculation\nSuppose a call center receives an average of 10 calls per hour (\\(\\lambda = 10\\)). What is the probability of receiving exactly 7 calls in the next hour ($ k = 7 $)?\n\\[\nP(X = 7) = \\frac{10^7 e^{-10}}{7!}\n\\]"
  },
  {
    "objectID": "lectures/5.html#poisson-distribution-properties",
    "href": "lectures/5.html#poisson-distribution-properties",
    "title": "Distribution Functions",
    "section": "Poisson Distribution Properties",
    "text": "Poisson Distribution Properties\n\nMean: \\(\\mu = \\lambda\\)\nVariance: \\(\\sigma^2 = \\lambda\\)\nStandard Deviation: \\(\\sigma = \\sqrt{\\lambda}\\)"
  },
  {
    "objectID": "lectures/5.html#applications-3",
    "href": "lectures/5.html#applications-3",
    "title": "Distribution Functions",
    "section": "Applications",
    "text": "Applications\n\nTraffic FlowDefective Products\n\n\nA toll booth observes an average of 3 cars passing through every 5 minutes (\\(\\lambda = 3\\)). What is the probability of seeing exactly 5 cars in the next 5 minutes?\nUsing the formula: \\[\nP(X = 5) = \\frac{3^5 e^{-3}}{5!} = \\frac{243 \\cdot 0.0498}{120} \\approx 0.1008\n\\]\n\n\nA factory produces an average of 2 defective items per day (\\(\\lambda = 2\\)). What is the probability of finding no defective items in a day (\\(k = 0\\))?\n\\[\nP(X = 0) = \\frac{2^0 e^{-2}}{0!} = e^{-2} \\approx 0.1353\n\\]"
  },
  {
    "objectID": "lectures/5.html#distributions-in-r",
    "href": "lectures/5.html#distributions-in-r",
    "title": "Distribution Functions",
    "section": "Distributions in R",
    "text": "Distributions in R\n\n?Distributions"
  },
  {
    "objectID": "lectures/5.html#function-structure",
    "href": "lectures/5.html#function-structure",
    "title": "Distribution Functions",
    "section": "Function Structure",
    "text": "Function Structure\n\n\n\nLetter\nFunctionality\n\n\n\n\n“d”\nreturns the height of the probability density function\n\n\n“p”\nreturns the cummulative density function value\n\n\n“q”\nreturns the inverse cummulative density function (percentiles)\n\n\n“r”\nreturns a randomly generated number"
  },
  {
    "objectID": "lectures/5.html#probabilities",
    "href": "lectures/5.html#probabilities",
    "title": "Distribution Functions",
    "section": "Probabilities",
    "text": "Probabilities\nR can compute the probabilities of a distribution given the correct parameters:\n\nCummulative probability: p is used in front of the distribution R function\nProbability for a discrete distribution: d is used in front of the distribution R function\n\nNote: Continuous Distribution Functions will not yield a valid probability value."
  },
  {
    "objectID": "lectures/5.html#examples-1",
    "href": "lectures/5.html#examples-1",
    "title": "Distribution Functions",
    "section": "Examples",
    "text": "Examples\n\n123\n\n\nFind \\(P(X \\leq 5 )\\) where \\(X \\sim N(6,2)\\).\n\n\nCode\npnorm(5, mean = 6, sd = 2)\n\n\n\n\nFind \\(P(X \\geq 7 )\\) where \\(X \\sim N(6,2)\\).\n\n\nCode\n1 - pnorm(7, mean = 6, sd = 2)\n\n\n\n\nFind \\(P(X = 20 )\\) where \\(X \\sim Bin(30,0.8)\\).\n\n\nCode\ndbinom(20, size = 30, prob = 0.8)"
  },
  {
    "objectID": "lectures/5.html#more-examples-1",
    "href": "lectures/5.html#more-examples-1",
    "title": "Distribution Functions",
    "section": "More Examples",
    "text": "More Examples\n\n123\n\n\nFind \\(P(2 \\leq X \\leq 5 )\\) where \\(X \\sim N(6,2)\\).\n\n\nCode\npnorm(5, 6, 2) - pnorm(2, 6, 2) \n\n\n\n\nFind \\(P(14 &lt; X &lt; 20)\\) where \\(X \\sim Pois(16)\\).\n\n\nCode\nppois(19, lambda = 16) - ppois(14, lambda = 16)\n\n## OR\n\ndpois(15, 16) + dpois(16, 16) + dpois(17, 16) + dpois(18, 16) + dpois(19, 16)\n\n\n\n\nFind \\(P(23 &lt; X)\\) where \\(X \\sim Pois(12)\\).\n\n\nCode\n1 - ppois(23, 12)"
  },
  {
    "objectID": "lectures/5.html#percentiles-1",
    "href": "lectures/5.html#percentiles-1",
    "title": "Distribution Functions",
    "section": "Percentiles",
    "text": "Percentiles\nFinding the values (percentiles) for any distributions can be found by using the q-based distribution R function such as qnorm(), qpois(), and qbinom() functions."
  },
  {
    "objectID": "lectures/5.html#examples-2",
    "href": "lectures/5.html#examples-2",
    "title": "Distribution Functions",
    "section": "Examples",
    "text": "Examples\n\n123\n\n\nFinding the \\(95^{th}\\) percentile from \\(N(0,1)\\), we will use the qnorm().\n\n\nCode\nqnorm(.95, 0, 1)\n\n\n\n\nFinding the \\(95^{th}\\) percentile from a Poisson distribution with \\(\\lambda = 9.5\\).\n\n\nCode\nqpois(.95, 9.5)\n\n\n\n\nFinding the \\(75^{th}\\) percentile for \\(Bin(45,.4)\\).\n\n\nCode\nqbinom(.75, 45, .4)"
  },
  {
    "objectID": "lectures/5.html#random-number-generator",
    "href": "lectures/5.html#random-number-generator",
    "title": "Distribution Functions",
    "section": "Random Number Generator",
    "text": "Random Number Generator\nR is capable of generating random numbers. For example if we want to generate a random sample of size fifty from a normal distribution with mean eight and variance three, we will use the rnorm(). If we want to generate a random sample from any distribution, use the distribution function with r in front of it."
  },
  {
    "objectID": "lectures/5.html#examples-3",
    "href": "lectures/5.html#examples-3",
    "title": "Distribution Functions",
    "section": "Examples",
    "text": "Examples\n\n1234\n\n\nLet’s first generate the random sample of fifty from \\(X \\sim N(8,3)\\).\n\n\nCode\nrnorm(50, 8, sqrt(3))\n\n\n\n\nGenerate a random sample of 100 form an \\(X \\sim Gamma (2,3)\\).\n\n\nCode\nrgamma(100, 2,3)\n\n\n\n\nGenerate a random sample of 100 form an \\(X \\sim Binom (25,.23)\\).\n\n\nCode\nrbinom(n = 100, size  = 25, prob = 0.23)\n\n\n\n\nGenerate a random sample of 100 form an \\(X \\sim Pois (34.4)\\).\n\n\nCode\nrpois(n = 100, lambda = 34.4)"
  },
  {
    "objectID": "lectures/7.html#melanoma",
    "href": "lectures/7.html#melanoma",
    "title": "Simple Logistic Regression",
    "section": "Melanoma",
    "text": "Melanoma\nMelanoma is a type of skin cancer that causes the cells that produce melanin to grow out of control. What makes Melanoma so dangerous is that it can metastasize to other parts of the body."
  },
  {
    "objectID": "lectures/7.html#outcome-of-interest",
    "href": "lectures/7.html#outcome-of-interest",
    "title": "Simple Logistic Regression",
    "section": "Outcome of Interest",
    "text": "Outcome of Interest\nWe are interested in learning how do different factors affect and individual’s chances of survival. Therefore, we are measuring patients and if they lived or died during a study period."
  },
  {
    "objectID": "lectures/7.html#data",
    "href": "lectures/7.html#data",
    "title": "Simple Logistic Regression",
    "section": "Data",
    "text": "Data\nWe will be using the Melanoma data set with the following variables: dead (died by Melanoma, 1=yes, 0= no) and thickness (tumour thickness in mm)."
  },
  {
    "objectID": "lectures/7.html#plot",
    "href": "lectures/7.html#plot",
    "title": "Simple Logistic Regression",
    "section": "Plot",
    "text": "Plot\n\n\nCode\nMelanoma |&gt; \n  ggplot(aes(thickness, dead)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/7.html#plot-1",
    "href": "lectures/7.html#plot-1",
    "title": "Simple Logistic Regression",
    "section": "Plot",
    "text": "Plot\n\n\nCode\nMelanoma |&gt; \n  ggplot(aes(thickness, dead)) +\n  geom_point() +\n  stat_smooth(method = \"glm\", se = F,  \n              method.args = list(family = \"binomial\"))"
  },
  {
    "objectID": "lectures/7.html#construct-model",
    "href": "lectures/7.html#construct-model",
    "title": "Simple Logistic Regression",
    "section": "Construct Model",
    "text": "Construct Model\n\\[\n\\left(\\begin{array}{c}\nDead \\\\\nAlive\n\\end{array}\\right) = \\beta_0 + \\beta_1X\n\\]"
  },
  {
    "objectID": "lectures/7.html#let",
    "href": "lectures/7.html#let",
    "title": "Simple Logistic Regression",
    "section": "Let …",
    "text": "Let …\n\\[\nY = \\left\\{\\begin{array}{cc}\n1 & Dead \\\\\n0 & Alive\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "lectures/7.html#construct-a-model",
    "href": "lectures/7.html#construct-a-model",
    "title": "Simple Logistic Regression",
    "section": "Construct a Model",
    "text": "Construct a Model\n\\[\nP\\left(Y = 1\\right) = \\beta_0 + \\beta_1X\n\\]"
  },
  {
    "objectID": "lectures/7.html#construct-a-model-1",
    "href": "lectures/7.html#construct-a-model-1",
    "title": "Simple Logistic Regression",
    "section": "Construct a Model",
    "text": "Construct a Model\n\\[\nP\\left(Y = 1\\right) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\n\\]"
  },
  {
    "objectID": "lectures/7.html#construct-a-model-2",
    "href": "lectures/7.html#construct-a-model-2",
    "title": "Simple Logistic Regression",
    "section": "Construct a Model",
    "text": "Construct a Model\n\\[\n\\frac{P(Y = 1)}{P(Y = 0)} = e^{\\beta_0 + \\beta_1X}\n\\]\nwhere \\(\\frac{P(Y = 1)}{P(Y = 0)}\\) are considered the odds of observing \\(Y = 1\\)."
  },
  {
    "objectID": "lectures/7.html#the-logistic-model",
    "href": "lectures/7.html#the-logistic-model",
    "title": "Simple Logistic Regression",
    "section": "The Logistic Model",
    "text": "The Logistic Model\n\\[\n\\log\\left\\{\\frac{P(Y = 1)}{P(Y = 0)}\\right\\} = \\beta_0 + \\beta_1X\n\\]"
  },
  {
    "objectID": "lectures/7.html#logistic-regression-1",
    "href": "lectures/7.html#logistic-regression-1",
    "title": "Simple Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic Regression is used to model the association between a set of predictors and a binary outcome variable.\n\nThis is similar Linear Regression which models the association between a set of predictors and a numerical outcome variable."
  },
  {
    "objectID": "lectures/7.html#logistic-regression-2",
    "href": "lectures/7.html#logistic-regression-2",
    "title": "Simple Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic Regression uses the logistic model to formulate the relationship between the predictors and the outcome.\n\nMore specifically, for an outcome of Y:\n\\[\nY = \\left\\{\\begin{array}{cc}\n1 & \\text{Category 1} \\\\\n0 & \\text{Category 2}\n\\end{array}\\right.\n\\]\nThe Predictors variable will model the probability of observing category 1 (\\(P(Y=1)\\))"
  },
  {
    "objectID": "lectures/7.html#logistic-model",
    "href": "lectures/7.html#logistic-model",
    "title": "Simple Logistic Regression",
    "section": "Logistic Model",
    "text": "Logistic Model\n\\[\n\\log\\left\\{\\frac{P(Y = 1)}{P(Y = 0)}\\right\\} = \\beta_0 + \\beta_1X\n\\]"
  },
  {
    "objectID": "lectures/7.html#regression-coefficients-beta",
    "href": "lectures/7.html#regression-coefficients-beta",
    "title": "Simple Logistic Regression",
    "section": "Regression Coefficients \\(\\beta\\)",
    "text": "Regression Coefficients \\(\\beta\\)\nThe regression coefficients quantify how a specific predictor changes the odds of observing the first category of the outcome (\\(Y = 1\\))"
  },
  {
    "objectID": "lectures/7.html#estimating-beta",
    "href": "lectures/7.html#estimating-beta",
    "title": "Simple Logistic Regression",
    "section": "Estimating \\(\\beta\\)",
    "text": "Estimating \\(\\beta\\)\nTo obtain the numerical value for \\(\\beta\\), denoted as \\(\\hat \\beta\\), we will be finding the values of \\(\\hat \\beta\\) that maximizes the likelihood function:\n\\[\nL(\\boldsymbol \\beta) = \\prod_{i=1}^n \\left(\\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\\right)^{Y_i}\\left(\\frac{1}{1 + e^{\\beta_0 + \\beta_1X}}\\right)^{1-Y_i}\n\\]\nThe likelihood function can be thought as the probability of observing the entire data set. Therefore, we want to choose the values the \\(\\beta_0\\) and \\(\\beta_1\\) that will result in the highest probability of observing the data."
  },
  {
    "objectID": "lectures/7.html#estimated-parameters",
    "href": "lectures/7.html#estimated-parameters",
    "title": "Simple Logistic Regression",
    "section": "Estimated Parameters",
    "text": "Estimated Parameters\nThe values you obtain (\\(\\hat \\beta\\)) tell you the relationship between the a predictor variable and the log odds of observing the first category of the outcome \\(Y=1\\).\n\nExponentiating the estimate (\\(e^{\\hat \\beta}\\)) will give you the relationship between a predictor variable and the odds of observing the first category of the outcome \\(Y=1\\)."
  },
  {
    "objectID": "lectures/7.html#interpreting-hat-beta",
    "href": "lectures/7.html#interpreting-hat-beta",
    "title": "Simple Logistic Regression",
    "section": "Interpreting \\(\\hat \\beta\\)",
    "text": "Interpreting \\(\\hat \\beta\\)\nFor a continuous predictor variable:\nAs X increases by 1 unit, the odds of observing the first category (\\(Y = 1\\)) increases by a factor of \\(e^{\\hat\\beta}\\)."
  },
  {
    "objectID": "lectures/7.html#logistic-regression-in-r",
    "href": "lectures/7.html#logistic-regression-in-r",
    "title": "Simple Logistic Regression",
    "section": "Logistic Regression in R",
    "text": "Logistic Regression in R\n\n\nCode\nglm(Y ~ X,\n    data = DATA,\n    family = binomial())"
  },
  {
    "objectID": "lectures/7.html#example",
    "href": "lectures/7.html#example",
    "title": "Simple Logistic Regression",
    "section": "Example",
    "text": "Example\nModelling dead by thickness:\n\n\nCode\nglm(dead ~ thickness,\n    data = Melanoma,\n    family = binomial())\n\n\n#&gt; \n#&gt; Call:  glm(formula = dead ~ thickness, family = binomial(), data = Melanoma)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    thickness  \n#&gt;     -1.6140       0.2088  \n#&gt; \n#&gt; Degrees of Freedom: 204 Total (i.e. Null);  203 Residual\n#&gt; Null Deviance:       242.4 \n#&gt; Residual Deviance: 226.1     AIC: 230.1\n\n\n\\[\n\\log(odds\\ of\\ dying ) = -1.614 + 0.21 (thickness)\n\\]"
  },
  {
    "objectID": "lectures/7.html#interpretation",
    "href": "lectures/7.html#interpretation",
    "title": "Simple Logistic Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\n\\log(odds\\ of\\ dying ) = -1.614 + 0.21 (thickness)\n\\]\n\n\nCode\nexp(0.21)\n\n\n#&gt; [1] 1.233678\n\n\n\nAs age increases by 1 year, the odds of experiencing death increases by a factor of 1.232."
  },
  {
    "objectID": "lectures/7.html#working-with-probabilities",
    "href": "lectures/7.html#working-with-probabilities",
    "title": "Simple Logistic Regression",
    "section": "Working with probabilities",
    "text": "Working with probabilities\nAs you can see, working with odds may be unintuitive for the average person. It will be better to predict the probability and display those results to individuals."
  },
  {
    "objectID": "lectures/7.html#predicting-probability",
    "href": "lectures/7.html#predicting-probability",
    "title": "Simple Logistic Regression",
    "section": "Predicting Probability",
    "text": "Predicting Probability\n\\[\n\\hat P\\left(Y = 1\\right) = \\frac{e^{\\hat\\beta_0 + \\hat\\beta_1X}}{1 + e^{\\hat\\beta_0 + \\hat\\beta_1X}}\n\\]"
  },
  {
    "objectID": "lectures/7.html#prediction-in-r",
    "href": "lectures/7.html#prediction-in-r",
    "title": "Simple Logistic Regression",
    "section": "Prediction in R",
    "text": "Prediction in R\n\n\nCode\nxglm &lt;- glm(Y ~ X,\n            data = DATA,\n            family = binomial())\nndf &lt;- data.frame(X = VAL)\npredict(xglm,\n        ndf,\n        type = \"response\")"
  },
  {
    "objectID": "lectures/7.html#example-1",
    "href": "lectures/7.html#example-1",
    "title": "Simple Logistic Regression",
    "section": "Example 1",
    "text": "Example 1\nPredict the probability of observing death for a patient with a tumor thickness of 2.9.\n\n\nCode\nxglm &lt;- glm(dead ~ thickness,\n    data = Melanoma,\n    family = binomial())\nndf &lt;- data.frame(thickness = 2.9)\npredict(xglm, ndf, type = \"response\")\n\n\n#&gt;        1 \n#&gt; 0.267291"
  },
  {
    "objectID": "lectures/7.html#example-2",
    "href": "lectures/7.html#example-2",
    "title": "Simple Logistic Regression",
    "section": "Example 2",
    "text": "Example 2\nPredict the probability of observing death for a patient who a tumor thickness of 1.9.\n\n\nCode\nxglm &lt;- glm(dead ~ thickness,\n    data = Melanoma,\n    family = binomial())\nndf &lt;- data.frame(thickness = 1.9)\npredict(xglm, ndf, type = \"response\")\n\n\n#&gt;         1 \n#&gt; 0.2284212"
  },
  {
    "objectID": "lectures/7.html#example-3",
    "href": "lectures/7.html#example-3",
    "title": "Simple Logistic Regression",
    "section": "Example 3",
    "text": "Example 3\nPredict the probability of observing death for a patient who a tumor thickness of 3.9.\n\n\nCode\nxglm &lt;- glm(dead ~ thickness,\n    data = Melanoma,\n    family = binomial())\nndf &lt;- data.frame(thickness = 3.9)\npredict(xglm, ndf, type = \"response\")\n\n\n#&gt;         1 \n#&gt; 0.3101168"
  },
  {
    "objectID": "lectures/7.html#example-4",
    "href": "lectures/7.html#example-4",
    "title": "Simple Logistic Regression",
    "section": "Example",
    "text": "Example\n\n\n\nTumor Thickness\n1.9\n2.9\n3.9\n\n\n\n\nProbabiltiy\n22.8%\n26.7%\n31.0%"
  },
  {
    "objectID": "lectures/8.html#explaining-variation",
    "href": "lectures/8.html#explaining-variation",
    "title": "Group Regression Models",
    "section": "Explaining Variation",
    "text": "Explaining Variation\n\nThis is the process where we try to reduce the variation with the use of other variables.\n\n\nCan be thought of as getting it less wrong when taking an educated guess."
  },
  {
    "objectID": "lectures/8.html#explaining-variation-1",
    "href": "lectures/8.html#explaining-variation-1",
    "title": "Group Regression Models",
    "section": "Explaining Variation",
    "text": "Explaining Variation\n\n\nCode\nggplot(penguins, aes(body_mass_g)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/8.html#variation-with-one-variable",
    "href": "lectures/8.html#variation-with-one-variable",
    "title": "Group Regression Models",
    "section": "Variation with One Variable",
    "text": "Variation with One Variable\n\n\nCode\nggplot(penguins, aes(body_mass_g, fill = species)) +\n  geom_density(alpha = .5)"
  },
  {
    "objectID": "lectures/8.html#generated-model",
    "href": "lectures/8.html#generated-model",
    "title": "Group Regression Models",
    "section": "Generated Model",
    "text": "Generated Model\n\\[\nY \\sim DGP_1\n\\]"
  },
  {
    "objectID": "lectures/8.html#a-simple-model-1",
    "href": "lectures/8.html#a-simple-model-1",
    "title": "Group Regression Models",
    "section": "A Simple Model",
    "text": "A Simple Model\n\n\nCode\nggplot(penguins, aes(body_mass_g)) +\n  geom_density()"
  },
  {
    "objectID": "lectures/8.html#a-simple-model-2",
    "href": "lectures/8.html#a-simple-model-2",
    "title": "Group Regression Models",
    "section": "A Simple Model",
    "text": "A Simple Model\n\\[\nY = \\_\\_\\_ + error\n\\]"
  },
  {
    "objectID": "lectures/8.html#notation",
    "href": "lectures/8.html#notation",
    "title": "Group Regression Models",
    "section": "Notation",
    "text": "Notation\n\\[\nY = \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\varepsilon\n\\]"
  },
  {
    "objectID": "lectures/8.html#the-simple-generated-model",
    "href": "lectures/8.html#the-simple-generated-model",
    "title": "Group Regression Models",
    "section": "The Simple Generated Model",
    "text": "The Simple Generated Model\n\\[\nY \\sim \\beta_0 + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP_2\n\\]\n\n\\(DGP_2\\) is not the same as the \\(DGP_1\\), it is transformed due \\(\\beta_0\\). Consider this the NULL \\(DGP\\)."
  },
  {
    "objectID": "lectures/8.html#observing-data",
    "href": "lectures/8.html#observing-data",
    "title": "Group Regression Models",
    "section": "Observing Data",
    "text": "Observing Data\n\\[\nY = \\beta_0 + \\varepsilon\n\\]"
  },
  {
    "objectID": "lectures/8.html#estimated-line",
    "href": "lectures/8.html#estimated-line",
    "title": "Group Regression Models",
    "section": "Estimated Line",
    "text": "Estimated Line\n\\[\n\\hat Y=\\hat\\beta_0\n\\]"
  },
  {
    "objectID": "lectures/8.html#notation-1",
    "href": "lectures/8.html#notation-1",
    "title": "Group Regression Models",
    "section": "Notation",
    "text": "Notation\n\n\nObserved\n\\[\nY = \\beta_0 + \\varepsilon\n\\]\n\nEstimated\n\\[\n\\hat Y = \\hat \\beta_0\n\\]"
  },
  {
    "objectID": "lectures/8.html#indexing-data",
    "href": "lectures/8.html#indexing-data",
    "title": "Group Regression Models",
    "section": "Indexing Data",
    "text": "Indexing Data\nThe data in a data set can be indexed by a number.\n\npenguins[1,-c(1:2)]\n\n#&gt; # A tibble: 1 × 6\n#&gt;   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n#&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n#&gt; 1           39.1          18.7               181        3750 male   2007\n\n\n\nMaking the variable “body_mass_g” be represented by \\(Y\\) and “flipper_length_mm” as \\(X\\):\n\\[\nY_1 = 3750 \\ \\ X_1=181\n\\]"
  },
  {
    "objectID": "lectures/8.html#indexing-data-1",
    "href": "lectures/8.html#indexing-data-1",
    "title": "Group Regression Models",
    "section": "Indexing Data",
    "text": "Indexing Data\n\\[\nY_i, X_i\n\\]"
  },
  {
    "objectID": "lectures/8.html#data",
    "href": "lectures/8.html#data",
    "title": "Group Regression Models",
    "section": "Data",
    "text": "Data\nWith the data that we collect from a sample, we hypothesize how the data was generated.\n\nUsing a simple model:\n\\[\nY_i = \\beta_0 + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "lectures/8.html#estimated-value",
    "href": "lectures/8.html#estimated-value",
    "title": "Group Regression Models",
    "section": "Estimated Value",
    "text": "Estimated Value\n\\[\n\\hat Y_i = \\hat \\beta_0\n\\]"
  },
  {
    "objectID": "lectures/8.html#estimation",
    "href": "lectures/8.html#estimation",
    "title": "Group Regression Models",
    "section": "Estimation",
    "text": "Estimation\nTo estimate \\(\\hat \\beta_0\\), we minimize the follow function:\n\\[\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n\\]\n\nThis is known as the sum squared errors, SSE"
  },
  {
    "objectID": "lectures/8.html#residuals",
    "href": "lectures/8.html#residuals",
    "title": "Group Regression Models",
    "section": "Residuals",
    "text": "Residuals\nThe residuals are known as the observed errors from the data in the model:\n\\[\nr_i = Y_i - \\hat Y_i\n\\]"
  },
  {
    "objectID": "lectures/8.html#estimation-in-r",
    "href": "lectures/8.html#estimation-in-r",
    "title": "Group Regression Models",
    "section": "Estimation in R",
    "text": "Estimation in R\n\nlm(Y ~ 1, data = DATA)\n\n\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/8.html#modeling-body-mass-in-penguins",
    "href": "lectures/8.html#modeling-body-mass-in-penguins",
    "title": "Group Regression Models",
    "section": "Modeling Body Mass in Penguins",
    "text": "Modeling Body Mass in Penguins\n\nlm(body_mass_g ~ 1, data = penguins)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ 1, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)  \n#&gt;        4207\n\n\n\n\\[\n\\hat Y = 4207\n\\]"
  },
  {
    "objectID": "lectures/8.html#visualize",
    "href": "lectures/8.html#visualize",
    "title": "Group Regression Models",
    "section": "Visualize",
    "text": "Visualize\n\n\nCode\nggplot(penguins, aes(body_mass_g)) +\n  geom_density() +\n  geom_vline(xintercept = 4207)"
  },
  {
    "objectID": "lectures/8.html#linear-model-1",
    "href": "lectures/8.html#linear-model-1",
    "title": "Group Regression Models",
    "section": "Linear Model",
    "text": "Linear Model\nThe goal of Statistics is to develop models the have a better explanation of the outcome \\(Y\\).\n\nIn particularly, reduce the sum of squared errors.\n\n\nBy utilizing a bit more of information, \\(X\\), we can increase the predicting capabilities of the model.\n\n\nThus, the linear model is born."
  },
  {
    "objectID": "lectures/8.html#visualization",
    "href": "lectures/8.html#visualization",
    "title": "Group Regression Models",
    "section": "Visualization",
    "text": "Visualization\n\n1-Dimensional2-Dimensional\n\n\n\n\nCode\nggplot(penguins, aes(body_mass_g)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, fill = after_stat(level))) +\n  stat_density_2d(geom = \"polygon\")"
  },
  {
    "objectID": "lectures/8.html#linear-model-2",
    "href": "lectures/8.html#linear-model-2",
    "title": "Group Regression Models",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]\n\\[\n\\varepsilon \\sim DGP_3\n\\]"
  },
  {
    "objectID": "lectures/8.html#scatter-plot",
    "href": "lectures/8.html#scatter-plot",
    "title": "Group Regression Models",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\n\nCode\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/8.html#imposing-a-line",
    "href": "lectures/8.html#imposing-a-line",
    "title": "Group Regression Models",
    "section": "Imposing a Line",
    "text": "Imposing a Line\n\n\nCode\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) + \n  geom_point() +\n  stat_smooth(method = \"lm\", se = F)"
  },
  {
    "objectID": "lectures/8.html#modelling-the-data",
    "href": "lectures/8.html#modelling-the-data",
    "title": "Group Regression Models",
    "section": "Modelling the Data",
    "text": "Modelling the Data\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "lectures/8.html#linear-model-3",
    "href": "lectures/8.html#linear-model-3",
    "title": "Group Regression Models",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\n\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 X_i\n\\]\n\nGoal is to obtain numerical values for \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) that will minimize the SSE."
  },
  {
    "objectID": "lectures/8.html#sse",
    "href": "lectures/8.html#sse",
    "title": "Group Regression Models",
    "section": "SSE",
    "text": "SSE\n\\[\n\\sum^n_{i=1} (Y_i-\\hat Y_i)^2\n\\]\n\\[\n\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 X_i\n\\]"
  },
  {
    "objectID": "lectures/8.html#fitting-a-model-in-r",
    "href": "lectures/8.html#fitting-a-model-in-r",
    "title": "Group Regression Models",
    "section": "Fitting a Model in R",
    "text": "Fitting a Model in R\n\nlm(Y ~ X, data = DATA)\n\n\nX: Name Predictor Variable of Interest in data frame DATA\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/8.html#example",
    "href": "lectures/8.html#example",
    "title": "Group Regression Models",
    "section": "Example",
    "text": "Example\nY: “body_mass_g”; X: “flipper_length_mm”\n\nlm(body_mass_g ~ flipper_length_mm, data = penguins)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm  \n#&gt;          -5872.09              50.15\n\n\n\\[\n\\hat Y_i = -5872.09 + 50.15 X_i\n\\]"
  },
  {
    "objectID": "lectures/8.html#interpretation-of-hatbeta_0",
    "href": "lectures/8.html#interpretation-of-hatbeta_0",
    "title": "Group Regression Models",
    "section": "Interpretation of \\(\\hat\\beta_0\\)",
    "text": "Interpretation of \\(\\hat\\beta_0\\)\nThe intercept \\(\\hat \\beta_0\\) can be interpreted as the base value when \\(X\\) is set to 0.\n\nSome times the intercept can be interpretable to real world scenarios.\n\n\nOther times it cannot."
  },
  {
    "objectID": "lectures/8.html#interpreting-example",
    "href": "lectures/8.html#interpreting-example",
    "title": "Group Regression Models",
    "section": "Interpreting Example",
    "text": "Interpreting Example\n\\[\n\\hat Y_i = -5872.09 + 50.15 X_i\n\\]\nWhen flipper length is 0 mm, the body mass is -5872 grams."
  },
  {
    "objectID": "lectures/8.html#interpretation-of-hat-beta_1",
    "href": "lectures/8.html#interpretation-of-hat-beta_1",
    "title": "Group Regression Models",
    "section": "Interpretation of \\(\\hat \\beta_1\\)",
    "text": "Interpretation of \\(\\hat \\beta_1\\)\nThe slope \\(\\hat \\beta_1\\) indicates how will y change when x increases by 1 unit.\n\nIt will demonstrate if there is, on average, a positive or negative relationship based on the sign provided."
  },
  {
    "objectID": "lectures/8.html#interpreting-example-1",
    "href": "lectures/8.html#interpreting-example-1",
    "title": "Group Regression Models",
    "section": "Interpreting Example",
    "text": "Interpreting Example\n\\[\n\\hat Y_i = -5872.09 + 50.15 X_i\n\\]\nWhen flipper length increases by 1 mm, the body mass will increase by 50.15 grams."
  },
  {
    "objectID": "lectures/8.html#body-mass-with-species",
    "href": "lectures/8.html#body-mass-with-species",
    "title": "Group Regression Models",
    "section": "Body Mass with Species",
    "text": "Body Mass with Species\n\n\nCode\nggplot(penguins, aes(body_mass_g)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/8.html#body-mass-with-species-1",
    "href": "lectures/8.html#body-mass-with-species-1",
    "title": "Group Regression Models",
    "section": "Body Mass with Species",
    "text": "Body Mass with Species\n\n\nCode\nggplot(penguins, aes(body_mass_g, fill = species)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/8.html#group-statistics",
    "href": "lectures/8.html#group-statistics",
    "title": "Group Regression Models",
    "section": "Group Statistics",
    "text": "Group Statistics\nWe can use statistics to explain a continuous variable by the categories.\n\nCompute statistics for each group.\n\n\n\nnum_by_cat_stats(DATA, NUM, CAT)\n\n\nNUM: Name of the numerical variable\nCAT: Name of the categorical variable\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/8.html#compute-group-statistics",
    "href": "lectures/8.html#compute-group-statistics",
    "title": "Group Regression Models",
    "section": "Compute Group Statistics",
    "text": "Compute Group Statistics\n\nnum_by_cat_stats(penguins, body_mass_g, species)\n\n#&gt;   Categories  min    q25     mean median  q75  max      sd      var   iqr\n#&gt; 1     Adelie 2850 3362.5 3706.164   3700 4000 4775 458.620 210332.4 637.5\n#&gt; 2  Chinstrap 2700 3487.5 3733.088   3700 3950 4800 384.335 147713.5 462.5\n#&gt; 3     Gentoo 3950 4700.0 5092.437   5050 5500 6300 501.476 251478.3 800.0\n#&gt;   missing\n#&gt; 1       0\n#&gt; 2       0\n#&gt; 3       0"
  },
  {
    "objectID": "lectures/8.html#lm-with-categorical-variables",
    "href": "lectures/8.html#lm-with-categorical-variables",
    "title": "Group Regression Models",
    "section": "LM with Categorical Variables",
    "text": "LM with Categorical Variables\nA line is normally used to model 2 continuous variables.\n\nHowever, the predictor variable \\(X\\) can be restricted to a set a variables that can symbolize categories.\n\n\nA category will be used as a reference for a model."
  },
  {
    "objectID": "lectures/8.html#binary-dummy-variables",
    "href": "lectures/8.html#binary-dummy-variables",
    "title": "Group Regression Models",
    "section": "Binary (Dummy) Variables",
    "text": "Binary (Dummy) Variables\nBinary variables are variable that can only take on the value 0 or 1.\n\\[\nD_i = \\left\\{\n\\begin{array}{cc}\n1 & Category\\\\\n0 & Other\n\\end{array}\n\\right.\n\\]"
  },
  {
    "objectID": "lectures/8.html#binary-dummy-variables-1",
    "href": "lectures/8.html#binary-dummy-variables-1",
    "title": "Group Regression Models",
    "section": "Binary (Dummy) Variables",
    "text": "Binary (Dummy) Variables\nTo fit a model with categorical variables, we must utilize dummy (binary) variables that indicate which category is being referenced. We use \\(C-1\\) dummy variables where \\(C\\) indicates the number of categories. When coded correctly, each category will be represented by a combination of dummy variables."
  },
  {
    "objectID": "lectures/8.html#example-1",
    "href": "lectures/8.html#example-1",
    "title": "Group Regression Models",
    "section": "Example",
    "text": "Example\nIf we have 4 categories, we will need 3 dummy variables:\n\n\n\n\nCat 1\nCat 2\nCat 3\nCat 4\n\n\n\n\nDummy 1\n1\n0\n0\n0\n\n\nDummy 2\n0\n1\n0\n0\n\n\nDummy 2\n0\n0\n1\n0"
  },
  {
    "objectID": "lectures/8.html#species-dummy-variables",
    "href": "lectures/8.html#species-dummy-variables",
    "title": "Group Regression Models",
    "section": "Species Dummy Variables",
    "text": "Species Dummy Variables\n\n\n\n\nChinstrap\nGentoo\nAdelie\n\n\n\n\n\\(D_1\\)\n1\n0\n0\n\n\n\\(D_2\\)\n0\n1\n0"
  },
  {
    "objectID": "lectures/8.html#linear-model-4",
    "href": "lectures/8.html#linear-model-4",
    "title": "Group Regression Models",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\n\\hat Y_i = \\hat \\beta_0 + \\hat\\beta_1 D_{1i} + \\hat\\beta_2 D_{2i}\n\\]\n\n\\(\\hat \\beta_1\\) indicates how body mass changes from Adelie to Chinstrap.\n\n\n\\(\\hat \\beta_2\\) indicates how body mass changes from Adelie to Gentoo.\n\n\n\\(\\hat \\beta_0\\) represents the baseline level, in this case the body mass of Adelie."
  },
  {
    "objectID": "lectures/8.html#fitting-a-model-in-r-1",
    "href": "lectures/8.html#fitting-a-model-in-r-1",
    "title": "Group Regression Models",
    "section": "Fitting a Model in R",
    "text": "Fitting a Model in R\n\nlm(Y ~ X, data = DATA)\n\n\nX: Name Predictor Variable of Interest in data frame DATA, must be a factor variable\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/8.html#x-not-a-factor",
    "href": "lectures/8.html#x-not-a-factor",
    "title": "Group Regression Models",
    "section": "X not a Factor",
    "text": "X not a Factor\n\nlm(Y ~ factor(X), data = DATA)\n\n\nX: Name Predictor Variable of Interest in data frame DATA, not a factor variable\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/8.html#example-2",
    "href": "lectures/8.html#example-2",
    "title": "Group Regression Models",
    "section": "Example",
    "text": "Example\n\nlm(body_mass_g ~ species, penguins)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;      (Intercept)  speciesChinstrap     speciesGentoo  \n#&gt;          3706.16             26.92           1386.27\n\n\n\\[\n\\hat Y_i = 3706 + 26.92 D_{1i} + 1386.27 D_{2i}\n\\]"
  },
  {
    "objectID": "lectures/8.html#finding-the-adelie-mass",
    "href": "lectures/8.html#finding-the-adelie-mass",
    "title": "Group Regression Models",
    "section": "Finding the Adelie MASS",
    "text": "Finding the Adelie MASS\n\\[\n\\hat Y_i = 3706 + 26.92 (0) + 1386.27 (0)\n\\]"
  },
  {
    "objectID": "lectures/8.html#finding-the-chinstrap-mass",
    "href": "lectures/8.html#finding-the-chinstrap-mass",
    "title": "Group Regression Models",
    "section": "Finding the Chinstrap MASS",
    "text": "Finding the Chinstrap MASS\n\\[\n\\hat Y_i = 3706 + 26.92 (1) + 1386.27 (0)\n\\]"
  },
  {
    "objectID": "lectures/8.html#finding-the-gentoo-mass",
    "href": "lectures/8.html#finding-the-gentoo-mass",
    "title": "Group Regression Models",
    "section": "Finding the Gentoo MASS",
    "text": "Finding the Gentoo MASS\n\\[\n\\hat Y_i = 3706 + 26.92 (0) + 1386.27 (1)\n\\]"
  },
  {
    "objectID": "lectures/8.html#intepreting-hat-beta_1",
    "href": "lectures/8.html#intepreting-hat-beta_1",
    "title": "Group Regression Models",
    "section": "Intepreting \\(\\hat \\beta_1\\)",
    "text": "Intepreting \\(\\hat \\beta_1\\)\nOn average, Chinstrap has a larger mass than Adelie by about 26.92 grams."
  },
  {
    "objectID": "lectures/8.html#intepreting-hat-beta_2",
    "href": "lectures/8.html#intepreting-hat-beta_2",
    "title": "Group Regression Models",
    "section": "Intepreting \\(\\hat \\beta_2\\)",
    "text": "Intepreting \\(\\hat \\beta_2\\)\nOn average, Gentoo has a larger mass than Adelie by about 1386.27 grams."
  },
  {
    "objectID": "lectures/8.html#correlation",
    "href": "lectures/8.html#correlation",
    "title": "Group Regression Models",
    "section": "Correlation",
    "text": "Correlation\nCorrelation is a statistics that can be used to describe the strength of the relationship between 2 continuous variables.\n\n\\[\nr = \\frac{1}{n-1}\\sum^n_{i=1}\\frac{x_i - \\bar x}{s_x}\\frac{y_i - \\bar y}{s_y}\n\\]\n\n\\(\\bar x\\), \\(\\bar y\\): sample means\n\\(s_x\\), \\(s_y\\): sample standard deviations\n\n\n\n\\[\n-1 \\leq r \\leq 1\n\\]"
  },
  {
    "objectID": "lectures/8.html#correlations",
    "href": "lectures/8.html#correlations",
    "title": "Group Regression Models",
    "section": "Correlations",
    "text": "Correlations\n\nFrom IMS 2e"
  },
  {
    "objectID": "lectures/8.html#coefficient-of-determination",
    "href": "lectures/8.html#coefficient-of-determination",
    "title": "Group Regression Models",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\nThe coefficient of determination evaluates the strength between an outcome \\(Y\\) and the linear model, which includes \\(X\\).\n\n\\[\nR^2 = r^2\n\\]\n\n\n\\[\n0 \\leq R^2 \\leq 1\n\\]\n\n\nThe coefficient of determination measures the total variation explained by the linear model. The closer to 1, the better the linear model."
  },
  {
    "objectID": "lectures/8.html#correlation-in-r",
    "href": "lectures/8.html#correlation-in-r",
    "title": "Group Regression Models",
    "section": "Correlation in R",
    "text": "Correlation in R\n\ncor(DATA$Y, DATA$X)\n\n\nX: Name Predictor Variable of Interest in data frame DATA\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/8.html#example-3",
    "href": "lectures/8.html#example-3",
    "title": "Group Regression Models",
    "section": "Example",
    "text": "Example\n\ncor(penguins$body_mass_g, penguins$flipper_length_mm)\n\n#&gt; [1] 0.8729789"
  },
  {
    "objectID": "lectures/8.html#coefficient-of-determination-in-r",
    "href": "lectures/8.html#coefficient-of-determination-in-r",
    "title": "Group Regression Models",
    "section": "Coefficient of Determination in R",
    "text": "Coefficient of Determination in R\n\nxlm &lt;- lm(Y ~ X, data = DATA)\nr2(xlm)\n\n\nX: Name Predictor Variable of Interest in data frame DATA\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame"
  },
  {
    "objectID": "lectures/8.html#example-4",
    "href": "lectures/8.html#example-4",
    "title": "Group Regression Models",
    "section": "Example",
    "text": "Example\n\nxlm &lt;- lm(body_mass_g ~ species, penguins)\nr2(xlm)\n\n#&gt; [1] 0.6744887"
  },
  {
    "objectID": "lectures/8.html#statistical-model",
    "href": "lectures/8.html#statistical-model",
    "title": "Group Regression Models",
    "section": "Statistical Model",
    "text": "Statistical Model\n\\[\n\\hat Y = \\hat \\beta_0 + \\hat \\beta_1 X\n\\]\n\n\\(X\\): Input\n\\(\\hat Y\\): Output"
  },
  {
    "objectID": "lectures/8.html#prediction-1",
    "href": "lectures/8.html#prediction-1",
    "title": "Group Regression Models",
    "section": "Prediction",
    "text": "Prediction\nUsing the equation \\(\\hat Y\\), we can give it a value of \\(X\\) and then, in return, a value of \\(\\hat Y\\) that predicts the true value \\(Y\\)."
  },
  {
    "objectID": "lectures/8.html#prediction-in-r",
    "href": "lectures/8.html#prediction-in-r",
    "title": "Group Regression Models",
    "section": "Prediction in R",
    "text": "Prediction in R\n\nxlm &lt;- lm(Y ~ X,\n            data = DATA)\n\npredict_df &lt;- data.frame(X = VAL)\n\npredict(xlm,\n        predict_df)\n\n\nX: Name Predictor Variable of Interest in data frame DATA\nY: Name Outcome Variable of Interest in data frame DATA\nDATA: Name of the data frame\nVAL: Value for the Predictor Variable"
  },
  {
    "objectID": "lectures/8.html#example-1-1",
    "href": "lectures/8.html#example-1-1",
    "title": "Group Regression Models",
    "section": "Example 1",
    "text": "Example 1\n\nExampleCode\n\n\nPredict the body mass for a gentoo penguin.\n\n\n\nxlm &lt;- lm(body_mass_g ~ species,\n            data = penguins)\n\nxlm\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ species, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;      (Intercept)  speciesChinstrap     speciesGentoo  \n#&gt;          3706.16             26.92           1386.27\n\npredict_df &lt;- data.frame(species = \"Gentoo\")\n\npredict(xlm,\n        predict_df)\n\n#&gt;        1 \n#&gt; 5092.437"
  },
  {
    "objectID": "lectures/8.html#example-2-1",
    "href": "lectures/8.html#example-2-1",
    "title": "Group Regression Models",
    "section": "Example 2",
    "text": "Example 2\n\nExampleCode\n\n\nPredict the body mass for a penguin with a flipper length of 190.\n\n\n\nxlm &lt;- lm(body_mass_g ~ flipper_length_mm,\n            data = penguins)\n\n\nxlm\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm  \n#&gt;          -5872.09              50.15\n\npredict_df &lt;- data.frame(flipper_length_mm = 190)\n\npredict(xlm,\n        predict_df)\n\n#&gt;        1 \n#&gt; 3657.028"
  },
  {
    "objectID": "lectures/8.html#interpolation",
    "href": "lectures/8.html#interpolation",
    "title": "Group Regression Models",
    "section": "Interpolation",
    "text": "Interpolation\nInterpolation is the process of estimating a value within the range of the observed input data \\(X\\)."
  },
  {
    "objectID": "lectures/8.html#extrapolation",
    "href": "lectures/8.html#extrapolation",
    "title": "Group Regression Models",
    "section": "Extrapolation",
    "text": "Extrapolation\nExtrapolation is the process of estimating a value beyond the range of observed input data \\(X\\). It’s about venturing into the unknown, using what we know as a guide."
  },
  {
    "objectID": "lectures/8.html#extrapolation-1",
    "href": "lectures/8.html#extrapolation-1",
    "title": "Group Regression Models",
    "section": "Extrapolation",
    "text": "Extrapolation\n\n\nCode\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) + \n  xlim(160, 250) +\n  ylim(2600, 7000) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F)"
  }
]